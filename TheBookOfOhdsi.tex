\documentclass[11pt]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmainfont[]{Times New Roman}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[paperheight=10in,paperwidth=7in,margin=1in,left=0.6in,right=0.6in,top=1in,bottom=0.6in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={The Book of OHDSI},
            pdfauthor={Observational Health Data Science and Informatics},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{The Book of OHDSI}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Observational Health Data Science and Informatics}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-07-10}

\usepackage{booktabs}
\usepackage{amsthm}

\usepackage{makeidx}
\makeindex
\usepackage[nottoc]{tocbibind}

\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother


\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\makeatletter
\@ifundefined{Shaded}{
}{\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}}
\makeatother

\newenvironment{rmdblock}[1]
  {
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
  \setlength{\fboxsep}{1em}
  \begin{kframe}
  \item
  }
  {
  \end{kframe}
  \end{itemize}
  }
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdsummary}
  {\begin{rmdblock}{summary}}
  {\end{rmdblock}}

%\sloppy % helps with poorly wrapped URLs, but can look ugly in places
%\AtBeginDocument{\raggedright}

%\usepackage{fontspec} % Requires xelatex
%\setmainfont{Arial}

%\usepackage{titlesec}
%\titleformat{\chapter}[runin]{}{}{}{}[]
%\titleformat{\chapter}% reformat chapter headings
%     [hang]% like section, with number on same line
%     {\Huge\bfseries}% formatting applied to whole
%     {\thechapter}% Chapter number
%     {0.5em}% space between # and title
%     {}% formatting applied just to title

\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\},fontsize=\small}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

This is a book about OHDSI, and is currently very much under development.

The book is written in \href{https://rmarkdown.rstudio.com}{RMarkdown} with \href{https://bookdown.org}{bookdown}. It is automatically rebuilt from \href{https://github.com/OHDSI/TheBookOfOhdsi}{source} by \href{http://travis-ci.org/}{travis}.

\hypertarget{goals-of-this-book}{%
\section*{Goals of this book}\label{goals-of-this-book}}
\addcontentsline{toc}{section}{Goals of this book}

This book aims to be a central knowledge repository for OHDSI, and focuses on describing the OHDSI community, data standards, and tools. It is intended both for those new to OHDSI and veterans alike, and aims to be practical, providing the necessary theory and subsequent instructions on how to do things. After reading this book you will understand what OHDSI is, and how you can join the journey. You will learn what the common data model and standard vocabularies are, and how they can be used to standard an observational healthcare database. You will learn there are three main uses cases for these data: characterization, population-level estimation, and patient-level prediction, and that all three activities are supported by OHDSI's open source tools, and how to use them. You will learn how to establish the quality of the generated evidence through data quality, clinical validity, software validity, and method validity. Lastly, you will learn how these tools can be used to execute these studies in a distributed research network.

\hypertarget{structure-of-the-book}{%
\section*{Structure of the book}\label{structure-of-the-book}}
\addcontentsline{toc}{section}{Structure of the book}

This book is organized in five major sections:

\begin{enumerate}
\def\labelenumi{\Roman{enumi})}
\tightlist
\item
  The OHDSI Community
\item
  Uniform data representation
\item
  Data Analytics
\item
  Evidence Quality
\item
  OHDSI Studies
\end{enumerate}

Each section has multiple chapters, and each chapter aims to follow the following main outline: Introduction, Theory, Practice, Excercises.

\hypertarget{contributors}{%
\section*{Contributors}\label{contributors}}
\addcontentsline{toc}{section}{Contributors}

TODO: make list of contributors complete

Each chapter lists one or more chapter leads. These are the people who lead the writing of the chapters. However, there are many others that have contributed to the book, whom we would like to acknowledge here:

\begin{tabular}{l|l|l}
\hline
Hamed Abedtash & Mustafa Ascha & Mark Beno\\
\hline
Clair Blacketer & Brian Christian & Gino Cloft\\
\hline
Sara Dempster & Jon Duke & Sergio Eslava\\
\hline
Clark Evans & Thomas Falconer & George Hripcsak\\
\hline
Mark Khayter & Greg Klebanov & Kristin Kostka\\
\hline
Bob Lanese & Wanda Lattimore & Chun Li\\
\hline
David Madigan & Sindhoosha  Malay & Harry Menegay\\
\hline
Akihiko Nishimura & Ellen Palmer & Nirav Patil\\
\hline
Jose Posada & Dani Prieto-Alhambra & Christian Reich\\
\hline
Jenna Reps & Peter Rijnbeek & Patrick Ryan\\
\hline
Craig Sachson & Izzy Saridakis & Paula Saroufim\\
\hline
Martijn Schuemie & Sarah Seager & Chan Seng You\\
\hline
Anthony Senna & Sunah Song & Matt Spotnitz\\
\hline
Marc Suchard & Joel Swerdel & Devin Tian\\
\hline
Don Torok & Kees van Bochove & Mui Van Zandt\\
\hline
Kristin Waite & Mike Warfe & Jamie Weaver\\
\hline
James Wiggins & Andrew Williams & Chan You Seng\\
\hline
\end{tabular}

\hypertarget{part-the-ohdsi-community}{%
\part{The OHDSI Community}\label{part-the-ohdsi-community}}

\hypertarget{MissionVissionValues}{%
\chapter{Mission, vision, values}\label{MissionVissionValues}}

\emph{Chapter lead: George Hripcsak}

\hypertarget{our-mission}{%
\section{Our Mission}\label{our-mission}}

\begin{quote}
To improve health by empowering a community to collaboratively generate the evidence that promotes better health decisions and better care.
\end{quote}

\hypertarget{our-vision}{%
\section{Our Vision}\label{our-vision}}

\begin{quote}
A world in which observational research produces a comprehensive understanding of health and disease.
\end{quote}

\hypertarget{our-objectives}{%
\section{Our Objectives}\label{our-objectives}}

\begin{itemize}
\item
  \textbf{Innovation}: Observational research is a field which will benefit greatly from disruptive thinking. We actively seek and encourage fresh methodological approaches in our work.
\item
  \textbf{Reproducibility}: Accurate, reproducible, and well-calibrated evidence is necessary for health improvement.
\item
  \textbf{Community}: Everyone is welcome to actively participate in OHDSI, whether you are a patient, a health professional, a researcher, or someone who simply believes in our cause.
\item
  \textbf{Collaboration}: We work collectively to prioritize and address the real world needs of our community's participants.
\item
  \textbf{Openness}: We strive to make all our community's proceeds open and publicly accessible, including the methods, tools and the evidence that we generate.
\item
  \textbf{Beneficence}: We seek to protect the rights of individuals and organizations within our community at all times.
\end{itemize}

\hypertarget{Collaborators}{%
\chapter{Collaborators}\label{Collaborators}}

\emph{Chapter lead: Patrick Ryan}

History of OHDSI

Map of collaborators
Forums
Wiki
Workgroups and chapters
Symposia and hack-a-thons

Governance at local sites

\hypertarget{OpenScience}{%
\chapter{Open Science}\label{OpenScience}}

\emph{Chapter lead: Kees van Bochove}

From the inception of the OHDSI community, the goal was to establish an international collaborative by building on open science values, such as the use of open source software, public availability of all conference proceedings and materials, and transparent, open access publication of generated medical evidence. But what exactly is open science? And how could OHDSI build an open science or open data strategy around medical data, which is very privacy sensitive and typically not open at all for good reasons? Why is it so important to have reproducibility of analysis, and how does the OHDSI community aim to achieve this? These are some of the questions that we touch on in this chapter.

\hypertarget{open-science}{%
\section{Open Science}\label{open-science}}

The term `open science' has been used since the nineties, but really gained traction in the 2010s, during the same period OHDSI was born. Wikipedia \citep{wiki:Open_science} defines it as ``the movement to make scientific research (including publications, data, physical samples, and software) and its dissemination accessible to all levels of an inquiring society, amateur or professional'', and goes on to state that it is typically developed through collaborative networks. Although the OHDSI community never positioned itself explicitly as an `open science' collective or network, the term is frequently used to explain the driving concepts and principles behind OHDSI. For example, in 2015, Jon Duke presented OHDSI as ``An Open Science Approach to Medical Evidence Generation''\footnote{\url{https://www.ohdsi.org/wp-content/uploads/2014/07/ARM-OHDSI_Duke.pdf}}, and in 2019, the EHDEN projects' introductory webinar hailed the OHDSI network approach as ``21st Century Real World Open Science''\footnote{\url{https://www.ehden.eu/webinars/}}. Indeed, as we shall see in this chapter, many of the practices of open science can be found in today's OHDSI community. One could argue that the OHDSI community is a grassroots open science collective driven by a shared desire for improving the transparency and reliability of medical evidence generation.

Open science or ``Science 2.0'' \citep{wiki:Science_2.0} approaches mean to address a number of perceived problems within the current scientific practice. Information technology has led to an explosion of data generation and analysis methods, and for individual researchers, it is very hard to keep up with all literature published in their area of expertise. This holds even more true for medical doctors who have a practice to run as day job, but still need to keep abreast of the latest medical evidence. In addition, there is growing concern that many experiments may suffer from poor statistical designs, publication bias, p-hacking and similar statistical problems, and are hard to reproduce. The traditional method of correcting these problems, peer review of published articles, often fails to identify and tackle these problems. The special 2018 Nature edition on ``Challenges in irreproducible research''\footnote{\url{https://www.nature.com/collections/prbfkwmwvz}} includes several examples of this. A group of authors attempting to apply systematic peer review on the articles in their field found that, for various reasons, it was very hard to get the errors they identified rectified. They encountered common statistical problems such as poor randomization designs leading to false conclusions about statistical significance, miscalculations in meta-analyses, and inappropriate baseline comparisons. \citep{allison_2016} Another paper from the same collection, taking experiences from physics as an example, argues that it is critical to not only provide access to the underlying data, but also to publish openly the data processing and analysis scripts to achieve reproducibility.

The OHDSI community addresses these challenges in its own way, and puts significant emphasis on the importance of generating medical evidence at scale. As stated in \citet{schuemie_2018b}, while the current paradigm ``centres on generating one estimate at a time using a unique study design with unknown reliability and publishing (or not) one estimate at a time'', the OHDSI community ``advocates for high-throughput observational studies
using consistent and standardized methods, allowing evaluation, calibration and unbiased dissemination to generate a more reliable and complete evidence base.'' This is achieved by a combination of a network of medical data sources that map their data to the OMOP common data model, open source analytics code that can be used and verified by all, and large-scale baseline data such as the condition occurences published at howoften.org. In the following paragraphs, concrete examples are provided and the open science approach of OHDSI is detailed further using the triad of Open Standards, Open Source and Open Data as a guide. The chapter is concluded with a brief reference to the FAIR principles and outlook for OHDSI from an open science perspective.

\hypertarget{open-science-in-action-the-study-a-thon}{%
\section{Open Science in Action: the Study-a-thon}\label{open-science-in-action-the-study-a-thon}}

A recent development in the community is the emergence of `study-a-thons': short, concentrated face to face gatherings of a multidisciplinary group of scientists aimed at answering an important clinically relevant research question using the OMOP data model and the OHDSI tools. A nice example is the 2018 Oxford study-a-thon, which is explained in an EHDEN webinar (\url{https://youtu.be/X5yuoJoL6xs}) which provides a walkthrough of the process and also highlights the openly available results. In the period leading up to the study-a-thon, the participants propose medically relevant research questions to study, and one or more research questions are selected to study during the study-a-thon itself. Data is provided through participants that have access to patient-level data in OMOP format and are able to run queries on these data sources. Much of the actual study-a-thon time is devoted to discussing the statistical approach (see also the next chapter), the suitability of the data sources, the results which are interactively produced and the follow-up questions that are inevitably raised by these results. In the case of the Oxford study-a-thon, the questions centered around studying adverse post-surgical effects of different knee replacement methods, and the results were published interactively during the study-a-thon using the OHDSI forums and tools. \footnote{\url{https://forums.ohdsi.org/t/ohdsi-oxford-study-a-thon-any-data-partners-want-to-participate-this-week-in-two-studies-on-knee-replacement/5650}}

\hypertarget{open-standards}{%
\section{Open Standards}\label{open-standards}}

A very significant community resource that is maintained in the OHDSI community is the OMOP Common Data Model and associated Standardized Vocabularies. The model itself is scoped to capture observational healthcare data, and is specifically meant to analyze associations between exposures such as drugs, procedures, devices etc. and outcomes such as conditions and measurements (see also chapter 5.1). However, harmonizing healthcare data worldwide from a wide variety of coding systems, healthcare paradigm and different types of healthcare sources requires a massive amount of `mappings' between source codes and their closest standardized counterparts. The OMOP Standardized Vocabulary is further described in chapter 6 and includes mappings from hundreds of medical coding systems that are in used worldwide, and is browseable through the OHDSI Athena tool. By providing these vocabularies and mappings as a freely available community resource, OMOP and the OHDSI community make a significant contribution to healthcare data analytics and is by several accounts the most comprehensive model for this purpose, representing approximately 1.2 billion healthcare records worldwide \citep{garza_2016} \footnote{\url{https://www.ema.europa.eu/en/events/common-data-model-europe-why-which-how}}.

\hypertarget{open-source}{%
\section{Open Source}\label{open-source}}

Another key resource the OHDSI community provides are open source programs. These can be divided in several categories, such as the helper tools to map data to OMOP, the OHDSI Methods Library which contain a powerful suite of commonly used statistical methods, and ATLAS, Athena and other infrastructure-related software which underpins the OHDSI ecosystem. See chapter 9 for a detailed overview.
From an open science perspective, one of the most important resources is the Methods Library, which ensures a consistent re-use of statistical methods across analytical use cases, and which can be inspected, reviewed and contributed to via GitHub.

\hypertarget{open-data}{%
\section{Open Data}\label{open-data}}

Because of the privacy-sensitive nature of healthcare data, fully open comprehensive patient-level datasets are typically not available. However, the OHDSI community provides simulated datasets such as SynPUF for testing and development purposes, and the OHDSI Research Network (see chapter 21) can be leveraged to run studies in a network of available datasources that have mapped their data to OMOP.

\hypertarget{ohdsi-and-the-fair-guiding-principles}{%
\section{OHDSI and the FAIR Guiding Principles}\label{ohdsi-and-the-fair-guiding-principles}}

This last paragraph of the chapter takes a look at the current state of the OHDSI community and tooling, using the 15 FAIR Data Guilding Principles published in \citet{wilkinson2016}.

\hypertarget{findability}{%
\subsection{Findability}\label{findability}}

Any healthcare database that is mapped to OMOP and used for analytics, should from a scientific perspective be persisted for future reference and reproducibility. The use of persistent identifiers for OMOP databases is not yet widespread, partly because these databases are often contained behind firewalls and on internal networks and not necessarily connected to the internet. However, it is of course entirely possible to publish summaries of the databases as a descriptor record that can be referenced for e.g.~citation purposes. This method is followed in for example the EMIF catalog \footnote{\url{https://emif-catalogue.eu}}, which provides a comprehensive record of the database in terms of data gathering purpose, sources, vocabularies and terms, access control mechanisms, license, consents etc. \citep{Oliveira2019} This approach is further developed in the IMI EHDEN project.

\hypertarget{accessibility}{%
\subsection{Accessibility}\label{accessibility}}

Accessibility of OMOP mapped data through an open protocol is typically achieved through the SQL interface, which combined with the OMOP CDM provides a standardized and well-documented method for accessing OMOP data. However, as discussed above, OMOP sources are often not directly available over the internet for security reasons. Creating a secure worldwide healthcare data network that is accessible for researchers is an active research topic and operational goal of projects like IMI EHDEN. However, what can be openly published are results of analyses in multiple OMOP databases, as shown through OHDSI initiatives such as LEGEND and howfoften.org.

\hypertarget{interoperability}{%
\subsection{Interoperability}\label{interoperability}}

Interoperability is arguably the strong suit of the OMOP data model and OHDSI tooling. In order to build a strong network of medical data sources worldwide which can be leveraged for evidence generation, achieving interoperability between healthcare data sources is key, and this is achieved through the OMOP model and Standardized Vocabularies. However, by sharing cohort definitions and statistical approaches, the OHDSI community goes beyond code mapping and also provides a platform to build an interoperable understanding of the analysis methods for healthcare data.
Since healthcare systems such as hospitals are often the source of record for OMOP data, the interoperability of the OHDSI approach could be further enhanced by alignment with operational healthcare interoperability standards such as HL7 FHIR, HL7 CIMI, openEHR. The same goes for alignment with clinical interoperability standards such as CDISC and biomedical ontologies. Especially in areas such as oncology, this is an important topic, and the Oncology Working Group and Clinica Trials Working Group in the OHDSI community provide good examples of forums where these issues are actively discussed.
In terms of references to other data and specifically ontology terms, OHDSI Athena is an important tool as it allows the exploration of the OMOP Standardized Vocabularies in the context of other available medical coding systems.

\hypertarget{reusability}{%
\subsection{Reusability}\label{reusability}}

The FAIR principles around reusability focus on important issues such as the data license, provenance and the link to relevant community standards. The data provenance of OMOP databases is a very interesting topic, as there are potential improvements for making these available in an automated way, provided the ETL and mapping tools would persist metadata about for example the used CDM version, Standardized Vocabularies release, custom code lists etc. The OHDSI ETL tools do not currently produce this information automatically, but working groups such as the Data Quality Working Group and Metadata Working Group actively work on these. Another important aspect is the provenance of the underlying databases itself, for example it is important to know if a hospital or GP information system was replaced or changed, and when known data omissions or other data issues ocurred historically. Exploring ways to attach this metadata systematically in the OMOP CDM is the domain of the Metadata Working Group.

\hypertarget{conclusions}{%
\section{Conclusions}\label{conclusions}}

To conclude, the OHDSI community itself can be seen as an open science community that is actively pursuing the interoperability and reproducibility of medical evidence generation. It also advocates a paradigm shift from single study and single estimate medical research to large-scale systematic evidence generation, where facts such as baseline occurence are known and the evidence focuses on statistically estimating the effects of interventions and treatments from real world healthcare sources.

\hypertarget{WhereToBegin}{%
\chapter{Where to begin}\label{WhereToBegin}}

\emph{Chapter leads: Hamed Abedtash and Krista Kostka}

\begin{quote}
``A journey of a thousand miles begins with a single step.'' - Lao Tzu
\end{quote}

The OHDSI community represents a mosaic of stakeholders across academia, industry and government-entities. Our work benefits a range of individuals and organizations, including patients, providers, and researchers, as well as health care systems, industry, and government agencies. This benefit is achieved by improving both the quality of healthcare data analytics as well as the usefulness of healthcare data to these stakeholders. We believe observational research is a field which benefits greatly from disruptive thinking. We actively seek and encourage fresh methodological approaches in our work.

\hypertarget{joining-the-journey}{%
\section{Joining the Journey}\label{joining-the-journey}}

Everyone is welcome to actively participate in OHDSI, whether you are a patient, a health professional, a researcher, or someone who simply believes in our cause. OHDSI maintains an inclusive membership model. To become an OHDSI collaborator requires no membership fee. Collaboration is as simple as raising a hand to be included in the yearly OHDSI membership count. Involvement is entirely at-will. A collaborator can have any level of contribution within the community, ranging from someone who attends weekly community calls to leading network studies or OHDSI working groups. Collaborators do not have to be data holders to be considered active members of the community. The OHDSI community aims to serve data holders, researchers, health care providers and patients \& consumers alike. A record of collaborator profiles are maintained and periodically updated on the OHDSI website. Membership is fostered via OHDSI community calls, workgroups and regional chapters.

\hypertarget{ohdsi-community-calls}{%
\subsection{OHDSI Community Calls}\label{ohdsi-community-calls}}

OHDSI Community Calls are a weekly forum to spotlight ongoing activity within the OHDSI community. Held every Tuesday from 12-1pm ET, these teleconferences are a time for the OHDSI community to come together to share recent developments and recognize the accomplishments of individual collaborators, working groups and the community as a whole. Each week's meeting is recorded, and presentations are archived in the OHDSI website resources.

All OHDSI Collaborators are welcome to participate in this weekly teleconference and encouraged to propose topics for community discussion. OHDSI Community Calls can be a forum to share research findings, present and seek feedback for active works-in-progress, demonstrate open-source software tools under development, debate community best practices for data modeling and analytics, and brainstorm future collaborative opportunities for grants/publications/conference workshops. If you are a Collaborator with a topic for an upcoming OHDSI Collaborator meeting, you are invited to post your thoughts on the OHDSI Forums.

As a newcomer to the OHDSI community, it's highly encouraged to add this call to your calendar to get acquainted with what's happening across the OHDSI network. Newcomers are invited to introduce themselves on their first call and tell the community about themselves, their background and what brought them to OHDSI. If you'd like to join an OHDSI call, please contact Maura Beaton (\href{mailto:beaton@ohdsi.org}{\nolinkurl{beaton@ohdsi.org}}) for the latest dial-in details or consult the OHDSI wiki (\url{https://www.ohdsi.org/web/wiki/doku.php?id=projects:ohdsi_community}). Community call topics vary from week-to-week. Consult the OHDSI Weekly Digest on the OHDSI forum for more information on weekly presentation topics.

\hypertarget{ohdsi-workgroups}{%
\subsection{OHDSI Workgroups}\label{ohdsi-workgroups}}

OHDSI has a variety of ongoing projects lead by workgroup teams. Each workgroup has its own leadership team which determine the project's objectives, goals and artefacts to be contributed to the community. Workgroup participation is open to all who have an interest in contributing to the project objectives and goals. Workgroups may be long-standing, strategic objectives or short-term projects to accomplish a specific need in the community. Workgroup meeting cadence is determined by the project leadership and will vary from group to group. A list of the active workgroups is maintained on the OHDSI Wiki (\url{https://www.ohdsi.org/web/wiki/doku.php?id=projects:overview}).

\texttt{**May\ update\ to\ include\ a\ graphic\ of\ the\ workgroups\ by\ use\ case**}

\hypertarget{ohdsi-regional-chapters}{%
\subsection{OHDSI Regional Chapters}\label{ohdsi-regional-chapters}}

An OHDSI regional chapter represents a group of OHDSI collaborators located in a geographic area who wish to hold local networking events and meetings to address problems specific to their geographic location. Today, OHDSI regional chapters include OHDSI in Europe (\url{https://www.ohdsi-europe.org/}), OHDSI in South Korea (\url{http://forums.ohdsi.org/c/For-collaborators-wishing-to-communicate-in-Korean}) and OHDSI in China (\url{https://ohdsichina.org/}). If you would like to set-up an OHDSI regional chapter in your region, you may do so by following the OHDSI regional chapter process outlined on the OHDSI website (\url{https://www.ohdsi.org/who-we-are/regional-chapters}).

\hypertarget{ohdsi-research-network}{%
\subsection{OHDSI Research Network}\label{ohdsi-research-network}}

Many OHDSI collaborators are interested in converting their data into the OMOP Common Data Model. The OHDSI research network represents a diverse, global community of observational databases that have undergone {[}{[}ETL{]}{]} processes to become OMOP compliant. If your journey in the OHDSI community includes transforming data, there are numerous community resources available to aid you in your journey including {[}{[}tutorials{]}{]} on the OMOP CDM and Vocabularies, freely available tools to assist with conversion {[}{[}ETL chapter reference{]}{]} and workgroups targeting specific domains or types of data conversions. OHDSI collaborators are encouraged to utilize the OHDSI forum to discuss and troubleshoot challenges that arise during CDM conversions.

\hypertarget{navigating-your-odyssey-in-ohdsi}{%
\section{Navigating Your Odyssey in OHDSI}\label{navigating-your-odyssey-in-ohdsi}}

As discussed in the previous section, there are many ways to begin your journey in the OHDSI community. Collaborators often find the journey from initial interest in OHDSI to actively contributing to be as circuitous as Homer's Odyssey. For those interested in running OHDSI research studies, the simplest way to navigate your path forward is to learn how to speak in ``OHDSI'' terms.

\hypertarget{how-to-translate-your-research-question-into-an-ohdsi-framework}{%
\subsection{How to Translate Your Research Question into an OHDSI Framework}\label{how-to-translate-your-research-question-into-an-ohdsi-framework}}

The OHDSI community has a wide range of standardized analytic tools depending on the type of question you are formulating. In the following chapters, we will discuss the intricacies of these frameworks as well as the open source tools and code available to conduct these analyses. In this section, we will briefly discuss how to take your question and reframe it in OHDSI-speak, what analytical methods and tools are appropriate for data analysis, and where you can find the resources within the OHDSI community.

\hypertarget{step-1-identify-the-proper-framework}{%
\subsubsection{Step 1: Identify the proper framework}\label{step-1-identify-the-proper-framework}}

Before formulating your research question for execution on OHDSI platform, it is important to understand which OHDSI framework suits the objectives of research question, whether it is a ``clinical characterization'', ``population-level estimation'', or ``patient-level prediction''. Clinical characterization provides answers to ``What happened to them'' questions, population-level estimation responses to ``What are the causal effects'' question, and population-level estimation answers ``What will happen to me'' question. To learn more about the OHDSI use cases, please refer to Chapter \ref{DataAnalyticsUseCases}.

Once you understand the relationship between OHDSI framework and different study types you will be using, you can then further refine your question into OHDSI-speak. THere are examples of study categories that correspond to each OHDSI framework:

\begin{itemize}
\tightlist
\item
  \textbf{Clinical characterization}

  \begin{itemize}
  \tightlist
  \item
    Disease natural history
  \item
    Incidence rate
  \item
    Prevalence
  \item
    Treatment utilization
  \item
    Treatment pathway
  \item
    Quality improvement
  \end{itemize}
\item
  \textbf{Population-level effect estimation}

  \begin{itemize}
  \tightlist
  \item
    Safety surveillance
  \item
    Effect estimation
  \item
    Comparative effectiveness
  \end{itemize}
\item
  \textbf{Patient-level prediction}

  \begin{itemize}
  \tightlist
  \item
    Precision Medicine
  \item
    Disease onset and progression
  \item
    Treatment choice
  \item
    Disease interception
  \item
    Treatment response
  \item
    Treatment safety
  \item
    Treatment adherence
  \end{itemize}
\end{itemize}

Many people start with a simple characterization (e.g., how many people have angioedema? How often does a patient receive ACE inhibitors?). Even if you are thinking about an estimation or prediction question, you'll probably want to start with preliminary characterization analyses to understand your target and outcome cohorts. In fact, estimation and prediction studies produce characterization results as part of their standardized outputs.

The tool below also provides a crosswalk of the type of question with a desired output you may be formulating to what OHDSI framework may be most appropriate for that question.

\begin{itemize}
\tightlist
\item
  If your question is:

  \begin{itemize}
  \tightlist
  \item
    How many patients\ldots?
  \item
    How often does\ldots?
  \item
    What proportion ofpatients\ldots?
  \item
    What is the distribution of values for lab\ldots?
  \item
    What are the HbA1c levels for patients with\ldots?
  \item
    What are the {[}lab values{]} for patients\ldots?
  \item
    What is the median length of exposure for patients on\ldots.?
  \item
    What are the trends over time in\ldots?
  \item
    What are other drugs that these patients are using?
  \item
    What are concomitant therapies?
  \item
    Do we have enough cases of\ldots?
  \item
    Would it be feasible to study X\ldots?
  \item
    What are the demographics of\ldots?
  \item
    What are the risk factors of\ldots? (if identifying a specific risk factor, maybe estimation, not prediction)
  \item
    What are the predictors of\ldots?
  \end{itemize}
\item
  And the desired output is:

  \begin{itemize}
  \tightlist
  \item
    Count or percentage
  \item
    Averages
  \item
    Descriptive statistics
  \item
    Incidence rate
  \item
    Prevalence
  \item
    Cohort
  \item
    Rule-based phenotype
  \item
    Drug utilization
  \item
    Disease natural history
  \item
    Adherence
  \item
    Comorbidity profile
  \item
    Treatment pathways
  \item
    Line of therapy
  \end{itemize}
\item
  Then you're probably asking for:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Clinical characterization}
  \end{itemize}
\item
  If your question is:

  \begin{itemize}
  \tightlist
  \item
    What is the effect of\ldots?
  \item
    What if I do {[}intervention{]}\ldots?
  \item
    Which treatment works better?
  \item
    What is the risk of X on Y?
  \item
    What is the time-to-event of\ldots?
  \end{itemize}
\item
  And the desired output is:

  \begin{itemize}
  \tightlist
  \item
    Relative risk
  \item
    Hazards ratio
  \item
    Odds ratio
  \item
    Average treatment effect
  \item
    Causal effect
  \item
    Association
  \item
    Correlation
  \item
    Safety surveillance
  \item
    Comparative effectiveness
  \end{itemize}
\item
  Then you're probably asking for:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Population-level effect estimation}
  \end{itemize}
\item
  If your question is:

  \begin{itemize}
  \tightlist
  \item
    What is the chance that this patient will\ldots?
  \item
    Who are candidates for\ldots?
  \end{itemize}
\item
  And the desired output is:

  \begin{itemize}
  \tightlist
  \item
    Probability for an individual
  \item
    Prediction model
  \item
    High/low risk groups
  \item
    Probabilistic phenotype
  \end{itemize}
\item
  Then you're probably asking for:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Patient-level prediction}
  \end{itemize}
\end{itemize}

\hypertarget{step-2-frame-research-question-in-ohdsi-speak}{%
\subsubsection{Step 2: Frame research question in OHDSI-speak}\label{step-2-frame-research-question-in-ohdsi-speak}}

You now know from the previous step what OHDSI framework best suits your research question. Depending on the study objectives, an OHDSI-compliant research question should be structured in a way that explicitly describes the target population, the output(s) of interest, and analysis method (if applicable).

The list below provides OHDSI-speak template and example questions for different study categories as the best practice to formulate research questions. The subsequent chapters will explain the analytical methods to run your study.

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[b]{0.30\columnwidth}\raggedright
Study Category\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Template Question\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Example\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Disease onset and progression\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Amongst patients who are newly diagnosed with \textbf{{[}insert the disease of interest{]}}, which patients will go on to have \textbf{{[}another disease or related complication{]}} within \textbf{{[}time horizon from diagnosis{]}}?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Among newly diagnosed \textbf{AFib} patients, which patients will go onto to have \textbf{ischemic stroke} in next \textbf{3 years}? Among newly diagnosed \textbf{Melonoma}, which patients will go onto to have \textbf{brain cancer} in next \textbf{6 months}?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Treatment choice\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Amongst patients with \textbf{{[}indicated disease{]}} who are treated with either \textbf{{[}treatment 1{]}} or \textbf{{[}treatment 2{]}}, which patients were treated with \textbf{{[}treatment 1{]} (on day 0)}?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Among \textbf{AFib} patients who took either \textbf{warfarin} or \textbf{dabigatran}, which patients got warfarin? (as defined for propensity score model)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Treatment response\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Amongst patients who are new users of \textbf{{[}insert the chronically-used drug of interest{]}}, which patients will \textbf{{[}insert desired effect{]}} in \textbf{{[}time window{]}}?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Which patients with \textbf{T2DM} who start \textbf{metformin} stay on \textbf{metformin} after \textbf{3 years}?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Treatment safety\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Amongst patients who are new users of \textbf{{[}insert the drug of interest{]}}, which patients will experience \textbf{{[}insert your favorite known adverse event from the drug profile{]}} within \textbf{{[}time horizon following exposure start{]}}?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Among new users of \textbf{warfarin}, which patients will have \textbf{GI bleeding} in \textbf{1 year}?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Treatment adherence\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Amongst patients who are new users of \textbf{{[}insert the chronically-used drug of interest{]}}, which patients will achieve \textbf{{[}adherence metric threshold{]}} at \textbf{{[}time horizon{]}}?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Which patients with \textbf{T2DM} who start on \textbf{metformin} will achieve \textbf{\textgreater=80\% proportion of days covered} at \textbf{1 year}?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Comparative effectiveness\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
To compare the risk of \textbf{{[}Insert the outcome of interest{]}} between \textbf{{[}Insert the target exposure{]}} and \textbf{{[}Insert the comparator cohort{]}}, we will estimate the population-level effect of exposure on the \textbf{{[}Insert the metric of analysis model here: hazards for Cox/ odds for logistic / rate ratio for Poisson{]}} of the outcome during the period from \textbf{{[}Insert the time-at-risk start: e.g.~1 day after exposure start{]}} to \textbf{{[}Insert the time-at-risk end: e.g.~30 days after exposure end{]}}.\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
To compare the risk of \textbf{angioedema} between new users of \textbf{levetiracetam} and new users of \textbf{phenytoin}, we will estimate the population-level effect of exposure on the \textbf{hazards} of the outcome during the period from \textbf{1 day after exposure start} to \textbf{0 days after exposure end}.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{example-of-a-study-in-ohdsi-speak}{%
\subsection{Example of a Study in OHDSI-speak}\label{example-of-a-study-in-ohdsi-speak}}

You're a researcher interested in studying the effects of ACE inhibitor monotherapy vs.~thiazide diuretic monotherapy on the outcomes of acute myocardial infarction and angioedema as first-line treatment for hypertension. You understand that based on the OHDSI literature, you are asking a population-level effect estimation question but first, you need to do some homework on how to characterize this particular treatment of interest.

\hypertarget{characterization-questions}{%
\subsubsection{Characterization Questions}\label{characterization-questions}}

Acute myocardial infarction is a cardiovascular complication that can occur in patients with high blood pressure, so effective treatment for hypertension should reduce the risk. Angioedema is a known side effect of ACE inhibitors, which is rare but potentially serious. You start by creating {[}{[}cohorts{]}{]} for the exposures of interest (new users of ACE inhibitors and new users of thiazide diuretics). You perform a {[}{[}characterization analysis{]}{]} to summarize baseline characteristics of these exposure populations, including demographics, comorbid conditions, and concomitant medications. You perform another characterization analysis to estimate the incidence of selected outcomes within these exposure populations. Here, you ask `how often does 1) acute myocardial infarction and 2) angioedema occur during the period of exposure to ACE inhibitors and thiazide diuretics?' These characterizations allow us to assess the feasibility of conducting a {[}{[}population-level effect estimation{]}{]}, to evaluate whether the two treatment groups are comparable, and to identify `risk factors' that might predict which treatment choice that patients made.

\hypertarget{population-level-estimation-question}{%
\subsubsection{Population-Level Estimation Question}\label{population-level-estimation-question}}

The population-level effect estimation study estimates the relative risk of ACE inhibitor vs, thiazide use for the outcomes of AMI and angioedema. Here, you further evaluate through study diagnostics and negative controls whether we can produce a reliable estimate of the average treatment effect.

Independent of whether there is a causal effect of the exposures, you are also interested in trying to determine which patients are at highest risk of the outcomes. (This is a patient-level prediction problem). Here, you develop a prediction model that evaluates: amongst the patients who are new users of ACE inhibitors, which patients are at highest risk of developing acute myocardial infarction during the 1 year after starting treatment. The model allows us to predict, for a patient who has just been prescribed ACE for the first time, based on events observed from their medical history, what is the chance that they will experience AMI in the next 1 year.

\hypertarget{more-real-example-questions}{%
\subsection{More real example questions}\label{more-real-example-questions}}

In this section, we provide more real examples of questions as they have been submitted to the community. We have also reframed them to the OHDSI-speak format (if needed) and mapped them to OHDSI analytic frameworks:

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[b]{0.30\columnwidth}\raggedright
Unframed question we've heard from potential researchers\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Reframed Question in OHDSI-speak \emph{(italics denote additions to the original question for clarification)}\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
OHDSI Framework\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Among patients addicted to opioids, what is the proportion of patients taking benzos concurrently?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Amongst opioid-addicted patients, how many patients did concurrently use benzodiazepines \emph{any time over the last 5 years of data}?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Clinical characterization\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Among patients newly diagnosed with cancer, how many received guideline-concordant care?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Amongst newly diagnosed cancer patients, how many patients did receive guideline-concordant care \emph{over the last 5 years of data}?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Clinical characterization\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Among patients diagnosed with pneumonia, who develops ocular retinopathy within 2 years?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Amongst patients who are \emph{newly} diagnosed with pneumonia, which patients will develop ocular retinopathy after 2 years?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Patient-level prediction\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Among Patients with an ICD9/10 for psoriasis over the last 5 years, how many presented with major adverse cardiovascular events (e.g., heart attack, stroke, MI, atrial fibrillation)? What were the red cell distribution width (RDW) values for these patients?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Amongst diagnosed patients with psoriasis over the last 5 years \emph{of data}, how many patients experienced major cardiovascular adverse events \emph{any time during the study period}? Amongst diagnosed patients with psoriasis over the last 5 years of data, what were \emph{max-min range, median, IQR, mean, and SD} of red cell distribution width (RDW) values \emph{any time during the study period}?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Clinical characterization\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
How many patients had a shoulder arthroscopy in the last year?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
How many patients did undergo arthroscopy \emph{procedure} in the last year?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Clinical characterization\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Among patients with neuro-degenerative Parkinson's disease, onset age 65 or older, how many subsequently suffered from brain stroke or dementia?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Amongst patients diagnosed with neurodegenerative Parkinson's disease who were 65 years or older at onset, how many patients experienced brain stroke or dementia \emph{after diagnosis}?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Clinical characterization\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
What are the rates of chemotherapy-induced neutropenia and subsequent chemotherapy withdrawal in patients taking cisplatin?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Amongst patients who take cisplatin, what is the rate of developing chemotherapy-induced neutropenia \emph{per 1,000 patient per year over the last 2 years of data}?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Clinical characterization\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Among outpatients, how many presented with ADHD were taking Ritalin for the last 6 months?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Amongst admitted patients in ambulatory setting \emph{over the last year of data}, how many ADHD patients did use Ritalin within the last 6 months \emph{prior to last visit}?\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Clinical characterization\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{summary}{%
\section{Summary}\label{summary}}

\texttt{**add\ if\ needed**}

\hypertarget{part-uniform-data-representation}{%
\part{Uniform Data Representation}\label{part-uniform-data-representation}}

\hypertarget{CommonDataModel}{%
\chapter{The Common Data Model}\label{CommonDataModel}}

\emph{Chapter lead: Clair Blacketer}

No single observational data source provides a comprehensive view of the clinical data a patient accumulates while receiving healthcare, and therefore none can be sufficient to meet all expected outcome analysis needs. This explains the need for assessing and analyzing multiple data sources concurrently using a common data standard. This standard is provided by the OMOP Common Data Model (CDM). In this chapter we provide an overview of the data model itself, design, conventions, and discussion of select tables.

The CDM is designed to support the conduct of research to identify and evaluate associations between interventions (drug exposure, procedures, healthcare policy changes etc.) and outcomes caused by these interventions (condition occurrences, procedures, drug exposure etc.). Outcomes can be efficacious (benefit) or adverse (safety risk). Often times, specific patient cohorts (e.g., those taking a certain drug or suffering from a certain disease) may be defined for treatments or outcomes, using clinical events (diagnoses, observations, procedures, etc.) that occur in predefined temporal relationships to each other. The CDM, combined with its standardized content (via the Standardized Vocabularies), will ensure that research methods can be systematically applied to produce meaningfully comparable and reproducible results.

An overview of all the tables in the CDM is provided in Figure \ref{fig:cdmDiagram}.

\begin{figure}
\includegraphics[width=1\linewidth]{images/CommonDataModel/cdmDiagram} \caption{Overview of all tables in the CDM version 6.0. Note that not all relationships between tables are shown.}\label{fig:cdmDiagram}
\end{figure}

\hypertarget{design-principles}{%
\section{Design Principles}\label{design-principles}}

The CDM is designed to include all observational health data elements (experiences of the patient receiving health care) that are relevant for analysis use cases to support the generation of reliable scientific evidence about disease natural history, healthcare delivery, effects of medical interventions, the identification of demographic information, health care interventions and outcomes.

Therefore, the CDM is designed to store observational data to allow for research, under the following principles:

\begin{itemize}
\tightlist
\item
  \textbf{Suitability for purpose}: The CDM aims to provide data organized in a way optimal for analysis, rather than for the purpose of addressing the operational needs of health care providers or payers.
\item
  \textbf{Data protection}: All data that might jeopardize the identity and protection of patients, such as names, precise birthdays etc. are limited. Exceptions are possible where the research expressly requires more detailed information, such as precise birth dates for the study of infants.
\item
  \textbf{Design of domains}: The domains are modeled in a person-centric relational data model, where for each record the identity of the person and a date is captured as a minimum. Here, a relational data model is one where the data is represented as a collection of tables linked by primary and foreign keys.
\item
  \textbf{Rationale for domains}: Domains are identified and separately defined in an entity-relationship model if they have an analysis use case (conditions, for example) and the domain has specific attributes that are not otherwise applicable. All other data can be preserved as an observation in the observation table in an entity-attribute-value structure.
\item
  \textbf{Standardized Vocabularies}: To standardize the content of those records, the CDM relies on the Standardized Vocabularies containing all necessary and appropriate corresponding standard healthcare concepts.
\item
  \textbf{Reuse of existing vocabularies}: If possible, these concepts are leveraged from national or industry standardization or vocabulary definition organizations or initiatives, such as the National Library of Medicine, the Department of Veterans' Affairs, the Center of Disease Control and Prevention, etc.
\item
  \textbf{Maintaining source codes}: Even though all codes are mapped to the Standardized Vocabularies, the model also stores the original source code to ensure no information is lost.
\item
  \textbf{Technology neutrality}: The CDM does not require a specific technology. It can be realized in any relational database, such as Oracle, SQL Server etc., or as SAS analytical datasets.
\item
  \textbf{Scalability}: The CDM is optimized for data processing and computational analysis to accommodate data sources that vary in size, including databases with up to hundreds of millions of persons and billions of clinical observations.
\item
  \textbf{Backwards compatibility}: All changes from previous CDMs are clearly delineated in the github repository \href{https://github.com/OHDSI/CommonDataModel}{(https://github.com/OHDSI/CommonDataModel)}. Older versions of the CDM can be easily created from the current version, and no information is lost that was present previously.
\end{itemize}

\hypertarget{data-model-conventions}{%
\section{Data Model Conventions}\label{data-model-conventions}}

There are a number of implicit and explicit conventions that have been adopted in the CDM. Developers of methods that run against the CDM need to understand these conventions.

\hypertarget{model-conv}{%
\subsection{General conventions of the model}\label{model-conv}}

The OMOP CDM is considered a ``person-centric'' model, meaning that the people (or patients) drive the event and observation tables. At a minimum, the tables have a foreign key into the PERSON table and a date. This allows for a longitudinal view on all healthcare-relevant events by person. The exceptions from this rule are the standardized health system data tables, which are linked directly to events of the various domains.

\hypertarget{general-conventions-of-schemas}{%
\subsection{General conventions of schemas}\label{general-conventions-of-schemas}}

New to CDM v6.0 is the concept of schemas. This allows for more separation between read-only and writeable tables. The clinical data, event, and vocabulary tables are in the ``CDM'' schema and are considered read-only to the end user. This means that the tables can be queried but no information can be accidentally removed or written over except by the database administrator. Tables that need to be manipulated by web-based tools or end users have moved to the ``Results'' schema. Currently the only two tables in the ``Results'' schema are COHORT and COHORT\_DEFINITON, though likely more will be added over the course of v6.0 point releases. The COHORT and COHORT\_DEFINITION tables are meant to describe groups of interest that the user might define, as detailed in chapter \ref{Cohorts}. These tables can be written to, meaning that a cohort created in ATLAS or by a user can be stored in the COHORT table and accessed at a later date. This does mean that cohorts in the COHORT table can be manipulated by anyone so it is always recommended that the SQL code used to create the cohort be saved along with the project or analysis in the event it needs to be regenerated.

\hypertarget{general-conventions-of-data-tables}{%
\subsection{General conventions of data tables}\label{general-conventions-of-data-tables}}

The CDM is platform-independent. Data types are defined generically using ANSI SQL data types (VARCHAR, INTEGER, FLOAT, DATE, DATETIME, CLOB). Precision is provided only for VARCHAR. It reflects the minimal required string length and can be expanded within a CDM instantiation. The CDM does not prescribe the date and datetime format. Standard queries against CDM may vary for local instantiations and date/datetime configurations.

In most cases, the first field in each table ends in "\_id", containing a record identifier that can be used as a foreign key in another table. For example, the CONDITION\_OCCURRENCE table contains the field visit\_occurrence\_id which is a foreign key to the VISIT\_OCCURRENCE table where visit\_occurrence\_id is the primary key.

\emph{Note}: While the data model itself is platform independent, many of the tools that have been built to work with it require certain specifications. For more about this please see chapter \ref{OhdsiAnalyticsTools}

\hypertarget{general-conventions-of-domains}{%
\subsection{General conventions of domains}\label{general-conventions-of-domains}}

One of the ways in which the CDM is standardized is by the use of concept domains. Domains refer to the nature of a clinical entity and define the event table or field in an event table where a data record should be stored. This idea is covered fully in chapter \ref{StandardizedVocabularies} but it is important to the understanding of the data model that we touch on them here. All standard concepts in the OMOP vocabularies are organized into 30 domains, as shown in table \ref{tab:domains}. Domains, like concepts, are a major reason why every researcher who uses the OMOP CDM is considered to be ``speaking the same language''. Once source codes from a native database are mapped to standard concepts (see section \ref{concepts-sources}) the concepts themselves dictate which table they belong in by use of domains. In this way, a researcher using the CDM always knows where a record should be located rather than having to guess.

\begin{longtable}[]{@{}ll@{}}
\caption{\label{tab:domains} Number of standard concepts belonging to each domain.}\tabularnewline
\toprule
Concept Count & Domain\_Id\tabularnewline
\midrule
\endfirsthead
\toprule
Concept Count & Domain\_Id\tabularnewline
\midrule
\endhead
1731378 & Drug\tabularnewline
477597 & Device\tabularnewline
257000 & Procedure\tabularnewline
163807 & Condition\tabularnewline
145898 & Observation\tabularnewline
89645 & Measurement\tabularnewline
33759 & Spec Anatomic Site\tabularnewline
17302 & Meas Value\tabularnewline
1799 & Specimen\tabularnewline
1215 & Provider Specialty\tabularnewline
1046 & Unit\tabularnewline
944 & Metadata\tabularnewline
538 & Revenue Code\tabularnewline
336 & Type Concept\tabularnewline
194 & Relationship\tabularnewline
183 & Route\tabularnewline
180 & Currency\tabularnewline
158 & Payer\tabularnewline
123 & Visit\tabularnewline
51 & Cost\tabularnewline
50 & Race\tabularnewline
13 & Plan Stop Reason\tabularnewline
11 & Plan\tabularnewline
6 & Episode\tabularnewline
6 & Sponsor\tabularnewline
5 & Meas Value Operator\tabularnewline
3 & Spec Disease Status\tabularnewline
2 & Gender\tabularnewline
2 & Ethnicity\tabularnewline
1 & Observation Type\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{general-conventions-of-fields}{%
\subsection{General conventions of fields}\label{general-conventions-of-fields}}

Variable names across all tables follow one convention:

\begin{longtable}[]{@{}ll@{}}
\caption{\label{tab:fieldConventions} Field name conventions.}\tabularnewline
\toprule
\begin{minipage}[b]{0.34\columnwidth}\raggedright
Notation\strut
\end{minipage} & \begin{minipage}[b]{0.61\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.34\columnwidth}\raggedright
Notation\strut
\end{minipage} & \begin{minipage}[b]{0.61\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.34\columnwidth}\raggedright
{[}entity{]}\_id\strut
\end{minipage} & \begin{minipage}[t]{0.61\columnwidth}\raggedright
Unique identifiers for key entities, which can serve as foreign keys to establish relationships across entities. For example, person\_id uniquely identifies each individual. visit\_occurrence\_id uniquely identifies a PERSON encounter at a point of care.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
{[}entity{]}\_source\_value\strut
\end{minipage} & \begin{minipage}[t]{0.61\columnwidth}\raggedright
Verbatim information from the source data, typically used in ETL to map to concept\_id, and not to be used by any standard analytics. For example, condition\_source\_value = `787.02' was the ICD-9 code captured as a diagnosis from the administrative claim.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
{[}entity{]}\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.61\columnwidth}\raggedright
Foreign key into the Standardized Vocabularies (i.e.~the standard concept attribute for the corresponding term is true), which serves as the primary basis for all standardized analytics. For example, condition\_concept\_id = \href{http://athena.ohdsi.org/search-terms/terms/31967}{31967} contains the reference value for the SNOMED concept of ``Nausea''.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
{[}entity{]}\_source\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.61\columnwidth}\raggedright
Foreign key into the Standardized Vocabularies representing the concept and terminology used in the source data, when applicable. For example, condition\_source\_concept\_id = \href{http://athena.ohdsi.org/search-terms/terms/45431665}{45431665} denotes the concept of ``Nausea'' in the Read terminology; the analogous condition\_concept\_id is \href{http://athena.ohdsi.org/search-terms/terms/31967}{31967}, since SNOMED-CT is the Standardized Vocabulary for most clinical diagnoses and findings.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
{[}entity{]}\_type\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.61\columnwidth}\raggedright
Delineates the origin of the source information, standardized within the Standardized Vocabularies. For example, drug\_type\_concept\_id can allow analysts to discriminate between ``Pharmacy dispensing'' and ``Prescription written''\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{representation-of-content-through-concepts}{%
\subsection{Representation of content through Concepts}\label{representation-of-content-through-concepts}}

In CDM data tables the content of each record is represented using Concepts. Concepts are stored in event tables with their concept IDs as foreign keys to the CONCEPT table, which contains concepts necessary to describe the healthcare experience of a patient. If a Standard Concept does not exist or cannot be identified, the concept ID 0 is used, representing a non-existing concept or un-mappable source value.

Records in the CONCEPT table contain detailed information about each concept (name, domain, class etc.). Concepts, Concept Relationships, Concept Ancestors and other information relating to Concepts is contained in the tables of the Standardized Vocabularies.

\hypertarget{concepts-sources}{%
\subsection{Difference between Concept IDs and Source Values}\label{concepts-sources}}

Many tables contain equivalent information in multiple places: As a Source Value, a Source Concept and as a Standard Concept.

\begin{itemize}
\tightlist
\item
  \textbf{Source Values} contain the codes from public code systems such as ICD-9-CM, NDC, CPT-4, READ etc. or locally controlled vocabularies (such as F for female and M for male) copied from the source data. Source Values are stored in the {[}entity{]}\_source\_value fields in the data tables.
\item
  \textbf{Concepts} are CDM-specific entities that represent the meaning of a clinical fact. Most concepts are based on code systems used in healthcare (called Source Concepts), while others were created de-novo (concept\_code = ``OMOP generated''). Concepts have unique IDs across all domains.
\item
  \textbf{Source Concepts} are the concepts that represent the code used in the source. Source Concepts are only used for common healthcare code systems, not for OMOP-generated Concepts. Source Concepts are stored in the {[}entity{]}\_source\_concept\_id field in the data tables.
\item
  \textbf{Standard Concepts} are those concepts that are used to define the unique meaning of a clinical entity. For each entity there is one Standard Concept. Standard Concepts are typically drawn from existing public vocabulary sources. Concepts that have the equivalent meaning to a Standard Concept are mapped to the Standard Concept. Standard Concepts are referred to in the {[}entity{]}\_concept\_id field of the data tables.
\end{itemize}

Source Values are only provided for convenience and quality assurance (QA) purposes. Source Values and Source Concepts are optional, while \textbf{Standard Concepts are mandatory}. Source Values may contain information that is only meaningful in the context of a specific data source. This mandatory use of Standard Concepts is what allows all OHDSI collaborators to speak the same language. For example, let's look at the condition ``Pulmonary Tuberculosis'' (TB). Figure \ref{fig:pulmTubICD9} shows that the ICD9CM code for TB is 011.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{images/CommonDataModel/pulmTubICD9} 

}

\caption{ICD9CM code for Pulmonary Tuberculosis}\label{fig:pulmTubICD9}
\end{figure}

Without the use of a standard way to represent TB the code 011 could be interpreted as ``Hospital Inpatient (Including Medicare Part A)'' in the UB04 vocabulary, or as ``Nervous System Neoplasms without Complications, Comorbidities'' in the DRG vocabulary. This is where Concept IDs, both Source and Standard, are valuable. The Concept ID that represents the 011 ICD9CM code is \href{http://athena.ohdsi.org/search-terms/terms/44828631}{44828631}. This differentiates the ICD9CM from the UBO4 and from the DRG. The Standard Concept that ICD9CM code maps to is \href{http://athena.ohdsi.org/search-terms/terms/253954}{253954} as shown in figure \ref{fig:pulmTubMap} by the relationship ``Non-standard to Standard map (OMOP)''. This same mapping relationship exists between Read, ICD10, CIEL, and MeSH codes, among others, so that any research that references the standard SNOMED concept is sure to include all supported source codes.

\begin{figure}
\includegraphics[width=1\linewidth]{images/CommonDataModel/pulmTubMap} \caption{SNOMED code for Pulmonary Tuberculosis}\label{fig:pulmTubMap}
\end{figure}

An example of how the standard concept-source code relationship is depicted in the tables is shown in Table \ref{tab:conditionOccurrence}.

\hypertarget{omop-cdm-standardized-tables}{%
\section{OMOP CDM Standardized Tables}\label{omop-cdm-standardized-tables}}

The OMOP CDM contains 16 Clinical data tables, 10 Vocabulary tables, 2 Metadata tables, 4 Health System data tables, 2 Health Economics data tables, 3 standardized derived elements, and 2 results schema tables. These tables are fully specified in the CDM Wiki\footnote{\url{https://github.com/OHDSI/CommonDataModel/wiki}}.

To illustrate how these tables are used in practice the data of one person will be used as a common thread throughout the rest of the chapter. While part of the CDM the Vocabulary tables are not covered here, rather, they are detailed in depth in Chapter \ref{StandardizedVocabularies}.

\hypertarget{running-example-endometriosis}{%
\subsection{Running Example: Endometriosis}\label{running-example-endometriosis}}

Endometriosis is a painful condition whereby cells normally found in the lining of a woman's uterus occur elsewhere in the body. Severe cases can lead to infertility, bowel, and bladder problems. The following sections will detail one patient's experience with this disease and how her clinical experience might be represented in the Common Data Model.

\begin{center}\includegraphics[width=0.5\linewidth]{images/CommonDataModel/Lauren} \end{center}

\begin{quote}
Every step of this painfull journey I had to convince everyone how much pain I was in.
\end{quote}

Lauren had been experiencing endometriosis symptoms for many year; however, it took a ruptured cyst in her ovary before she was diagnosed. You can read more about Lauren at \url{https://www.endometriosis-uk.org/laurens-story}.

\hypertarget{person}{%
\subsection{PERSON table}\label{person}}

As the Common Data Model is a person-centric model (see section \ref{model-conv}) let's start with how she would be represented in the PERSON table.

\textbf{What do we know about Lauren?}

\begin{itemize}
\tightlist
\item
  She is a 36-year-old woman
\item
  Her birthday is 12-March-1982
\item
  She is white
\item
  She is english
\end{itemize}

With that in mind, her PERSON table might look something like this:

\begin{longtable}[]{@{}lll@{}}
\caption{\label{tab:person} The PERSON table.}\tabularnewline
\toprule
\begin{minipage}[b]{0.28\columnwidth}\raggedright
Column name\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
Value\strut
\end{minipage} & \begin{minipage}[b]{0.49\columnwidth}\raggedright
Explanation\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.28\columnwidth}\raggedright
Column name\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
Value\strut
\end{minipage} & \begin{minipage}[b]{0.49\columnwidth}\raggedright
Explanation\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.28\columnwidth}\raggedright
person\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
person\_id should be an integer, either directly from the source or generated as part of the build process.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
gender\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
8532\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The concept ID referring to female gender is \href{http://athena.ohdsi.org/search-terms/terms/8532}{8532}.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
year\_of\_birth\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
1982\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
month\_of\_birth\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
3\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
day\_of\_birth\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
12\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
birth\_datetime\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
1982-03-12 00:00:00\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
When the time is not known midnight is used.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
death\_datetime\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
race\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
8527\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The concept ID referring to white race is \href{http://athena.ohdsi.org/search-terms/terms/8527}{8527}.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
ethnicity\_concept\_ id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
38003564\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
Typically hispanic status is stored for ethnicity. The concept ID \href{http://athena.ohdsi.org/search-terms/terms/38003564}{38003564} refers to ``Not hispanic''.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
location\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
Her address is not known.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
provider\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
Her primary care provider is not known.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
care\_site\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
Her primary care site is not known.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
person\_source\_value\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
Typically this would be her identifier in the source data, though often is it the same as the person\_id.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
gender\_source\_value\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
F\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The gender value as it appears in the source is stored here.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
gender\_source\_ concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If the gender value in the source was coded using a vocabulary recognized by OHDSI, that concept ID would go here. For example, if her gender was ``Sex-F'' in the source and it was stated to be in the PCORNet vocabulary concept ID \href{http://athena.ohdsi.org/search-terms/terms/44814665}{44814665} would go in this field.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
race\_source\_value\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
white\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The race value as it appears in the source is stored here.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
race\_source\_concept\_ id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
Same principle as gender\_source\_concept\_id.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
ethnicity\_source\_ value\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
english\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The ethnicity value as it appears in the source is stored here.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
ethnicity\_source\_ concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
Same principle as gender\_source\_concept\_id.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{observationPeriod}{%
\subsection{OBSERVATION\_PERIOD table}\label{observationPeriod}}

The OBSERVATION\_PERIOD table is designed to define the amount of time for which a patient's clinical events are recorded in the source system. For US healthcare insurance claims this is typically the enrollment period of the patient. When working with data from electronic health records (EHR) often the first record in the system is considered the observation\_period\_start\_date and the latest record is considered the observation\_period\_end\_date with the understanding that only the clinical events that happened within that particular system were recorded.

\textbf{How can we determine Lauren's observation period?}

Lauren's information as shown in Table \ref{tab:encounters} is most similar to EHR data in that we only have records of her encounters from which to determine her observation period.

\begin{longtable}[]{@{}llll@{}}
\caption{\label{tab:encounters} Lauren's healthcare encounters.}\tabularnewline
\toprule
Encounter ID & Start date & Stop date & Type\tabularnewline
\midrule
\endfirsthead
\toprule
Encounter ID & Start date & Stop date & Type\tabularnewline
\midrule
\endhead
70 & 2010-01-06 & 2010-01-06 & outpatient\tabularnewline
80 & 2011-01-06 & 2011-01-06 & outpatient\tabularnewline
90 & 2012-01-06 & 2012-01-06 & outpatient\tabularnewline
100 & 2013-01-07 & 2013-01-07 & outpatient\tabularnewline
101 & 2013-01-14 & 2013-01-14 & ambulatory\tabularnewline
102 & 2013-01-17 & 2013-01-24 & inpatient\tabularnewline
\bottomrule
\end{longtable}

Based on the encounter records her OBSERVATION\_PERIOD table might look something like this:

\begin{longtable}[]{@{}lll@{}}
\caption{\label{tab:observationPeriod} The OBSERVATION\_PERIOD table.}\tabularnewline
\toprule
\begin{minipage}[b]{0.28\columnwidth}\raggedright
Column name\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
Value\strut
\end{minipage} & \begin{minipage}[b]{0.49\columnwidth}\raggedright
Explanation\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.28\columnwidth}\raggedright
Column name\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
Value\strut
\end{minipage} & \begin{minipage}[b]{0.49\columnwidth}\raggedright
Explanation\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.28\columnwidth}\raggedright
observation\_period\_ id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This is typically an autogenerated field that creates a unique ID number for each record in the table.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
person\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This comes from the PERSON table and links PERSON and OBSERVATION\_PERIOD.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
observation\_period\_ start\_date\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
2010-01-06\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This is the start date of her earliest encounter on record.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
observation\_period\_ end\_date\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
2013-01-24\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This is the end date of her latest encounter on record.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
period\_type\_ concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
44814725\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The best option in the Vocabulary with the concept class ``Obs Period Type'' is \href{http://athena.ohdsi.org/search-terms/terms/44814724}{44814724}, which stands for ``Period covering healthcare encounters''.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{visitOccurrence}{%
\subsection{VISIT\_OCCURRENCE}\label{visitOccurrence}}

The VISIT\_OCCURRENCE table houses information about a patient's encounters with the health care system. Within the OHDSI vernacular these are referred to as visits and are considered to be discreet events. There are 12 categories of visits though the most common are inpatient, outpatient, emergency and long term care.

\textbf{How do we represent Lauren's encounters as visits?}

As an example let's represent the inpatient encounter in Table \ref{tab:encounters} as a record in the VISIT\_OCCURRENCE table.

\begin{longtable}[]{@{}lll@{}}
\caption{\label{tab:visitOccurrence} The VISIT\_OCCURRENCE table.}\tabularnewline
\toprule
\begin{minipage}[b]{0.28\columnwidth}\raggedright
Column name\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
Value\strut
\end{minipage} & \begin{minipage}[b]{0.49\columnwidth}\raggedright
Explanation\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.28\columnwidth}\raggedright
Column name\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
Value\strut
\end{minipage} & \begin{minipage}[b]{0.49\columnwidth}\raggedright
Explanation\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.28\columnwidth}\raggedright
visit\_occurrence\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
514\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This is typically an autogenerated field that creates a unique ID number for each visit on the person's record in the converted CDM database.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
person\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This comes from the PERSON table and links PERSON and VISIT\_OCCURRENCE.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
visit\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
9201\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The concept ID referring to an inpatient visit is \href{http://athena.ohdsi.org/search-terms/terms/9201}{9201}.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
visit\_start\_date\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
2013-01-17\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The start date of the visit.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
visit\_start\_datetime\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
2013-01-17 00:00:00\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The date and time of the visit started. When time is unknown midnight is used.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
visit\_end\_date\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
2013-01-24\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The end date of the visit. If this is a one-day visit the end date should match the start date.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
visit\_end\_datetime\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
2013-01-24 00:00:00\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The date and time of the visit end. If time is unknown midnight is used.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
visit\_type\_concept\_ id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
32034\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This column is intended to provide information about the provenance of the visit record, i.e.~does it come from an insurance claim, hospital billing record, EHR record, etc. For this example the concept ID \href{http://athena.ohdsi.org/search-terms/terms/32035}{32035} (``Visit derived from EHR encounter record'') is used as the encounters are similar to electronic health records\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
provider\_id*\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If the encounter record has a provider associated, the ID for that provider goes in this field. This should be the provider\_id from the PROVIDER table that represents the provider on the encounter.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
care\_site\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If the encounter record has a care site associated, the ID for that care site goes in this field. This should be the care\_site\_id from the CARE\_SITE table that codes for the care site on the encounter.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
visit\_source\_value\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
inpatient\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The visit value as it appears in the source goes here. In this context ``visit'' means outpatient, inpatient, emergency, etc.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
visit\_source\_ concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If the visit value from the source is coded using a vocabulary that is recognized by OHDSI, the concept ID that represents the visit source value would go here.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
admitted\_from\_ concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If known, this is the concept ID that represents where the patient was admitted from. This concept should have the concept class ``Place of Service'' and the domain ``Visit''. For example, if a patient was admitted to the hospital from home, the concept ID would be \href{http://athena.ohdsi.org/search-terms/terms/8536}{8536} (``Home'').\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
admitted\_from\_ source\_ value\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This is the value from the source that represents where the patient was admitted from. Using the above example, this would be ``home''.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
discharge\_to\_ concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If known, this is the concept ID that represents where the patient was discharged to. This concept should have the concept class ``Place of Service'' and the domain ``Visit''. For example, if a patient was released to an assisted living facility, the concept ID would be \href{http://athena.ohdsi.org/search-terms/terms/8615}{8615} (``Assisted Living Facility'').\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
discharge\_to\_source\_ value\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This is the value from the source that represents where the patient was discharged to. Using the above example, this would be ``assisted living facility''.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
preceding\_visit\_ occurrence\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The visit\_occurrence\_id for the visit immediately preceding the current one in time for the patient.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\begin{itemize}
\tightlist
\item
  A patient may interact with multiple health care providers during one visit, as is often the case with inpatient stays. These interactions can be recorded in the VISIT\_DETAIL table. While not covered in depth in this chapter, you can read more about the VISIT\_DETAIL table in the \href{https://github.com/OHDSI/CommonDataModel/wiki/VISIT_DETAIL}{CDM wiki}.
\end{itemize}

\hypertarget{conditionOccurrence}{%
\subsection{CONDITION\_OCCURRENCE}\label{conditionOccurrence}}

Records in the CONDITION\_OCCURRENCE table are diagnoses, signs, or symptoms of a condition either observed by a Provider or reported by the patient.

\textbf{What are Lauren's conditions?}

Revisiting her account she says:

\begin{quote}
About 3 years ago I noticed my periods, which had also been painful, were getting increasingly more painful. I started becoming aware of a sharp jabbing pain right by my colon and feeling tender and bloated around my tailbone and lower pelvis area. My periods had become so painful that I was missing 1-2 days of work a month. Painkillers sometimes dulled the pain, but usually they didn't do much.
\end{quote}

The SNOMED code for painful menstruation cramps, otherwise known as dysmenorrhea, is 266599000. Table \ref{tab:conditionOccurrence} shows how that would be represented in the CONDITION\_OCCURRENCE table:

\begin{longtable}[]{@{}lll@{}}
\caption{\label{tab:conditionOccurrence} The CONDITION\_OCCURRENCE table.}\tabularnewline
\toprule
\begin{minipage}[b]{0.28\columnwidth}\raggedright
Column name\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
Value\strut
\end{minipage} & \begin{minipage}[b]{0.49\columnwidth}\raggedright
Explanation\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.28\columnwidth}\raggedright
Column name\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
Value\strut
\end{minipage} & \begin{minipage}[b]{0.49\columnwidth}\raggedright
Explanation\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.28\columnwidth}\raggedright
condition\_ occurrence\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
964\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This is typically an autogenerated field that creates a unique ID number for each condition on the person's record in the converted CDM database.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
person\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This comes from the PERSON table and links PERSON and CONDITION\_OCCURRENCE.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
condition\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
194696\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The concept ID that represents the SNOMED code 266599000 is \href{http://athena.ohdsi.org/search-terms/terms/194696}{194696}.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
condition\_start\_date\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
2010-01-06\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The date when the instance of the Condition is recorded.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
condition\_start\_ datetime\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
2010-01-06 00:00:00\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The date and time when the instance of the Condition is recorded. Midnight is used when the time is unknown\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
condition\_end\_date\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If known, this is the date when the instance of the Condition is considered to have ended.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
condition\_end\_ datetime\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If known, this is the date and time when the instance of the Condition is considered to have ended.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
condition\_type\_ concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
32020\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This column is intended to provide information about the provenance of the condition, i.e.~does it come from an insurance claim, hospital billing record, EHR record, etc. For this example the concept ID \href{http://athena.ohdsi.org/search-terms/terms/32020}{32020} (``EHR encounter diagnosis'') is used as the encounters are similar to electronic health records. Concept IDs in this field should be in the ``Condition Type'' vocabulary.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
condition\_status\_ concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If known, the this represents when and/or how the condition was diagnosed. For example, a condition could be an admitting diagnosis, in which case the concept ID \href{http://athena.ohdsi.org/search-terms/terms/4203942}{4203942} would be used.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
stop\_reason\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If known, the reason that the Condition was no longer present, as indicated in the source data.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
provider\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If the condition record has a diagnosing provider listed, the ID for that provider goes in this field. This should be the provider\_id from the PROVIDER table that represents the provider on the encounter.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
visit\_occurrence\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
509\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If known, this is the visit (represented as visit\_occurrence\_id taken from the VISIT\_OCCURRENCE table) during which the condition was diagnosed.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
visit\_detail\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If known, this is the visit detail encounter (represented as VISIT\_DETAIL\_ID from the VISIT\_DETAIL table) during which the condition was diagnosed.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
condition\_source\_ value\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
266599000\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This is the value from the source that represents the condition. In Lauren's case of dysmenorrhea the SNOMED code for that condition is stored here and the standard concept ID mapped from that code is stored in condition\_concept\_id.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
condition\_source\_ concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
194696\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If the condition value from the source is coded using a vocabulary that is recognized by OHDSI, the concept ID that represents that value would go here. In the example of dysmennorhea the source value is a SNOMED code so the concept ID that represents that code is 194696. In this case it is the same as the condition\_concept\_id since the SNOMED vocabulary is the standard condition vocabulary.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
condition\_status\_ source\_ value\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If the condition status value from the source is coded using a vocabulary that is recognized by OHDSI, the concept ID that represents that source value would go here.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{drugExposure}{%
\subsection{DRUG\_EXPOSURE}\label{drugExposure}}

The DRUG\_EXPOSURE table captures records about the utilization of a drug when ingested or otherwise introduced into the body. Drugs include prescription and over-the-counter medicines, vaccines, and large-molecule biologic therapies. Radiological devices ingested or applied locally do not count as Drugs.

Drug exposures are inferred from clinical events associated with orders, prescriptions written, pharmacy dispensings, procedural administrations, and other patient-reported information.

\textbf{What are Lauren's drug exposures?}

We know that Lauren was given 60 acetaminophen 325mg oral tablets for 30 days (NDC code 69842087651) at her visit on 2010-01-06 to help with her dysmenorrhea pain. Here's how that might look in the DRUG\_EXPOSURE table:

\begin{longtable}[]{@{}lll@{}}
\caption{\label{tab:drugExposure} The DRUG\_EXPOSURE table.}\tabularnewline
\toprule
\begin{minipage}[b]{0.28\columnwidth}\raggedright
Column name\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
Value\strut
\end{minipage} & \begin{minipage}[b]{0.49\columnwidth}\raggedright
Explanation\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.28\columnwidth}\raggedright
Column name\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
Value\strut
\end{minipage} & \begin{minipage}[b]{0.49\columnwidth}\raggedright
Explanation\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.28\columnwidth}\raggedright
drug\_exposure\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
1001\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This is typically an autogenerated field that creates a unique ID number for each drug exposure on the person's record in the converted CDM database.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
person\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This comes from the PERSON table and links PERSON and DRUG\_EXPOSURE.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
drug\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
1127433\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The NDC code for acetaminophen maps to the RxNorm code 313782 which is represented by the concept ID \href{http://athena.ohdsi.org/search-terms/terms/1127433}{1127433}.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
drug\_exposure\_start\_ date\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
2010-01-06\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The start date of the drug exposure\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
drug\_exposure\_start\_ datetime\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
2010-01-06 00:00:00\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The start date and time of the drug exposure. Midnight is used when the time is not known.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
drug\_exposure\_end\_ date\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
2010-02-05\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The end date of the drug exposure. Depending on different sources, it could be a known or an inferred date and denotes the last day at which the patient was still exposed to the drug. In this case the end is inferred since we know Lauren had a 30 days supply.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
drug\_exposure\_end\_ datetime\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
2010-02-05 00:00:00\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The end date and time of the drug exposure. Similar rules apply as to drug\_exposure\_end\_date. Midnight is used when time is unknown\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
verbatim\_end\_date\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If the source provides an end date rather than just days supply that date goes here.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
drug\_type\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
38000177\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This column is intended to provide information about the provenance of the drug, i.e.~does it come from an insurance claim, prescription record, etc. For this example the concept ID \href{http://athena.ohdsi.org/search-terms/terms/38000177}{38000177} (``Prescription written'') is used as the drug record is from a written prescription. Concept IDs in this field should be in the ``Drug Type'' vocabulary.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
stop\_reason\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The reason the drug was stopped. Reasons include regimen completed, changed, removed, etc.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
refills\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The number of refills after the initial prescription. The initial prescription is not counted, values start with null. In the case of Lauren's acetaminophen she did not have any refills so the value is NULL.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
quantity\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
60\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The quantity of drug as recorded in the original prescription or dispensing record.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
days\_supply\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
30\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The number of days of supply of the medication as prescribed.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
sig\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The directions (`signetur') on the Drug prescription as recorded in the original prescription (and printed on the container) or dispensing record.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
route\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
4132161\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This concept is meant to represent the route of the drug the patient was was exposed to. Lauren took her acetaminophen orally so the concept ID \href{http://athena.ohdsi.org/search-terms/terms/4132161}{4132161} (``Oral'') is used.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
lot\_number\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
An identifier assigned to a particular quantity or lot of drug product from the manufacturer.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
provider\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If the drug record has a prescribing provider listed, the ID for that provider goes in this field. This should be the PROVIDER\_ID from the PROVIDER table that represents the provider on the encounter.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
visit\_occurrence\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
509\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If known, this is the visit (represented as visit\_occurrence\_id taken from the VISIT\_OCCURRENCE table) during which the drug was prescribed.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
visit\_detail\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If known, this is the visit detail (represented as visit\_detail\_id taken from the VISIT\_DETAIL table) during which the drug was prescribed.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
drug\_source\_value\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
69842087651\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This is the source code for the drug as it appears in the source data. In Lauren's case she was prescribed acetaminophen and the NDC code is stored here.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
drug\_source\_concept\_ id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
750264\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This is the concept ID that represents the drug source value. In this example the concept ID is \href{http://athena.ohdsi.org/search-terms/terms/750264}{750264}, the NDC code for ``Acetaminophen 325 MG Oral Tablet''.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
route\_source\_value\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The information about the route of administration as detailed in the source.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
dose\_unit\_source\_ value\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The information about the dose unit as detailed in the source.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{procedureOccurrence}{%
\subsection{PROCEDURE\_OCCURRENCE}\label{procedureOccurrence}}

The PROCEDURE\_OCCURRENCE table contains records of activities or processes ordered by, or carried out by, a healthcare provider on the patient to have a diagnostic or therapeutic purpose. Procedures are present in various data sources in different forms with varying levels of standardization. For example:

\begin{itemize}
\tightlist
\item
  Medical Claims include procedure codes that are submitted as part of a claim for health services rendered, including procedures performed.
\item
  Electronic Health Records that capture procedures as orders.
\end{itemize}

\textbf{What procedures did Lauren have?}
From her description we know she had a ultrasound of her left ovary on 2013-01-14 that showed a 4x5cm cyst. Here's how that would look in the PROCEDURE\_OCCURRENCE table:

\begin{longtable}[]{@{}lll@{}}
\caption{\label{tab:procedureOccurrence} The PROCEDURE\_OCCURRENCE table.}\tabularnewline
\toprule
\begin{minipage}[b]{0.28\columnwidth}\raggedright
Column name\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
Value\strut
\end{minipage} & \begin{minipage}[b]{0.49\columnwidth}\raggedright
Explanation\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.28\columnwidth}\raggedright
Column name\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
Value\strut
\end{minipage} & \begin{minipage}[b]{0.49\columnwidth}\raggedright
Explanation\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.28\columnwidth}\raggedright
procedure\_ occurrence\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
1277\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This is typically an autogenerated field that creates a unique ID number for each procedure occurrence on the person's record in the converted CDM database.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
person\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This comes from the PERSON table and links PERSON and PROCEDURE\_OCCURRENCE\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
procedure\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
4127451\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The SNOMED procedure code for a pelvic ultrasound is 304435002 which is represented by the concept ID \href{http://athena.ohdsi.org/search-terms/terms/4127451}{4127451}.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
procedure\_date\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
2013-01-14\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The date on which the procedure was performed.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
procedure\_datetime\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
2013-01-14 00:00:00\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The date and time on which the procedure was performed. Midnight is used when time is unknown.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
procedure\_type\_ concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
38000275\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This column is intended to provide information about the provenance of the procedure, i.e.~does it come from an insurance claim, EHR order, etc. For this example the concept ID \href{http://athena.ohdsi.org/search-terms/terms/38000275}{38000275} (``EHR order list entry'') is used as the procedure record is from an EHR record. Concept IDs in this field should be in the ``Procedure Type'' vocabulary.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
modifier\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This is meant for a concept ID representing the modifier on the procedure. For example, if the record indicated that a CPT4 procedure was performed bilaterally then the concept ID \href{http://athena.ohdsi.org/search-terms/terms/42739579}{42739579} (``Bilateral procedure'') would be used.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
quantity\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The quantity of procedures ordered or administered.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
provider\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If the procedure record has a provider listed, the ID for that provider goes in this field. This should be the provider\_id from the PROVIDER table that represents the provider on the encounter.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
visit\_occurrence\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
740\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If known, this is the visit (represented as visit\_occurrence\_id taken from the VISIT\_OCCURRENCE table) during which the procedure was performed.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
visit\_detail\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
If known, this is the visit detail (represented as visit\_detail\_id taken from the VISIT\_DETAIL table) during which the procedure was performed.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
procedure\_source\_ value\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
304435002\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The source code for the procedure as it appears in the source data. This code is mapped to a standard procedure Concept in the Standardized Vocabularies and the original code is, stored here for reference.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
procedure\_source\_ concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
4127451\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
This is the concept ID that represents the procedure source value.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
modifier\_source\_ value\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
NULL\strut
\end{minipage} & \begin{minipage}[t]{0.49\columnwidth}\raggedright
The source code for the modifier as it appears in the source data.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{additional-information}{%
\section{Additional Information}\label{additional-information}}

This chapter covers only a portion of the tables available in the OMOP CDM as examples of how data is represented. You are encouraged to visit the wiki site \url{https://github.com/OHDSI/CommonDataModel/wiki} for more information.

\hypertarget{summary-1}{%
\section{Summary}\label{summary-1}}

\BeginKnitrBlock{rmdsummary}
\begin{itemize}
\item
  The OMOP CDM is designed to support the conduct of research to identify and evaluate associations between interventions and outcomes
\item
  The OMOP CDM is a ``person-centric'' model
\item
  Source codes are represented as standard concept ids
\end{itemize}
\EndKnitrBlock{rmdsummary}

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

TODO

\hypertarget{StandardizedVocabularies}{%
\chapter{Standardized Vocabularies}\label{StandardizedVocabularies}}

The OMOP Standardized Vocabulary: Christian's (almost) finished paper + \url{http://www.ohdsi.org/web/wiki/doku.php?id=documentation:vocabulary}

\hypertarget{ExtractTransformLoad}{%
\chapter{Extract Transform Load}\label{ExtractTransformLoad}}

\emph{Chapter lead: Clair Blacketer}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

In order to get from the native/raw data to the OMOP CDM an extract, transform, and load (ETL) process has been designed and developed. This process consists of four major steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data experts and CDM experts together design the ETL
\item
  People with medical knowledge create the code mappings
\item
  A technical person implements the ETL
\item
  All are involved in quality control
\end{enumerate}

There are tools available that have been developed by the community and this chapter will cover the steps in the process and the three tools available to facilitate those steps.

\textbf{Chapter Objectives:}

\begin{itemize}
\tightlist
\item
  Examine best practices around designing an ETL specification
\item
  Introduce community tools available for facilitating the ETL process
\item
  Discuss CDM and ETL maintenance
\end{itemize}

\hypertarget{etl-step-1---data-experts-and-cdm-experts-together-design-the-etl}{%
\section{ETL Step 1 - Data experts and CDM experts together design the ETL}\label{etl-step-1---data-experts-and-cdm-experts-together-design-the-etl}}

\hypertarget{white-rabbit}{%
\subsection{White Rabbit}\label{white-rabbit}}

\textbf{Description}

To initiate an ETL process on a database you need to understand your data, including the tables, fields, and content. This is where the White Rabbit tool comes in; it scans your data and creates a report containing all the information necessary to begin writing ETL logic.

\textbf{Scope and Purpose}

WhiteRabbit's main function is to perform a scan of the source data, providing detailed information on the tables, fields, and values that appear in a field. The source data can be in comma-separated text files, or in a database (MySQL, SQL Server, Oracle, PostgreSQL, Microsoft APS, Microsoft Access, Amazon RedShift). The resulting scan will generate a report that can be used as a reference when designing the ETL, for instance by using it in conjunction with the Rabbit-In-a-Hat tool. White Rabbit differs from standard data profiling tools in that it attempts to prevent the display of personally identifiable information (PII) data values in the generated output data file.

\textbf{Process Overview}

The typical sequence for using the software to scan source data in preparation of developing an ETL into an OMOP CDM:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Set working folder, the location on the local desktop computer where results will be exported.
\item
  Connect to the source database or CSV text file and test connection.
\item
  Select the tables of interest for the scan and scan the tables.
\item
  WhiteRabbit creates an export of information about the source data.
\end{enumerate}

\textbf{Installation and support}

All source code and installation instructions available on GitHub: \url{https://github.com/OHDSI/WhiteRabbit}

Additional information available on the OHDSI wiki: \url{http://www.ohdsi.org/web/wiki/doku.php?id=documentation:software:whiterabbit}

\textbf{Setting a Working Folder}

After downloading and installing the WhiteRabbit application, the first thing you need to do is set a working folder. Any files that WhiteRabbit creates will be exported to this local folder. Use the ``Pick Folder'' button to navigate in your local environment where you would like the scan document to go.

\begin{figure}
\includegraphics[width=1\linewidth]{images/ExtractTransformLoad/WhiteRabbitLocation} \caption{The "Pick Folder" button allows the specification of a working folder for the WhiteRabbit application}\label{fig:WhiteRabbitLocation}
\end{figure}

\textbf{Connection to a Database}

WhiteRabbit supports delimited text files, Oracle, Sql Server, MySQL, and PostgreSQL. More detailed information for how to connect can be found on the wiki.

\textbf{Scanning the Tables in a Database}

After connecting to a database, you can scan the tables contained therein. A scan generates a report containing information on the source data that can be used to help design the ETL. Using the Scan tab in WhiteRabbit you can either select individual tables in the selected source database by clicking on `Add' (Ctrl + mouse click), or automatically select all tables in the database by clicking on `Add all in DB'.

\includegraphics[width=1\linewidth]{images/ExtractTransformLoad/WhiteRabbitAddTables}

There are a few setting options as well with the scan:

\begin{itemize}
\tightlist
\item
  Checking the `Scan field values' tells WhiteRabbit that you would like to investigate raw data items within tables selected for a scan (i.e.~if you select Table A, WhiteRabbit will review the contents in each column in Table A).
\item
  `Min cell count' is an option when scanning field values. By default this is set to 5, meaning values in the source data that appear less than 5 times will not appear in the report.
\item
  `Rows per table' is an option when scanning field values. By default, WhiteRabbit will random 1 million rows in the table. There are other options to review 100,000 or all rows within the table.
\item
  Unchecking the `Scan field values' tells WhiteRabbit to not review or report on any of the raw data items.
\item
  Once all settings are completed, press the ``Scan tables'' button. After the scan is completed the report will be written to the working folder.
\end{itemize}

\begin{figure}
\includegraphics[width=1\linewidth]{images/ExtractTransformLoad/WhiteRabbitScanSettings} \caption{WhiteRabbit scan settings}\label{fig:WhiteRabbitScanSettings}
\end{figure}

\textbf{Interpreting the Scan Report}

Once the scan is complete, an excel file is generated in the selected folder with one tab present for each table scanned as well as an overview tab. The overview tab lists all tables scanned, each field in each table, the data type of each field, the maximum length of the field, the number of rows in the table, the number of rows scanned, and how often each field was found to be empty.

\begin{figure}
\includegraphics[width=1\linewidth]{images/ExtractTransformLoad/ScanOverviewTab} \caption{Example overview tab from a scan report}\label{fig:ScanOverviewTab}
\end{figure}

The tabs for each of the tables, for example the conditions table in the raw\_synthea database, show each field, the values in each field, and the frequency of each value.

\begin{figure}
\includegraphics[width=1\linewidth]{images/ExtractTransformLoad/ScanConditionsTab} \caption{Example tab from a scan report}\label{fig:ScanConditionsTab}
\end{figure}

\hypertarget{rabbit-in-a-hat}{%
\subsection{Rabbit-In-a-Hat}\label{rabbit-in-a-hat}}

Rabbit-In-a-Hat comes with WhiteRabbit and is designed to read and display a WhiteRabbit scan document. WhiteRabbit generates information about the source data while Rabbit-In-a-Hat uses that information and through a graphical user interface to allow a user to connect source data to tables and columns within the CDM. Rabbit-In-a-Hat generates documentation for the ETL process it does not generate code to create an ETL.

Similar to WhiteRabbit, installation information can be found on the github\footnote{\url{https://github.com/OHDSI/WhiteRabbit}} and information about the different options available in the application can be found on the wiki\footnote{\url{http://www.ohdsi.org/web/wiki/doku.php?id=documentation:software:whiterabbit}}.

\textbf{Writing ETL Logic}

Once you have opened your WhiteRabbit scan report in Rabbit-In-a-Hat you are ready to begin designing and writing the logic for how to convert the source data to the OMOP CDM. As an example, the next few sections will depict how some of the tables in the Synthea\textsuperscript{TM}\footnote{Synthea\textsuperscript{T}M is a patient generator that aims to model real patients. Data are created based on parameters passed to the application.The structure of the data can be found here: \url{https://github.com/synthetichealth/synthea/wiki}.} database might look during conversion.

\textbf{General Flow of an ETL}

Since the OMOP CDM is a person-centric model it is always a good idea to start mapping the PERSON table first. Every clinical event table (CONDITION\_OCCURRENCE, DRUG\_EXPOSURE, PROCEDURE\_OCCURRENCE, etc.) refers back to the PERSON table by way of the person\_id so working out the logic for the PERSON table first makes it easier later on. After the PERSON table a good rule of thumb is to convert the OBSERVATION\_PERIOD table next. Each person in a CDM database should have at least one OBSERVATION\_PERIOD and, generally, most events for a person fall within this timeframe. Once the PERSON and OBSERVATION\_PERIOD tables are done the dimensional tables like PROVIDER, CARE\_SITE, and LOCATION are typically next. The final table logic that should be worked out prior to the clincal tables is VISIT\_OCCURRENCE. Often this is the most complicated logic in the entire ETL and it is some of the most crucial since most events that occur during the course of a person's patient journey will happen during visits. Once those tables are finished it is your choice which CDM tables to map and in which order.

\begin{figure}
\includegraphics[width=1\linewidth]{images/ExtractTransformLoad/flowOfEtl} \caption{General flow of an ETL and which tables to map first}\label{fig:etlFlow}
\end{figure}

\emph{Note}

It is often the case that, during CDM conversion, you will need to make provisions for intermediate tables. This could be for assigning the correct visit\_occurrence\_ids to events, or for mapping source codes to standard concepts (doing this step on the fly is often very slow). This is 100\% allowed and encouraged. What is discouraged is the persistence and reliance on these tables once the conversion is complete.

\hypertarget{mapping-example-person-table}{%
\subsubsection{Mapping Example: Person table}\label{mapping-example-person-table}}

The Synthea data structure contains 20 columns in the patients table (\url{https://github.com/synthetichealth/synthea/wiki/CSV-File-Data-Dictionary\#patients}) but not all were needed to populate the PERSON table, as seen in figure \ref{fig:syntheaPerson}. This is very common and should not be cause for alarm. In this example many of the data points in the Synthea patients table that were not used in the CDM PERSON table were additional identifiers like patient name, driver's license number, and passport number.

\begin{figure}
\includegraphics[width=1\linewidth]{images/ExtractTransformLoad/syntheaPersonTable} \caption{Mapping of Synthea Patients table to CDM PERSON table.}\label{fig:syntheaPerson}
\end{figure}

The table below shows the logic that was imposed on the Synthea patients table to convert it to the CDM PERSON table. The `Comment field' column gives explanations for why the logic was chosen.

\begin{longtable}[]{@{}llll@{}}
\caption{\label{tab:syntheaEtlPerson} ETL logic to convert the Synthea Patients table to CDM PERSON table.}\tabularnewline
\toprule
\begin{minipage}[b]{0.24\columnwidth}\raggedright
Destination Field\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\raggedright
Source field\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedright
Logic\strut
\end{minipage} & \begin{minipage}[b]{0.42\columnwidth}\raggedright
Comment field\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.24\columnwidth}\raggedright
Destination Field\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\raggedright
Source field\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedright
Logic\strut
\end{minipage} & \begin{minipage}[b]{0.42\columnwidth}\raggedright
Comment field\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.24\columnwidth}\raggedright
person\_id\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
Autogenerate\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
The person\_id will be generated at the time of implementation. This is because the id value from the source is a varchar value while the person\_id is an integer. The id field from the source is set as the person\_source\_value to preserve that value and allow for error-checking if necessary.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
gender\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
gender\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
When gender = `M' then set gender\_concept\_id to 8507, when gender = `F' then set to 8532. Drop any rows with missing/unknown gender.\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
These two concepts were chosen as they are the only two standard concepts in the gender domain\footnote{\url{http://athena.ohdsi.org/search-terms/terms?domain=Gender\&standardConcept=Standard\&page=1\&pageSize=15\&query=}}. The choice to drop patients with unknown genders tends to be site-based, though it is recommended they are removed as people without a gender are excluded from analyses.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
year\_of\_birth\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
birthdate\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
Take year from birthdate\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
month\_of\_birth\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
birthdate\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
Take month from birthdate\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
day\_of\_birth\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
birthdate\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
Take day from birthdate\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
birth\_datetime\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
birthdate\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
With midnight as time 00:00:00\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
Here, the source did not supply a time of birth so the choice was made to set it at midnight.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
race\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
race\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
When race = `WHITE' then set as 8527, when race = `BLACK' then set as 8516, when race = `ASIAN' then set as 8515, otherwise set as 0\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
These concepts were chosen because they are the standard concepts belonging to the race domain that most closely align with the race categories in the source\footnote{\url{http://athena.ohdsi.org/search-terms/terms?domain=Race\&standardConcept=Standard\&page=1\&pageSize=15\&query=}}.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
ethnicity\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
race ethnicity\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
When race = `HISPANIC', or when ethnicity in (`CENTRAL\_AMERICAN', `DOMINICAN', `MEXICAN', `PUERTO\_RICAN', `SOUTH\_AMERICAN') then set as 38003563, otherwise set as 0\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
This is a good example of how multiple source columns can contribute to one CDM column. In the CDM ethnicity is represented as either hispanic or not hispanice so values from both the source column race and source column ethnicity will determine this value.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
location\_id\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
provider\_id\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
care\_site\_id\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
person\_source\_value\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
id\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
gender\_source\_value\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
gender\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
gender\_source\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
race\_source\_value\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
race\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
race\_source\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
ethnicity\_source\_value\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
ethnicity\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
In this case the ethnicity\_source\_value will have more granularity than the ethnicity\_concept\_id.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
ethnicity\_source\_concept\_id\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{etl-step-2---people-with-medical-knowledge-create-the-code-mappings}{%
\section{ETL Step 2 - People with medical knowledge create the code mappings}\label{etl-step-2---people-with-medical-knowledge-create-the-code-mappings}}

\hypertarget{etl-step-3---a-technical-person-implements-the-etl}{%
\section{ETL Step 3 - A technical person implements the ETL}\label{etl-step-3---a-technical-person-implements-the-etl}}

\hypertarget{etl-step-4---all-are-involved-in-quality-control}{%
\section{ETL Step 4 - All are involved in quality control}\label{etl-step-4---all-are-involved-in-quality-control}}

Business Rules and Conventions: From the CDM Wiki + Themis

Conversion to OMOP CDM (ETL - Extract, Transform, Load): \url{http://www.ohdsi.org/web/wiki/doku.php?id=documentation:etl_best_practices}

\begin{itemize}
\tightlist
\item
  WhiteRabbit and Rabbit-in-a-Hat: \url{http://www.ohdsi.org/web/wiki/doku.php?id=documentation:software:whiterabbit}
\item
  Usagi: \url{http://www.ohdsi.org/web/wiki/doku.php?id=documentation:software:usagi}
\item
  Achilles: \url{http://www.ohdsi.org/web/wiki/doku.php?id=documentation:software:achilles}
\item
  Athena: \url{http://www.ohdsi.org/web/wiki/doku.php?id=documentation:vocabulary_etl}
\end{itemize}

Mapping and QA of codes to Standard Concepts

\begin{itemize}
\tightlist
\item
  Mapping codes locally versus through the OHDSI Standard Vocabularies
\item
  Usagi
\item
  Systematic mapping of Drug codes
\item
  Systematic mapping of Condition codes
\item
  Systematic mapping of Procedure codes
\item
  Systematic mapping of other codes
\end{itemize}

\hypertarget{part-data-analytics}{%
\part{Data Analytics}\label{part-data-analytics}}

\hypertarget{DataAnalyticsUseCases}{%
\chapter{Data Analytics Use Cases}\label{DataAnalyticsUseCases}}

\emph{Chapter lead: David Madigan}

The OHDSI collaboration focuses on generating reliable evidence from real-world healthcare data, typically in the form of claims databases or electronic health record databases. The use cases that OHDSI focuses on fall into three major categories:

\begin{itemize}
\tightlist
\item
  Characterization
\item
  Population-level estimation
\item
  Patient-level prediction
\end{itemize}

We describe these in detail below. Note, for all the use cases, the evidence we generate inherits the limitations of the data; we discuss these limitations at length in the book section on Evidence Quality (Chapters \ref{EvidenceQuality} - \ref{MethodValidity})

\hypertarget{characterization}{%
\section{Characterization}\label{characterization}}

Characterization attempts to answer the question

\begin{quote}
What happened to them?
\end{quote}

We can use the data to provide answers to questions about the characteristics of the persons in a cohort or the entire database, the practice of healthcare, and study how these things change over time.

The data can provide answers to questions like:

\begin{itemize}
\tightlist
\item
  For patients newly diagnosed with atrial fibrillation, how many receive a prescription for warfarin?
\item
  What is the average age of patients who undergo hip arthroplasty?
\item
  What is the incidence rate of pneumonia in patients over 65 years old?
\end{itemize}

\hypertarget{population-level-estimation}{%
\section{Population-level estimation}\label{population-level-estimation}}

To a limited extent, the data can support causal inferences about the effects of healthcare interventions, answering the question

\begin{quote}
What are the causal effects?
\end{quote}

We would like to understand causal effects to understand consequences of actions. For example, if we decide to take some treatment, how does that change what happens to us in the future?

The data can provide answers to questions like:

\begin{itemize}
\tightlist
\item
  For patients newly diagnosed with atrial fibrillation, in the first year after therapy initiation, does warfarin cause more major bleeds than dabigatran?
\item
  Does the causal effect of metformin on diarrhea vary by age?
\end{itemize}

\hypertarget{patient-level-prediction}{%
\section{Patient-Level prediction}\label{patient-level-prediction}}

Based on the collected patient health histories in the database, we can make patient-level predictions about future health events, answering the question

\begin{quote}
What will happen to me?
\end{quote}

The data can provide answers to questions like:

\begin{itemize}
\tightlist
\item
  For a specific patient newly diagnosed with major depressive disorder, what is the probability the patient will attempt suicide in the first year following diagnosis?
\item
  For a specific patient newly diagnosed with atrial fibrillation, in the first year after therapy initiation with warfarin, what is the probability the patient suffers an ischemic stroke?
\end{itemize}

Population-level estimation and patient-level prediction overlap to a certain extent. For example, an important use case for prediction is to predict an outcome for a specific patient had drug A been prescribed and also predict the same outcome had drug B been prescribed. Let's assume that in reality only one of these drugs is prescribed (say drug A) so we get to see whether the outcome following treatment with A actually occurs. Since drug B was not prescribed, the outcome following treatment B, while predictable, is ``counterfactual'' since it is not ever observed. Each of these prediction tasks falls under patient-level prediction. However, the difference between (or ratio of) the two outcomes is a unit-level \emph{causal} effect, and should be estimated using causal effect estimation methods instead.

\BeginKnitrBlock{rmdimportant}
People have a natural tendency to erroneously interpret predictive models as if they are causal models. But a predictive model can only show correlation, never causation. For example, diabetic drug use might be a strong predictor for myocardial infarction (MI) because diabetes is a strong risk factor for MI. However, that does not mean that stopping the diabetic drugs will prevent MI!
\EndKnitrBlock{rmdimportant}

\hypertarget{limitations-of-observational-research}{%
\section{Limitations of observational research}\label{limitations-of-observational-research}}

There are many important healthcare questions for which OHDSI databases cannot provide answers. These include:

\begin{itemize}
\tightlist
\item
  Causal effects of interventions compared to placebo. Sometimes it is possible to consider the causal effect of a treatment as compared with non-treatment but not placebo treatment.
\item
  Anything related to over-the-counter medications.
\item
  Many outcomes and other variables are sparsely recorded if at all. These include mortality, behavioral outcomes, lifestyle, and socioeconomic status.
\item
  Since patients tend to encounter the healthcare system only when they are unwell, measurement of the benefits of treatments can prove elusive.
\end{itemize}

\hypertarget{missing-data}{%
\subsection{Missing data}\label{missing-data}}

Missingness in OHDSI databases presents subtle challenges. A health event (e.g., prescription, laboratory value, etc.) that should be recorded in a database, but isn't, is ``missing.'' The statistics literature distinguishes between types of missingness such as ``missing completely at random,'' ``missing at random'', and ``missing not at random'' and methods of increasing complexity attempt to address these types. \citet{perkins2017principled} provide a useful introduction to this topic.

\hypertarget{summary-2}{%
\section{Summary}\label{summary-2}}

\BeginKnitrBlock{rmdsummary}
\begin{itemize}
\item
  In observational research we distinguish three large categories of uses cases.
\item
  \textbf{Characterization aims} to answer the questions ``What happened to them?''
\item
  \textbf{Population-level estimation} attempts to answer the question ``What are the causal effects?''
\item
  \textbf{Patient-level prediction} tries to answer ``What will happen to me?''
\item
  Prediction models are not causal models; There is no reason to believe that intervening on a strong predictor will impact the outcome.
\end{itemize}
\EndKnitrBlock{rmdsummary}

\hypertarget{OhdsiAnalyticsTools}{%
\chapter{OHDSI Analytics Tools}\label{OhdsiAnalyticsTools}}

\emph{Chapter leads: Martijn Schuemie \& Frank DeFalco}

OHDSI offers a wide range of open source tools to support the various data-analytics use cases. What these tools have in common is that they can all interaction with one or more databases using the Commond Data Model (CDM). Furthermore, these tools standardize the analytics for various use cases; Rather than having to start from scratch, an analysis can be implemented by filling in standard templates. This makes performign analysis easier, and also improves reproducibility and transparancy. For example, there appear to be a near-infinte number of ways to compute an incidence rate, but these can be specified in the OHDSI tools with a few choices, and anyone making those same choices will compute incidence rates the same way.

In this chapter we first describe various ways in which we can choose to implement an analysis, and what strategies the analysis can employ. We then review the various OHDSI tools and how they fit the various use cases.

\hypertarget{analysisImplementation}{%
\section{Analysis implementation}\label{analysisImplementation}}

Figure \ref{fig:implementations} shows the various ways in which we can choose to implement a study against a database using the CDM.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/OhdsiAnalyticsTools/implementations} 

}

\caption{Different ways to implement an analysis against data in the CDM.}\label{fig:implementations}
\end{figure}

We may choose to write our analysis as custom code, and not make use of any of the tools OHDSI has to offer. One could write a de novo analysis in R, SAS, or any other language. This provides the maximum flexibility, and may in fact be the only option if the specific analysis is not supported by any of our tools. However, this path requires a lot of technical skill, time, and effort, and as the analysis increases in complexity it becomes harder to avoid errors in the code.

An alternative is to develop the analysis in R, and make use of the packages in the \href{https://ohdsi.github.io/MethodsLibrary/}{OHDSI Methods Library}. At a minimum, one could use the \href{https://ohdsi.github.io/SqlRender/}{SqlRender} and \href{https://ohdsi.github.io/DatabaseConnector/}{DatabaseConnector} packages described in more detail in Chapter \ref{SqlAndR} that allow the same code to be executed on various database platforms, such as PostgreSQL, SQL Server, and Oracle. Other packages such as \href{https://ohdsi.github.io/CohortMethod/}{CohortMethod} and \href{https://ohdsi.github.io/PatientLevelPrediction/}{PatientLevelPrediction} offer R functions for advanced analytics against the CDM that can be called on in one's code. This still requires a lot of tecnhical expertise, but by re-using the validated components of the Methods Library we can be more efficient and error-free than when using completely custom code.

The third approach relies on our interactive analysis platform \href{https://github.com/OHDSI/Atlas/wiki}{ATLAS}, a web-based tool that allows non-programmers to perform a wide range of analyses efficiently. The downside is that some options may not be available.

ATLAS and the Methods Library are not independent. Some of the more complicated analytics that can be invoked in ATLAS are executed through calls to the packages in the Methods Library. Similarly, cohorts used in the Methods Library are often designed in ATLAS.

\hypertarget{analysis-strategy}{%
\section{Analysis strategy}\label{analysis-strategy}}

More or less independently of how we choose to implement our analysis is the strategy that our analytics takes in answering specific questions. Figure \ref{fig:strategies} highlights three strategies that are employed in OHDSI.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/OhdsiAnalyticsTools/strategies} 

}

\caption{Strategies for generating evidence for (clinical) questions.}\label{fig:strategies}
\end{figure}

The first strategy views every analysis as a single individual study. The analysis must be pre-specified in a protocol, implemented as code, executed against the data, after which the result can be compiled and interpreted. For every question, all steps must be repeated. An example of such an analysis is the OHDSI study into the risk of angioedema associated with levetiracetam compared with phenytoin. \citep{duke_2017} Here, a protocol was first written, analysis code using the OHDSI Methods Library was developed and executed across the OHDSI network, and results were compiled and disseminated in a journal publication.

The second strategy develops some app that allows users to answer a specific class of questions in real time or near-real time. Once the app has been developed, users can interactively define queries, submit them, and view the results. An example is the cohort definition and generation tool in ATLAS. This tool allows users to specify cohort definitions of arbitrary complexity, and execute the definition against a database to see how many people meet the various inclusion and exclusion criteria.

The third strategy similarly focuses on a class of questions, but then attempts to exhaustively generate all the evidence for the questions within the class. Users can then explore the evidence as needed, usually through some viewer app. One example is the OHDSI study into the effects of depression treatments \citep{schuemie_2018b}. In this study all depression treatments are compared for a large set of outcomes of interest across four large observational databases. The full set of results, including 17,718 empirically calibrated hazard ratios along with extensive study diagnostics, is available in an interactive web app \footnote{\url{http://data.ohdsi.org/SystematicEvidence/}}.

\hypertarget{atlas}{%
\section{ATLAS}\label{atlas}}

ATLAS is a web-based tool that must run on a server with access to the patient-level data in the CDM. To directly run the analyses against the data, ATLAS must therefore be installed behind your organization's firewall. However, there is also a public ATLAS \footnote{\url{http://www.ohdsi.org/web/atlas}}, and although this ATLAS instance only has access to a small simulated dataset, it can still be used for many purposes. For example, it is possible to fully define an effect estimation of prediction study in the public ATLAS, and automatically generate the R code for executing the study.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/OhdsiAnalyticsTools/atlas} 

}

\caption{ATLAS user interface.}\label{fig:atlas}
\end{figure}

A screenshot of ATLAS is provided in Figure \ref{fig:atlas}. On the left is a navigation bar showing the various functions provided by ATLAS:

\begin{description}
\tightlist
\item[Data Sources]
Data sources provides the capability review descriptive, standardized reporting for each of the data sources that you have configured within your Atlas platform. This feature uses the large-scale analytics strategy: all descriptives have been pre-computed. Data sources is discussed in Chapter \ref{Characterization}.
\item[Vocabulary Search]
Atlas provides the ability to search and explore the OMOP standardized vocabulary to understand what concepts exist within those vocabularies and how to apply those concepts in your standardized analysis against your data sources. This feature is discussed in Chapter \ref{StandardizedVocabularies}.
\item[Concept Sets]
Concept sets is the ability to create your own lists of codes that you are going to use throughout your standardized analyses so by searching the vocabulary and identifying the sets of terms that you're interested in you can save those and reuse them in all of your analyses.
\item[Cohort Definitions]
Cohort definitions is the ability to construct a set of persons who satisfy one or more criteria for a duration of time and these cohorts can then serve as the basis of inputs for all of your subsequent analyses. This feature is discussed in Chapter \ref{Cohorts}.
\item[Characterizations]
Characterisations is an analytic capability that allows you to look at one or more cohorts that you've defined and to summarize characteristics about those patient populations. This feature uses the real-time query strategy, and is discussed in Chapter \ref{Characterization}.
\item[Cohort Pathways]
Cohort pathways is an analytic tool that allows you to look at the sequence of clinical events that occur within one or more populations. This feature uses the real-time query strategy, and is discussed in Chapter \ref{Characterization}.
\item[Incidence Rates]
Incidence rates is a tool that allows you to estimate the incidence of outcomes within target populations of interest. This feature uses the real-time query strategy, and is discussed in Chapter \ref{Characterization}.
\item[Profiles]
Profiles is a tool that allows you to explore an individual patients longitudinal observational data to summarize what is going on within a given individual. This feature uses the real-time query strategy.
\item[Population Level Estimation]
Estimation is a capability to allow you to conduct population level effect estimation studies using a comparative cohort design whereby comparisons between one or more target and comparator cohorts can be explored for a series of outcomes. This feature can be said to implement the real-time query strategy, as no coding is required, and is discussed in Chapter \ref{PopulationLevelEstimation}.
\item[Patient Level Prediction]
Prediction is a capability to allow you to apply machine learning algorithms to conduct patient level prediction analyses whereby you can predict an outcome within any given target exposures. This feature can be said to implement the real-time query strategy, as no coding is required, and is discussed in Chapter \ref{PatientLevelPrediction}.
\item[Jobs]
Select the ``jobs'' menu item to explore jobs that are running in the background for long running processes such as generating a cohort or computing cohort reports.
\item[Configuration]
Select the ``configuration'' menu item to review the data sources that have been configured in the source configuration section.
\item[Feedback]
This will take you to the issue log for Atlas so that you can log a new issue or to search through existing issues. If you have ideas for new features or enhancements, this is also a place note these for the development community.
\end{description}

\hypertarget{security}{%
\subsection{Security}\label{security}}

\hypertarget{documentation}{%
\subsection{Documentation}\label{documentation}}

\hypertarget{system-requirements}{%
\subsection{System requirements}\label{system-requirements}}

\hypertarget{how-to-install}{%
\subsection{How to install}\label{how-to-install}}

\hypertarget{methods-library}{%
\section{Methods Library}\label{methods-library}}

The \href{https://ohdsi.github.io/MethodsLibrary/}{OHDSI Methods Library} is the collection of open source R packages show in Figure \ref{fig:methodsLibrary}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/OhdsiAnalyticsTools/methodsLibrary} 

}

\caption{Packages in the OHDSI Methods Library.}\label{fig:methodsLibrary}
\end{figure}

The packages offer R functions that together can be used to perform an observation study from data to estimates and supporting statistics, figures, and tables. The packages interact directly with observational data in the CDM, and can be used simply to provide cross-platform compatibility to completely custom analyses as described in Chapter \ref{SqlAndR}, or can provide advanced standardized analytics for population characterization (Chapter \ref{Characterization}), population-level causal effect estimation (Chapter \ref{PopulationLevelEstimation}), and patient-level prediction (Chapter \ref{PatientLevelPrediction}). The Methods Library supports best practices for use of observational data as learned from previous and ongoing research, such as transparency, reproducibility, as well as measuring of the operating characteristics of methods in a particular context and subsequent empirical calibration of estimates produced by the methods.

The Methods Library has already been used in many published clinical studies \citep{boland_2017, duke_2017, ramcharran_2017, weinstein_2017, wang_2017, ryan_2017, ryan_2018, vashisht_2018, yuan_2018, johnston_2019}, as well as methodological studies \citep{schuemie_2014, schuemie_2016, reps2018, tian_2018, schuemie_2018, schuemie_2018b, reps_2019}. Great care is taken to ensure the validity of the Methods Library, as described in Chapter \ref{SoftwareValidity}.

\hypertarget{support-for-large-scale-analytics}{%
\subsection{Support for large-scale analytics}\label{support-for-large-scale-analytics}}

One key feature incorporated in all packages is the ability to efficiently run many analyses. For example, when performing population-level estimation, the CohortMethod package allows for computing effect-size estimates for many exposures and outcomes, using various analysis settings, and the package will automatically choose the optimal path to compute all the required artifacts. Steps that can be re-used, such as extraction of covariates, or fitting a propensity model, will be executed only once. Where possible, computations will take place in parallel to maximuze the use of computational resources.

This feature allows for large-scale analytics, answering many questions at once, and is also essential for including control hypotheses (e.g.~negative controls) to measure the operating characteristics of our methods, and perform empirical calibration as described in Chapter \ref{MethodValidity}.

\hypertarget{BigDataSupport}{%
\subsection{Support for big data}\label{BigDataSupport}}

The Methods Library is also designed to run against very large databases and be able to perform computations involving large amounts of data. This achieved in three ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Most data manipulation is performed on the database server. An analysis usually only requires a small fraction of the entire data in the database, and the Methods Library, through the SqlRender and DatabaseConnector packages, allows for advanced operations to be performed on the server to preprocess and extract the relevant data.
\item
  Large local data objects are stored in a memory-efficient manner. For the data that is downloaded to the local machine, the Methods Library uses the \href{https://cran.r-project.org/web/packages/ff}{ff} package to store and work with large data objects. This allows us to work with data much larger than fits in memory.
\item
  High-performance computing is applied where needed. For example, the \href{https://ohdsi.github.io/Cyclops/}{Cyclops} package implements a highly efficient regression engine that is used throughout the Methods Library to perform large-scale regressions (large number of variables, large number of observations) that would not be possible to fit otherwise.
\end{enumerate}

\hypertarget{documentation-1}{%
\subsection{Documentation}\label{documentation-1}}

R provides a standard way of documenting package. Each package has a \emph{package manual} that documents every function and data set in the package. All package manuals are available online through the Methods Library website \footnote{\url{https://ohdsi.github.io/MethodsLibrary}}, through the package GitHub repositories, and for those packages available through CRAN they can be found in CRAN. Furtermore, from within R the package manual can be consulted by using the question mark. For example, after loading the DatabaseConnector package, typing the command \texttt{?connect} brings up the documentation on the ``connect'' function.

In addition to the package manual, many packages provide \emph{vignettes}. Vignettes are long-form documentation that describe how a package can be used to perform certain tasks. For example, one vignette \footnote{\url{https://ohdsi.github.io/CohortMethod/articles/MultipleAnalyses.html}} describes how to perform multiple analyses efficiently using the CohortMethod package. Vignettes can also be found through the Methods Library website , through the package GitHub repositories, and for those packages available through CRAN they can be found in CRAN.

\hypertarget{system-requirements-1}{%
\subsection{System requirements}\label{system-requirements-1}}

Two computing environments are relevant when discussing the system requirements: The database server, and the analytics workstation.

The database server must hold the observational healthcare data in CDM format. The Methods Library supports a wide array of database management systems including traditional database systems (PostgreSQL, Microsoft SQL Server, and Oracle), parallel data warehouses (Microsoft APS, IBM Netezza, and Amazon RedShift), as well as Big Data platforms (Hadoop through Impala, and Google BigQuery).

The analytics workstation is where the Methods Library is installed and run. This can either be a local machine, such as someone's laptop, or a remote server running RStudio Server. In all cases the requiments are that R is installed, preferrably together with RStudio. The Methods Library also requires that Java is installed. The analytics workstation should also be able to connect to the database server, specifically, any firewall between them should have the database server access ports opened the the workstation. Some of the analytics can be computationally intensive, so having multiple processing cores and ample memory can help speed up the analyses. We recommend having at least four cores and 16 gigabytes of memory.

\hypertarget{installR}{%
\subsection{How to install}\label{installR}}

Here are the steps for installing the required environment to run the OHDSI R packages. Four things needs to be installed:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{R} is a statistical computing environment. It comes with a basic user interface that is primarily a command-line interface.
\item
  \textbf{RTools} is a set of programs that is required on Windows to build R packages from source.
\item
  \textbf{RStudio} is an IDE (Integrated Development Environment) that makes R easier to use. It includes a code editor, debugging and visualization tools. Please use it to obtain a nice R experience.
\item
  \textbf{Java} is a computing environment that is needed to run some of the components in the OHDSI R packages, for example those needed to connect to a database.
\end{enumerate}

Below we describe how to install each of these in a Windows environment.

\BeginKnitrBlock{rmdimportant}
In Windows, both R and Java come in 32-bit and 64-bits architectures. If you install R in both architectures, you \textbf{must} also install Java in both architectures. It is recommended to only install the 64-bit version of R.
\EndKnitrBlock{rmdimportant}

\textbf{Installing R}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Go to \url{https://cran.r-project.org/}, click on ``Download R for Windows'', then ``base'', then click the Download link indicated in Figure \ref{fig:downloadR}.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/OhdsiAnalyticsTools/downloadR} 

}

\caption{Downloading R from CRAN.}\label{fig:downloadR}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  After the download has completed, run the installer. Use the default options everywhere, with two exceptions: First, it is better not to install into program files. Instead, just make R a subfolder of your C drive as shown in Figure \ref{fig:rDestination}. Second, to avoid problems due to differing architectures between R and Java, disable the 32-bit architecture as shown in Figure \ref{fig:no32Bits}.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/OhdsiAnalyticsTools/rDestination} 

}

\caption{Settings the destination folder for R.}\label{fig:rDestination}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/OhdsiAnalyticsTools/no32Bits} 

}

\caption{Disabling the 32-bit version of R.}\label{fig:no32Bits}
\end{figure}

Once completed, you should be able to select R from your Start Menu.

\textbf{Installing RTools}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Go to \url{https://cran.r-project.org/}, click on ``Download R for Windows'', then ``Rtools'', and select the very latest version of RTools to download.
\item
  After downloading has completed run the installer. Select the default options everywhere.
\end{enumerate}

\textbf{Installing RStudio}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Go to \url{https://www.rstudio.com/}, select ``Download RStudio'' (or the ``Download'' button under ``RStudio''), opt for the free version, and download the installer for Windows as shown in Figure \ref{fig:downloadRStudio}.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/OhdsiAnalyticsTools/downloadRStudio} 

}

\caption{Downloading RStudio.}\label{fig:downloadRStudio}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  After downloading, start the installer, and use the default options everywhere.
\end{enumerate}

\hypertarget{installing-java}{%
\section{Installing Java}\label{installing-java}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Go to \url{https://java.com/en/download/manual.jsp}, and select the Windows 64-bit installer as shown in Figure \ref{fig:downloadJava}. If you also installed the 32-bit version of R, you \emph{must} also install the other (32-bit) version of Java.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/OhdsiAnalyticsTools/downloadJava} 

}

\caption{Downloading Java.}\label{fig:downloadJava}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  After downloading just run the installer.
\end{enumerate}

\textbf{Verifying the installation}

You should now be ready to go, but we should make sure. Start RStudio, and type

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"SqlRender"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(SqlRender)}
\KeywordTok{translate}\NormalTok{(}\StringTok{"SELECT TOP 10 * FROM person;"}\NormalTok{, }\StringTok{"postgresql"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "SELECT  * FROM person LIMIT 10;"
\end{verbatim}

This function uses Java, so if all goes well we know both R and Java have been installed correctly!

Another test is to see if source packages can be built. Run the following R code to install the \texttt{CohortMethod} package from the OHDSI GitHub repository:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"drat"}\NormalTok{)}
\NormalTok{drat}\OperatorTok{::}\KeywordTok{addRepo}\NormalTok{(}\StringTok{"OHDSI"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"CohortMethod"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{deployment-strategies}{%
\section{Deployment strategies}\label{deployment-strategies}}

Deploying the entire OHDSI tool stack, including ATLAS and the Methods Library, in an organization is a daunting task. There are many components with dependencies that have to be considered, and configurations to set. For this reason, two initiatives have developed integrated deployment strategies that allow the entire stack to be installed as one package, using some forms of virtualization: Broadsea and Amazon Web Services (AWS).

\hypertarget{broadsea}{%
\subsection{Broadsea}\label{broadsea}}

BroadSea\footnote{\url{https://github.com/OHDSI/Broadsea}} uses Docker container technology\footnote{\url{https://www.docker.com/}}. The OHDSI tools are packaged along with dependencies into a single portable binary file called a Docker Image. This image can then be run on a Docker engine service, creating a virtual machine with all the software installed and ready to run. Docker engines are available for most operating systems, including Microsoft Windows, MacOS, and Linux. The Broadsea Docker image contains the main OHDSI tools, including the Methods Library and ATLAS.

\hypertarget{amazon-aws}{%
\subsection{Amazon AWS}\label{amazon-aws}}

Amazon has prepared two environments that can be instantiated in the AWS cloud computing environment with a click of the button: OHDSI-in-a-Box\footnote{\url{https://github.com/OHDSI/OHDSI-in-a-Box}} and OHDSIonAWS\footnote{\url{https://github.com/OHDSI/OHDSIonAWS}}.

OHDSI-in-a-Box is specifically created as a learning environment, and is used in most of the tutorials provided by the OHDSI community. It includes many OHDSI tools, sample data sets, RStudio and other supporting software in a single, low cost Windows virtual machine. A PostgreSQL database is used to store the CDM and also to store the intermediary results from ATLAS. The OMOP CDM data mapping and ETL tools are also included in OHDSI-in-a-Box. The architecture for OHDSI-in-a-Box is depicted in Figure \ref{fig:ohdsiinaboxDiagram}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/OhdsiAnalyticsTools/OHDSI-in-a-BoxDiagram} 

}

\caption{The Amazon Web Services architecure for OHDSI-in-a-Box.}\label{fig:ohdsiinaboxDiagram}
\end{figure}

OHDSIonAWS is a reference architecture for enterprise class, multi-user, scalable and fault tolerant OHDSI environments that can be used by organizations to perform their data analytics. It includes several sample datasets and can also automatically load your organization's real healthcare data. The data is placed in the Amazon Redshift database platform, which is supported by the OHDSI tools. Intermediary results of ATLAS are stored in a PostgreSQL database. On the front end, users have access to ATLAS and to RStudio through a web interface (leveraging RStudio Server). In RStudio the OHDSI Methods Library has already been installed, and can be used to connect to the databases. The automation to deploy OHDSIonAWS is open-source, and can be customized to include your organization's management tools and best practices. The architecture for OHDSIonAWS is depicted in Figure \ref{fig:ohdsionawsDiagram}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/OhdsiAnalyticsTools/OHDSIonAWSDiagram} 

}

\caption{The Amazon Web Services architecure for OHDSIonAWS.}\label{fig:ohdsionawsDiagram}
\end{figure}

\hypertarget{summary-3}{%
\section{Summary}\label{summary-3}}

\BeginKnitrBlock{rmdsummary}
\begin{itemize}
\tightlist
\item
  TODO: add
\end{itemize}
\EndKnitrBlock{rmdsummary}

\hypertarget{exercises-1}{%
\section{Exercises}\label{exercises-1}}

Todo

\hypertarget{SqlAndR}{%
\chapter{SQL and R}\label{SqlAndR}}

\emph{Chapter leads: Martijn Schuemie \& Peter Rijnbeek}

The Common Data Model (CDM) is a relational database model (all data is represented as records in tables that have fields), which means that the data will typically be stored in a relational database using a software platform like PostgreSQL, Oracle, or Microsoft SQL Server. The various OHDSI tools such as ATLAS and the Methods Library work by querying the database behind the scene, but we can also query the database directly ourselves if we have appropriate access rights. The main reason to do this is to perform analyses that currently are not supported by any existing tool. However, directly querying the database also comes with greater risk of making mistakes, as the OHDSI tools are often designed to help guide the user to appropriate analysis of the data, and direct queries do not provide such guidance.

The standard language for querying relational databases is SQL (Structured Query Language), which can be used both to query the database as well as to make changes to the data. Although the basic commands in SQL are indeed standard, meaning the same across software platforms, each platform has its own dialect, with subtle changes. For example, to retrieve the top 10 rows of the PERSON table on SQL Server one would type:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ TOP }\DecValTok{10} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ person;}
\end{Highlighting}
\end{Shaded}

Whereas the same query on PostgreSQL would be:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ person }\KeywordTok{LIMIT} \DecValTok{10}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

In OHDSI, we would like to be agnostic to the specific dialect a platform uses; We would like to `speak' the same SQL language across all OHDSI databases. For this reason OHDSI developed the \href{https://ohdsi.github.io/SqlRender/}{SqlRender} package, an R package that can translate from one standard dialect to any of the supported dialects that will be discussed later in this chapter. This standard dialect - \textbf{OHDSI SQL} - is mainly a subset of the SQL Server SQL dialect. The example SQL statements provided throughout this chapter will all use OHDSI SQL.

Each database platform also comes with its own software tools for querying the database using SQL. In OHDSI we developed the \href{https://ohdsi.github.io/DatabaseConnector/}{DatabaseConnector} package, one R package that can connect to many database platforms. DatabaseConnector will also be discussed later in this chapter.

So although one can query a database that conforms to the CDM without using any OHDSI tools, the recommended path is to use the DatabaseConnector and SqlRender packages. This allows queries that are developed at one site to be used at any other site without modification. R itself also immediately provides features to further analyse the data extracted from the database, such as performing statistical analyses and generating (interactive) plots.

In this chapter we assume the reader has a basic understanding of SQL. We first review how to use SqlRender and DatabaseConnector. If the reader does not intend to use these packages these sections can be skipped. In Section \ref{QueryTheCdm} we discuss how to use SQL (in this case OHDSI SQL) to query the CDM. The following section highlight how to use the OHDSI Standardized Vocabulary when querying the CDM. We highlight the QueryLibrary, a collection of commonly-used queries against the CDM that is publicly available. We close this chapter with an example study estimating incidence rates, and implement this study using SqlRender and DatabaseConnector.

\hypertarget{SqlRender}{%
\section{SqlRender}\label{SqlRender}}

The \href{https://ohdsi.github.io/SqlRender/}{SqlRender} package is available on CRAN (the Comprehensive R Archive Network), and can therefore be installed using:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"SqlRender"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

SqlRender supports a wide array of technical platforms including traditional database systems (PostgreSQL, Microsoft SQL Server, SQLite, and Oracle), parallel data warehouses (Microsoft APS, IBM Netezza, and Amazon RedShift), as well as Big Data platforms (Hadoop through Impala, and Google BigQuery). The R package comes with a package manual and a vignette that explores the full functionality. Here we describer some of the main features.

\hypertarget{sql-parameterization}{%
\subsection{SQL parameterization}\label{sql-parameterization}}

One of the functions of the package is to support parameterization of SQL. Often, small variations of SQL need to be generated based on some parameters. SqlRender offers a simple markup syntax inside the SQL code to allow parameterization. Rendering the SQL based on parameter values is done using the \texttt{render()} function.

\textbf{Substituting parameter values}

The \texttt{@} character can be used to indicate parameter names that need to be exchange for actual parameter values when rendering. In the following example, a variable called \texttt{a} is mentioned in the SQL. In the call to the render function the value of this parameter is defined:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sql <-}\StringTok{ "SELECT * FROM concept WHERE concept_id = @a;"}
\KeywordTok{render}\NormalTok{(sql, }\DataTypeTok{a =} \DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "SELECT * FROM concept WHERE concept_id = 123;"
\end{verbatim}

Note that, unlike the parameterization offered by most database management systems, it is just as easy to parameterize table or field names as values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sql <-}\StringTok{ "SELECT * FROM @x WHERE person_id = @a;"}
\KeywordTok{render}\NormalTok{(sql, }\DataTypeTok{x =} \StringTok{"observation"}\NormalTok{, }\DataTypeTok{a =} \DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "SELECT * FROM observation WHERE person_id = 123;"
\end{verbatim}

The parameter values can be numbers, strings, booleans, as well as vectors, which are converted to comma-delimited lists:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sql <-}\StringTok{ "SELECT * FROM concept WHERE concept_id IN (@a);"}
\KeywordTok{render}\NormalTok{(sql, }\DataTypeTok{a =} \KeywordTok{c}\NormalTok{(}\DecValTok{123}\NormalTok{, }\DecValTok{234}\NormalTok{, }\DecValTok{345}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "SELECT * FROM concept WHERE concept_id IN (123,234,345);"
\end{verbatim}

\textbf{If-then-else}

Sometimes blocks of codes need to be turned on or off based on the values of one or more parameters. This is done using the \texttt{\{Condition\}\ ?\ \{if\ true\}\ :\ \{if\ false\}} syntax. If the \emph{condition} evaluates to true or 1, the \emph{if true} block is used, else the \emph{if false} block is shown (if present).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sql <-}\StringTok{ "SELECT * FROM cohort \{@x\} ? \{WHERE subject_id = 1\}"}
\KeywordTok{render}\NormalTok{(sql, }\DataTypeTok{x =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "SELECT * FROM cohort "
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{render}\NormalTok{(sql, }\DataTypeTok{x =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "SELECT * FROM cohort WHERE subject_id = 1"
\end{verbatim}

Simple comparisons are also supported:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sql <-}\StringTok{ "SELECT * FROM cohort \{@x == 1\} ? \{WHERE subject_id = 1\};"}
\KeywordTok{render}\NormalTok{(sql, }\DataTypeTok{x =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "SELECT * FROM cohort WHERE subject_id = 1;"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{render}\NormalTok{(sql, }\DataTypeTok{x =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "SELECT * FROM cohort ;"
\end{verbatim}

As well as the \texttt{IN} operator:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sql <-}\StringTok{ "SELECT * FROM cohort \{@x IN (1,2,3)\} ? \{WHERE subject_id = 1\};"}
\KeywordTok{render}\NormalTok{(sql, }\DataTypeTok{x =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "SELECT * FROM cohort WHERE subject_id = 1;"
\end{verbatim}

\hypertarget{translation-to-other-sql-dialects}{%
\subsection{Translation to other SQL dialects}\label{translation-to-other-sql-dialects}}

Another function of the \href{https://ohdsi.github.io/SqlRender/}{SqlRender} package is to translate from OHDSI SQL to other SQL dialects. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sql <-}\StringTok{ "SELECT TOP 10 * FROM person;"}
\KeywordTok{translate}\NormalTok{(sql, }\DataTypeTok{targetDialect =} \StringTok{"postgresql"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "SELECT  * FROM person LIMIT 10;"
\end{verbatim}

The \texttt{targetDialect} parameter can have the following values: ``oracle'', ``postgresql'', ``pdw'', ``redshift'', ``impala'', ``netezza'', ``bigquery'', ``sqlite'', and ``sql server''.

\BeginKnitrBlock{rmdimportant}
There are limits to what SQL functions and constructs can be translated properly, both because only a limited set of translation rules have been implemented in the package, but also some SQL features do not have an equivalent in all dialects. This is the primary reason why OHDSI SQL was developed as its own, new SQL dialect. However, whenever possible we have kept to the SQL Server syntax to avoid reinventing the wheel.
\EndKnitrBlock{rmdimportant}

Despite our best effords, there are quite a few things to consider when writing OHDSI SQL that will run without error on all supported platforms. In what follows we discuss these considerations in detail.

\textbf{Functions and structures supported by translate}

These SQL Server functions have been tested and were found to be translated correctly to the various dialects:

\begin{longtable}[]{@{}lll@{}}
\caption{\label{tab:sqlFunctions} Functions supported by translate.}\tabularnewline
\toprule
Function & Function & Function\tabularnewline
\midrule
\endfirsthead
\toprule
Function & Function & Function\tabularnewline
\midrule
\endhead
ABS & EXP & RAND\tabularnewline
ACOS & FLOOR & RANK\tabularnewline
ASIN & GETDATE & RIGHT\tabularnewline
ATAN & HASHBYTES* & ROUND\tabularnewline
AVG & ISNULL & ROW\_NUMBER\tabularnewline
CAST & ISNUMERIC & RTRIM\tabularnewline
CEILING & LEFT & SIN\tabularnewline
CHARINDEX & LEN & SQRT\tabularnewline
CONCAT & LOG & SQUARE\tabularnewline
COS & LOG10 & STDEV\tabularnewline
COUNT & LOWER & SUM\tabularnewline
COUNT\_BIG & LTRIM & TAN\tabularnewline
DATEADD & MAX & UPPER\tabularnewline
DATEDIFF & MIN & VAR\tabularnewline
DATEFROMPARTS & MONTH & YEAR\tabularnewline
DATETIMEFROMPARTS & NEWID &\tabularnewline
DAY & PI &\tabularnewline
EOMONTH & POWER &\tabularnewline
\bottomrule
\end{longtable}

* Requires special privileges on Oracle. Has no equivalent on SQLite.

Similarly, many SQL syntax structures are supported. Here is a non-exhaustive lists of expressions that we know will translate well:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- Simple selects:}
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM} \KeywordTok{table}\NormalTok{;}

\CommentTok{-- Selects with joins:}
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ table_1 }\KeywordTok{INNER} \KeywordTok{JOIN}\NormalTok{ table_2 }\KeywordTok{ON}\NormalTok{ a }\OperatorTok{=}\NormalTok{ b;}

\CommentTok{-- Nested queries:}
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ (}\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ table_1) tmp }\KeywordTok{WHERE}\NormalTok{ a }\OperatorTok{=}\NormalTok{ b;}

\CommentTok{-- Limiting to top rows:}
\KeywordTok{SELECT}\NormalTok{ TOP }\DecValTok{10} \OperatorTok{*} \KeywordTok{FROM} \KeywordTok{table}\NormalTok{;}

\CommentTok{-- Selecting into a new table:}
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{INTO}\NormalTok{ new_table }\KeywordTok{FROM} \KeywordTok{table}\NormalTok{;}

\CommentTok{-- Creating tables:}
\KeywordTok{CREATE} \KeywordTok{TABLE} \KeywordTok{table}\NormalTok{ (field }\DataTypeTok{INT}\NormalTok{);}

\CommentTok{-- Inserting verbatim values:}
\KeywordTok{INSERT} \KeywordTok{INTO}\NormalTok{ other_table (field_1) }\KeywordTok{VALUES}\NormalTok{ (}\DecValTok{1}\NormalTok{);}

\CommentTok{-- Inserting from SELECT:}
\KeywordTok{INSERT} \KeywordTok{INTO}\NormalTok{ other_table (field_1) }\KeywordTok{SELECT} \FunctionTok{value} \KeywordTok{FROM} \KeywordTok{table}\NormalTok{;}
  
\CommentTok{-- Simple drop commands:}
\KeywordTok{DROP} \KeywordTok{TABLE} \KeywordTok{table}\NormalTok{;}

\CommentTok{-- Drop table if it exists:}
\ControlFlowTok{IF}\NormalTok{ OBJECT_ID(}\StringTok{'ACHILLES_analysis'}\NormalTok{, }\StringTok{'U'}\NormalTok{) }\KeywordTok{IS} \KeywordTok{NOT} \KeywordTok{NULL}
  \KeywordTok{DROP} \KeywordTok{TABLE}\NormalTok{ ACHILLES_analysis;}
  
\CommentTok{-- Drop temp table if it exists:}
\ControlFlowTok{IF}\NormalTok{ OBJECT_ID(}\StringTok{'tempdb..#cohorts'}\NormalTok{, }\StringTok{'U'}\NormalTok{) }\KeywordTok{IS} \KeywordTok{NOT} \KeywordTok{NULL}
  \KeywordTok{DROP} \KeywordTok{TABLE}\NormalTok{ #cohorts;  }

\CommentTok{-- Common table expressions:}
\KeywordTok{WITH}\NormalTok{ cte }\KeywordTok{AS}\NormalTok{ (}\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM} \KeywordTok{table}\NormalTok{) }\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ cte;}

\CommentTok{-- OVER clauses:}
\KeywordTok{SELECT} \FunctionTok{ROW_NUMBER}\NormalTok{() }\KeywordTok{OVER}\NormalTok{ (}\KeywordTok{PARTITION} \KeywordTok{BY}\NormalTok{ a }\KeywordTok{ORDER} \KeywordTok{BY}\NormalTok{ b)}
  \KeywordTok{AS} \OtherTok{"Row Number"} \KeywordTok{FROM} \KeywordTok{table}\NormalTok{;}
  
\CommentTok{-- CASE WHEN clauses:}
\KeywordTok{SELECT} \ControlFlowTok{CASE} \ControlFlowTok{WHEN}\NormalTok{ a}\OperatorTok{=}\DecValTok{1} \ControlFlowTok{THEN}\NormalTok{ a }\ControlFlowTok{ELSE} \DecValTok{0} \ControlFlowTok{END} \KeywordTok{AS} \FunctionTok{value} \KeywordTok{FROM} \KeywordTok{table}\NormalTok{;}

\CommentTok{-- UNIONs:}
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ a }\KeywordTok{UNION} \KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ b;}

\CommentTok{-- INTERSECTIONs:}
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ a }\KeywordTok{INTERSECT} \KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ b;}

\CommentTok{-- EXCEPT:}
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ a }\KeywordTok{EXCEPT} \KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ b;}
\end{Highlighting}
\end{Shaded}

\textbf{String concatenation}

String concatenation is one area where SQL Server is less specific than other dialects. In SQL Server, one would write \texttt{SELECT\ first\_name\ +\ \textquotesingle{}\ \textquotesingle{}\ +\ last\_name\ AS\ full\_name\ FROM\ table}, but this should be \texttt{SELECT\ first\_name\ \textbar{}\textbar{}\ \textquotesingle{}\ \textquotesingle{}\ \textbar{}\textbar{}\ last\_name\ AS\ full\_name\ FROM\ table} in PostgreSQL and Oracle. SqlRender tries to guess when values that are being concatenated are strings. In the example above, because we have an explicit string (the space surrounded by single quotation marks), the translation will be correct. However, if the query had been \texttt{SELECT\ first\_name\ +\ last\_name\ AS\ full\_name\ FROM\ table}, SqlRender would have had no clue the two fields were strings, and would incorrectly leave the plus sign. Another clue that a value is a string is an explicit cast to VARCHAR, so \texttt{SELECT\ last\_name\ +\ CAST(age\ AS\ VARCHAR(3))\ AS\ full\_name\ FROM\ table} would also be translated correctly. To avoid ambiguity altogether, it is probable best to use the \texttt{CONCAT()} function to concatenate two or more strings.

\textbf{Table aliases and the AS keyword}

Many SQL dialects allow the use of the \texttt{AS} keyword when defining a table alias, but will also work fine without the keyword. For example, both these SQL statements are fine for SQL Server, PostgreSQL, RedShift, etc.:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- Using AS keyword}
\KeywordTok{SELECT} \OperatorTok{*} 
\KeywordTok{FROM}\NormalTok{ my_table }\KeywordTok{AS}\NormalTok{ table_1}
\KeywordTok{INNER} \KeywordTok{JOIN}\NormalTok{ (}
  \KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ other_table}
\NormalTok{) }\KeywordTok{AS}\NormalTok{ table_2}
\KeywordTok{ON}\NormalTok{ table_1.person_id }\OperatorTok{=}\NormalTok{ table_2.person_id;}

\CommentTok{-- Not using AS keyword}
\KeywordTok{SELECT} \OperatorTok{*} 
\KeywordTok{FROM}\NormalTok{ my_table table_1}
\KeywordTok{INNER} \KeywordTok{JOIN}\NormalTok{ (}
  \KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ other_table}
\NormalTok{) table_2}
\KeywordTok{ON}\NormalTok{ table_1.person_id }\OperatorTok{=}\NormalTok{ table_2.person_id;}
\end{Highlighting}
\end{Shaded}

However, Oracle will throw an error when the \texttt{AS} keyword is used. In the above example, the first query will fail. It is therefore recommended to not use the \texttt{AS} keyword when aliasing tables. (Note: we can't make SqlRender handle this, because it can't easily distinguish between table aliases where Oracle doesn't allow \texttt{AS} to be used, and field aliases, where Oracle requires \texttt{AS} to be used.)

\textbf{Temp tables}

Temp tables can be very useful to store intermediate results, and when used correctly can be used to dramatically improve performance of queries. On most database platforms temp tables have very nice properties: they're only visible to the current user, are automatically dropped when the session ends, and can be created even when the user has no write access. Unfortunately, in Oracle temp tables are basically permanent tables, with the only difference that the data inside the table is only visible to the current user. This is why, in Oracle, SqlRender will try to emulate temp tables by

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Adding a random string to the table name so tables from different users will not conflict.
\item
  Allowing the user to specify the schema where the temp tables will be created.
\end{enumerate}

For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sql <-}\StringTok{ "SELECT * FROM #children;"}
\KeywordTok{translate}\NormalTok{(sql, }\DataTypeTok{targetDialect =} \StringTok{"oracle"}\NormalTok{, }\DataTypeTok{oracleTempSchema =} \StringTok{"temp_schema"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "SELECT * FROM temp_schema.i2ghr8sdchildren ;"
\end{verbatim}

Note that the user will need to have write privileges on \texttt{temp\_schema}.

Also note that because Oracle has a limit on table names of 30 characters, \textbf{temp table names are only allowed to be at most 22 characters long} because else the name will become too long after appending the session ID.

Furthermore, remember that temp tables are not automatically dropped on Oracle, so you will need to explicitly \texttt{TRUNCATE} and \texttt{DROP} all temp tables once you're done with them to prevent orphan tables accumulating in the Oracle temp schema.

\textbf{Implicit casts}

One of the few points where SQL Server is less explicit than other dialects is that it allows implicit casts. For example, this code will work on SQL Server:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{CREATE} \KeywordTok{TABLE}\NormalTok{ #temp (txt }\DataTypeTok{VARCHAR}\NormalTok{);}

\KeywordTok{INSERT} \KeywordTok{INTO}\NormalTok{ #temp}
\KeywordTok{SELECT} \StringTok{'1'}\NormalTok{;}

\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ #temp }\KeywordTok{WHERE}\NormalTok{ txt }\OperatorTok{=} \DecValTok{1}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

Even though \texttt{txt} is a VARCHAR field and we are comparing it with an integer, SQL Server will automatically cast one of the two to the correct type to allow the comparison. In contrast, other dialects such as PostgreSQL will throw an error when trying to compare a VARCHAR with an INT.

You should therefore always make casts explicit. In the above example, the last statement should be replaced with either

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ #temp }\KeywordTok{WHERE}\NormalTok{ txt }\OperatorTok{=} \FunctionTok{CAST}\NormalTok{(}\DecValTok{1} \KeywordTok{AS} \DataTypeTok{VARCHAR}\NormalTok{);}
\end{Highlighting}
\end{Shaded}

or

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ #temp }\KeywordTok{WHERE} \FunctionTok{CAST}\NormalTok{(txt }\KeywordTok{AS} \DataTypeTok{INT}\NormalTok{) }\OperatorTok{=} \DecValTok{1}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

\textbf{Case sensitivity in string comparisons}

Some DBMS platforms such as SQL Server always perform string comparisons in a case-insensitive way, while others such as PostgreSQL are always case sensitive. It is therefore recommended to always assume case-sensitive comparisons, and to explicitly make comparisons case-insensitive when unsure about the case. For example, instead of

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ concept }\KeywordTok{WHERE}\NormalTok{ concep_class_id }\OperatorTok{=} \StringTok{'Clinical Finding'}
\end{Highlighting}
\end{Shaded}

it is preferred to use

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ concept }\KeywordTok{WHERE} \FunctionTok{LOWER}\NormalTok{(concep_class_id) }\OperatorTok{=} \StringTok{'clinical finding'}
\end{Highlighting}
\end{Shaded}

\textbf{Schemas and databases}

In SQL Server, tables are located in a schema, and schemas reside in a database. For example, \texttt{cdm\_data.dbo.person} refers to the \texttt{person} table in the \texttt{dbo} schema in the \texttt{cdm\_data} database. In other dialects, even though a similar hierarchy often exists they are used very differently. In SQL Server, there is typically one schema per database (often called \texttt{dbo}), and users can easily use data in different databases. On other platforms, for example in PostgreSQL, it is not possible to use data across databases in a single session, but there are often many schemas in a database. In PostgreSQL one could say that the equivalent of SQL Server's database is the schema.

We therefore recommend concatenating SQL Server's database and schema into a single parameter, which we typically call \texttt{@databaseSchema}. For example, we could have the parameterized SQL

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ @databaseSchema.person}
\end{Highlighting}
\end{Shaded}

where on SQL Server we can include both database and schema names in the value: \texttt{databaseSchema\ =\ "cdm\_data.dbo"}. On other platforms, we can use the same code, but now only specify the schema as the parameter value: \texttt{databaseSchema\ =\ "cdm\_data"}.

The one situation where this will fail is the \texttt{USE} command, since \texttt{USE\ cdm\_data.dbo;} will throw an error. It is therefore preferred not to use the \texttt{USE} command, but always specify the database / schema where a table is located.

\textbf{Debugging parameterized SQL}

Debugging parameterized SQL can be a bit complicated; Only the rendered SQL can be tested against a database server, but changes to the code should be made in the parameterized (pre-rendered) SQL.

A Shiny app is included in the SqlRender package for interactively editing source SQL and generating rendered and translated SQL. The app can be started using:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{launchSqlRenderDeveloper}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Which will open the default browser with the app shown in Figure \ref{fig:sqlDeveloper}. The app is also publicly available on the web\footnote{\url{http://data.ohdsi.org/SqlDeveloper/}}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/SqlAndR/sqlDeveloper} 

}

\caption{The SqlDeveloper Shiny app.}\label{fig:sqlDeveloper}
\end{figure}

In the app you can enter OHDSI SQL, select the target dialect as well as provide values for the parameters that appear in your SQL, and the translation will automatically appear at the bottom.

\hypertarget{DatabaseConnector}{%
\section{DatabaseConnector}\label{DatabaseConnector}}

\href{https://ohdsi.github.io/DatabaseConnector/}{DatabaseConnector} is an R package for connecting to various database platforms using Java's JDBC drivers. The DatabaseConnector package is available on CRAN (the Comprehensive R Archive Network), and can therefore be installed using:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"DatabaseConnector"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

DatabaseConnector supports a wide array of technical platforms including traditional database systems (PostgreSQL, Microsoft SQL Server, SQLite, and Oracle), parallel data warehouses (Microsoft APS, IBM Netezza, and Amazon RedShift), as well as Big Data platforms (Hadoop through Impala, and Google BigQuery). The package already contains most drivers, but because of licensing reasons the drivers for BigQuery, Netezza and Impala are not included but must be obtained by the user. Type \texttt{?jdbcDrivers} for instructions on how to download these drivers. Once downloaded, you can use the \texttt{pathToDriver} argument of the \texttt{connect}, \texttt{dbConnect}, and \texttt{createConnectionDetails} functions.

\hypertarget{creating-a-connection}{%
\subsection{Creating a connection}\label{creating-a-connection}}

To connect to a database a number of details need to be specified, such as the database platform, the location of the server, the user name, and password. We can call the \texttt{connect} function and specify these details directly:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{conn <-}\StringTok{ }\KeywordTok{connect}\NormalTok{(}\DataTypeTok{dbms =} \StringTok{"postgresql"}\NormalTok{,}
                \DataTypeTok{server =} \StringTok{"localhost/postgres"}\NormalTok{,}
                \DataTypeTok{user =} \StringTok{"joe"}\NormalTok{,}
                \DataTypeTok{password =} \StringTok{"secret"}\NormalTok{,}
                \DataTypeTok{schema =} \StringTok{"cdm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Connecting using PostgreSQL driver
\end{verbatim}

See \texttt{?connect} for information on which details are required for each platform. Don't forget to close any connection afterwards:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{disconnect}\NormalTok{(conn)}
\end{Highlighting}
\end{Shaded}

Note that, instead of providing the server name, it is also possible to provide the JDBC connection string if this is more convenient:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{connString <-}\StringTok{ "jdbc:postgresql://localhost:5432/postgres"}
\NormalTok{conn <-}\StringTok{ }\KeywordTok{connect}\NormalTok{(}\DataTypeTok{dbms =} \StringTok{"postgresql"}\NormalTok{,}
                \DataTypeTok{connectionString =}\NormalTok{ connString,}
                \DataTypeTok{user =} \StringTok{"joe"}\NormalTok{,}
                \DataTypeTok{password =} \StringTok{"secret"}\NormalTok{,}
                \DataTypeTok{schema =} \StringTok{"cdm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Connecting using PostgreSQL driver
\end{verbatim}

Sometimes we may want to first specify the connection details, and defer connecting until later. This may be convenient for example when the connection is established inside a function, and the details need to be passed as an argument. We can use the \texttt{createConnectionDetails} function for this purpose:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{details <-}\StringTok{ }\KeywordTok{createConnectionDetails}\NormalTok{(}\DataTypeTok{dbms =} \StringTok{"postgresql"}\NormalTok{,}
                                   \DataTypeTok{server =} \StringTok{"localhost/postgres"}\NormalTok{,}
                                   \DataTypeTok{user =} \StringTok{"joe"}\NormalTok{,}
                                   \DataTypeTok{password =} \StringTok{"secret"}\NormalTok{,}
                                   \DataTypeTok{schema =} \StringTok{"cdm"}\NormalTok{)}
\NormalTok{conn <-}\StringTok{ }\KeywordTok{connect}\NormalTok{(details)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Connecting using PostgreSQL driver
\end{verbatim}

\hypertarget{querying}{%
\subsection{Querying}\label{querying}}

The main functions for querying database are the \texttt{querySql} and \texttt{executeSql} functions. The difference between these functions is that \texttt{querySql} expects data to be returned by the database, and can handle only one SQL statement at a time. In contrast, \texttt{executeSql} does not expect data to be returned, and accepts multiple SQL statements in a single SQL string.

Some examples:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{querySql}\NormalTok{(conn, }\StringTok{"SELECT TOP 3 * FROM person"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   PERSON_ID GENDER_CONCEPT_ID YEAR_OF_BIRTH
## 1         1              8507          1975
## 2         2              8507          1976
## 3         3              8507          1977
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{executeSql}\NormalTok{(conn, }\StringTok{"TRUNCATE TABLE foo; DROP TABLE foo;"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Both function provide extensive error reporting: When an error is thrown by the server, the error message and the offending piece of SQL are written to a text file to allow better debugging. The \texttt{executeSql} function also by default shows a progress bar, indicating the percentage of SQL statements that has been executed. If those attributes are not desired, the package also offers the \texttt{lowLevelQuerySql} and \texttt{lowLevelExecuteSql} functions.

\hypertarget{querying-using-ffdf-objects}{%
\subsection{Querying using ffdf objects}\label{querying-using-ffdf-objects}}

Sometimes the data to be fetched from the database is too large to fit into memory. As mentioned in Section \ref{BigDataSupport}, in such a case we can use the \texttt{ff} package to store R data objects on file, and use them as if they are available in memory. \texttt{DatabaseConnector} can download data directly into ffdf objects:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{querySql.ffdf}\NormalTok{(conn, }\StringTok{"SELECT * FROM person"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Where x is now an ffdf object.

\hypertarget{querying-different-platforms-using-the-same-sql}{%
\subsection{Querying different platforms using the same SQL}\label{querying-different-platforms-using-the-same-sql}}

The following convenience functions are available that first call the \texttt{render} and \texttt{translate} functions in the SqlRender package: \texttt{renderTranslateExecuteSql}, \texttt{renderTranslateQuerySql}, \texttt{renderTranslateQuerySql.ffdf}. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{renderTranslateQuerySql}\NormalTok{(conn, }
                             \DataTypeTok{sql =} \StringTok{"SELECT TOP 10 * FROM @schema.person"}\NormalTok{,}
                             \DataTypeTok{schema =} \StringTok{"cdm_synpuf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that the SQL Server-specific `TOP 10' syntax will be translated to for example `LIMIT 10' on PostgreSQL, and that the SQL parameter \texttt{@schema} will be instantiated with the provided value `cdm\_synpuf'.

\hypertarget{inserting-tables}{%
\subsection{Inserting tables}\label{inserting-tables}}

Although it is also possible to insert data in the database by sending SQL statements using the \texttt{executeSql} function, it is often more convenient and faster (due to some optimization) to use the \texttt{insertTable} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(mtcars)}
\KeywordTok{insertTable}\NormalTok{(conn, }\StringTok{"mtcars"}\NormalTok{, mtcars, }\DataTypeTok{createTable =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this example, we're uploading the mtcars data frame to a table called `mtcars' on the server, which will be automatically created.

\hypertarget{QueryTheCdm}{%
\section{Querying the CDM}\label{QueryTheCdm}}

In the following examples we use OHDSI SQL to query a database that adheres to the CDM. These queries use \texttt{@cdm} to denote the database schema where the data in CDM can be found.

We can start by just querying how many people are in the database:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \FunctionTok{COUNT}\NormalTok{(}\OperatorTok{*}\NormalTok{) }\KeywordTok{AS}\NormalTok{ person_count }\KeywordTok{FROM}\NormalTok{ @cdm.person;}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}r@{}}
\toprule
PERSON\_COUNT\tabularnewline
\midrule
\endhead
26299001\tabularnewline
\bottomrule
\end{longtable}

Or perhaps we're interested in the average length of an observation period:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \FunctionTok{AVG}\NormalTok{(DATEDIFF(}\DataTypeTok{DAY}\NormalTok{, }
\NormalTok{                    observation_period_start_date, }
\NormalTok{                    observation_period_end_date) }\OperatorTok{/} \FloatTok{365.25}\NormalTok{) }\KeywordTok{AS}\NormalTok{ num_years}
\KeywordTok{FROM}\NormalTok{ @cdm.observation_period;}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}r@{}}
\toprule
NUM\_YEARS\tabularnewline
\midrule
\endhead
1.980803\tabularnewline
\bottomrule
\end{longtable}

We can join tables to produce additional statistics. A join combines fields from multiple tables, typically by requiring specific fields in the tables to have the same value. For example, here we join the PERSON table to the OBSERVATION\_PERIOD table on the person\_id fields in both tables. In other words, the result of the join is a new table-like set that has all the fields of the two tables, but in all rows the person\_id fields from the two tables must have the same value. We can now for example compute the maximum age at observation end by using the observation\_period\_end\_date field from the OBSERVATION\_PERIOD table together with the year\_of\_birth field of the PERSON table:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \FunctionTok{MAX}\NormalTok{(}\DataTypeTok{YEAR}\NormalTok{(observation_period_end_date) }\OperatorTok{-}
\NormalTok{           year_of_birth) }\KeywordTok{AS}\NormalTok{ max_age}
\KeywordTok{FROM}\NormalTok{ @cdm.person}
\KeywordTok{INNER} \KeywordTok{JOIN}\NormalTok{ @cdm.observation_period}
  \KeywordTok{ON}\NormalTok{ person.person_id }\OperatorTok{=}\NormalTok{ observation_period.person_id;}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}r@{}}
\toprule
MAX\_AGE\tabularnewline
\midrule
\endhead
90\tabularnewline
\bottomrule
\end{longtable}

A much more complicated query is needed to determine the distribution of age at the start of observation. In this query, we first join the PERSON to the OBSERVATION\_PERIOD table to compute age at start of observetion. We also compute the ordering for this joined set based on age, and store it as order\_nr. Because we want to use the result of this join multiple times, we define it as a common table expression (CTE) (defined using \texttt{WITH\ ...\ AS}) that we call ``ages'', meaning We can refer to ages as if it is an existing table. We count the number of rows in ages to produce ``n'', and then for each quantile find the minimum age where the order\_nr is smaller than the fraction times n.~For example, median we use the minimum age where \(order\_nr < .50 * n\). The minimum and maximum age are computed separately:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{WITH}\NormalTok{ ages}
\KeywordTok{AS}\NormalTok{ (}
    \KeywordTok{SELECT}\NormalTok{ age,}
        \FunctionTok{ROW_NUMBER}\NormalTok{() }\KeywordTok{OVER}\NormalTok{ (}
            \KeywordTok{ORDER} \KeywordTok{BY}\NormalTok{ age}
\NormalTok{            ) order_nr}
    \KeywordTok{FROM}\NormalTok{ (}
        \KeywordTok{SELECT} \DataTypeTok{YEAR}\NormalTok{(observation_period_start_date) }\OperatorTok{-}\NormalTok{ year_of_birth }\KeywordTok{AS}\NormalTok{ age}
        \KeywordTok{FROM}\NormalTok{ @cdm.person}
        \KeywordTok{INNER} \KeywordTok{JOIN}\NormalTok{ @cdm.observation_period}
            \KeywordTok{ON}\NormalTok{ person.person_id }\OperatorTok{=}\NormalTok{ observation_period.person_id}
\NormalTok{        ) age_computed}
\NormalTok{    )}
\KeywordTok{SELECT} \FunctionTok{MIN}\NormalTok{(age) }\KeywordTok{AS}\NormalTok{ min_age,}
    \FunctionTok{MIN}\NormalTok{(}\ControlFlowTok{CASE} 
            \ControlFlowTok{WHEN}\NormalTok{ order_nr }\OperatorTok{<} \FloatTok{.25} \OperatorTok{*}\NormalTok{ n}
                \ControlFlowTok{THEN} \DecValTok{9999}
            \ControlFlowTok{ELSE}\NormalTok{ age}
            \ControlFlowTok{END}\NormalTok{) }\KeywordTok{AS}\NormalTok{ q25_age,}
    \FunctionTok{MIN}\NormalTok{(}\ControlFlowTok{CASE} 
            \ControlFlowTok{WHEN}\NormalTok{ order_nr }\OperatorTok{<} \FloatTok{.50} \OperatorTok{*}\NormalTok{ n}
                \ControlFlowTok{THEN} \DecValTok{9999}
            \ControlFlowTok{ELSE}\NormalTok{ age}
            \ControlFlowTok{END}\NormalTok{) }\KeywordTok{AS}\NormalTok{ median_age,}
    \FunctionTok{MIN}\NormalTok{(}\ControlFlowTok{CASE} 
            \ControlFlowTok{WHEN}\NormalTok{ order_nr }\OperatorTok{<} \FloatTok{.75} \OperatorTok{*}\NormalTok{ n}
                \ControlFlowTok{THEN} \DecValTok{9999}
            \ControlFlowTok{ELSE}\NormalTok{ age}
            \ControlFlowTok{END}\NormalTok{) }\KeywordTok{AS}\NormalTok{ q75_age,}
    \FunctionTok{MAX}\NormalTok{(age) }\KeywordTok{AS}\NormalTok{ max_age}
\KeywordTok{FROM}\NormalTok{ ages}
\KeywordTok{CROSS} \KeywordTok{JOIN}\NormalTok{ (}
    \KeywordTok{SELECT} \FunctionTok{COUNT}\NormalTok{(}\OperatorTok{*}\NormalTok{) }\KeywordTok{AS}\NormalTok{ n}
    \KeywordTok{FROM}\NormalTok{ ages}
\NormalTok{    ) population_size;}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rrrrr@{}}
\toprule
MIN\_AGE & Q25\_AGE & MEDIAN\_AGE & Q75\_AGE & MAX\_AGE\tabularnewline
\midrule
\endhead
0 & 6 & 17 & 34 & 90\tabularnewline
\bottomrule
\end{longtable}

More complex computations can also be performed in R instead of using SQL. For example, we can get the same answer using this R code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sql <-}\StringTok{ "SELECT YEAR(observation_period_start_date) -}
\StringTok{               year_of_birth AS age}
\StringTok{FROM @cdm.person}
\StringTok{INNER JOIN @cdm.observation_period}
\StringTok{  ON person.person_id = observation_period.person_id;"}
\NormalTok{age <-}\StringTok{ }\KeywordTok{renderTranslateQuerySql}\NormalTok{(conn, sql, }\DataTypeTok{cdm =} \StringTok{"cdm"}\NormalTok{)}
\KeywordTok{quantile}\NormalTok{(age[, }\DecValTok{1}\NormalTok{], }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   0%  25%  50%  75% 100% 
##    0    6   17   34   90
\end{verbatim}

Here we compute age on the server, download all ages, and then compute the age distribution. However, this requires millions of rows of data to be downloaded from the database server, and is therefore not very efficient. You will need to decide on a case-by-case basis whether a computation is best performed in SQL or in R.

Queries can use the source values in the CDM. For example, we can retrieve the top 10 most frequent condition source codes using:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ TOP }\DecValTok{10}\NormalTok{ condition_source_value, }
  \FunctionTok{COUNT}\NormalTok{(}\OperatorTok{*}\NormalTok{) }\KeywordTok{AS}\NormalTok{ code_count}
\KeywordTok{FROM}\NormalTok{ @cdm.condition_occurrence}
\KeywordTok{GROUP} \KeywordTok{BY}\NormalTok{ condition_source_value}
\KeywordTok{ORDER} \KeywordTok{BY} \OperatorTok{-}\FunctionTok{COUNT}\NormalTok{(}\OperatorTok{*}\NormalTok{);}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rr@{}}
\toprule
CONDITION\_SOURCE\_VALUE & CODE\_COUNT\tabularnewline
\midrule
\endhead
4019 & 49094668\tabularnewline
25000 & 36149139\tabularnewline
78099 & 28908399\tabularnewline
319 & 25798284\tabularnewline
31401 & 22547122\tabularnewline
317 & 22453999\tabularnewline
311 & 19626574\tabularnewline
496 & 19570098\tabularnewline
I10 & 19453451\tabularnewline
3180 & 18973883\tabularnewline
\bottomrule
\end{longtable}

Here we grouped records in the CONDITION\_OCCURRENCE table by values of the condition\_source\_value field, and counted the number of records in each group. We retrieve the condition\_source\_value and the count, and reverse-order it by the count.

\hypertarget{using-the-vocabulary-when-querying}{%
\section{Using the vocabulary when querying}\label{using-the-vocabulary-when-querying}}

Many operations require the vocabulary to be useful. The Vocabulary tables are part of the CDM, and are therefore available using SQL queries. Querying the Vocabulary is already described at length in Chapter \ref{StandardizedVocabularies}. Here we show how queries against the Vocabulary can be combined with queries against the CDM. Many fields in the CDM contain concept IDs which can be resolved using the CONCEPT table. For example, we may wish to count the number of persons in the database stratified by gender, and it would be convenient to resolve the GENDER\_CONCEPT\_ID field to a concept name:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \FunctionTok{COUNT}\NormalTok{(}\OperatorTok{*}\NormalTok{) }\KeywordTok{AS}\NormalTok{ subject_count,}
\NormalTok{  concept_name}
\KeywordTok{FROM}\NormalTok{ @cdm.person}
\KeywordTok{INNER} \KeywordTok{JOIN}\NormalTok{ @cdm.concept}
  \KeywordTok{ON}\NormalTok{ person.gender_concept_id }\OperatorTok{=}\NormalTok{ concept.concept_id}
\KeywordTok{GROUP} \KeywordTok{BY}\NormalTok{ concept_name;}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rr@{}}
\toprule
SUBJECT\_COUNT & CONCEPT\_NAME\tabularnewline
\midrule
\endhead
14927548 & FEMALE\tabularnewline
11371453 & MALE\tabularnewline
\bottomrule
\end{longtable}

A very powerful feature of the Vocabulary is its hierarchy. A very common query looks for a specific concept \emph{and all of its descendants}. For example, image we wish to count the number of prescriptions containing the ingredient ibuprofen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \FunctionTok{COUNT}\NormalTok{(}\OperatorTok{*}\NormalTok{) }\KeywordTok{AS}\NormalTok{ prescription_count}
\KeywordTok{FROM}\NormalTok{ @cdm.drug_exposure}
\KeywordTok{INNER} \KeywordTok{JOIN}\NormalTok{ @cdm.concept_ancestor}
  \KeywordTok{ON}\NormalTok{ drug_concept_id }\OperatorTok{=}\NormalTok{ descendant_concept_id}
\KeywordTok{INNER} \KeywordTok{JOIN}\NormalTok{ @cdm.concept ingredient}
  \KeywordTok{ON}\NormalTok{ ancestor_concept_id }\OperatorTok{=}\NormalTok{ ingredient.concept_id}
\KeywordTok{WHERE} \FunctionTok{LOWER}\NormalTok{(ingredient.concept_name) }\OperatorTok{=} \StringTok{'ibuprofen'}
  \KeywordTok{AND}\NormalTok{ ingredient.concept_class_id }\OperatorTok{=} \StringTok{'Ingredient'}
  \KeywordTok{AND}\NormalTok{ ingredient.standard_concept }\OperatorTok{=} \StringTok{'S'}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}r@{}}
\toprule
PRESCRIPTION\_COUNT\tabularnewline
\midrule
\endhead
26871214\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{querylibrary}{%
\section{QueryLibrary}\label{querylibrary}}

TODO: update this section when QueryLibrary is finalized.

QueryLibrary is a library of commonly-used SQL queries for the CDM. It is available as an online application \footnote{\url{http:/data.ohdsi.org/QueryLibrary}} shown in Figure \ref{fig:queryLibrary}, and as an R package\footnote{\url{https://github.com/OHDSI/QueryLibrary}}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/SqlAndR/queryLibrary} 

}

\caption{QueryLibrary: a library of SQL queries against the CDM.}\label{fig:queryLibrary}
\end{figure}

The purpose of the library is to help new users learn how to query the CDM. The queries in the library have been reviewed and approved by the OHDSI community. The query library is primarily intended for training purposes, but is also a valuable resource for experienced users.

The QueryLibrary makes use of SqlRender to output the queries in the SQL dialect of choice. Users can also specify the CDM database schema, vocabulary database schema (if separate), and the Oracle temp schema (if needed), so the queries will be automatically rendered with these settings.

\hypertarget{designing-a-simple-study}{%
\section{Designing a simple study}\label{designing-a-simple-study}}

\hypertarget{problem-definition}{%
\subsection{Problem definition}\label{problem-definition}}

Angioedema is a well-known side-effect of ACE inhibitors (ACEi). \citet{slater_1988} estimate the incidence rate of angioedema in the first week of ACEi treatment to be one case per 3,000 patients per week. Here we seek to replicate this finding, and stratify by age and gender, thus answering the question

\begin{quote}
What is the rate of angioedema in the first week following ACEi treatment initiation, stratified by age and gender?
\end{quote}

\hypertarget{exposure}{%
\subsection{Exposure}\label{exposure}}

We'll define exposure as first exposure to a drug containing an ingredient in the ACEi class. By first we mean no earlier exposure to any ingredient in the class. We require 365 days of continuous observation time prior to the first exposure.

\hypertarget{outcome}{%
\subsection{Outcome}\label{outcome}}

We define angioedema as any occurrence of an angioedema diagnose code during an inpatient or emergency room (ER) visit.

\hypertarget{time-at-risk}{%
\subsection{Time-at-risk}\label{time-at-risk}}

We will compute the incidence rate in the first week following treatment initiation, irrespective of whether patients were exposed for the full week.

\hypertarget{implementing-the-study-using-sql-and-r}{%
\section{Implementing the study using SQL and R}\label{implementing-the-study-using-sql-and-r}}

Although we are not bound to any of the OHDSI tool conventions, it is helpful to follow the same principles. In this case, we will use SQL to populate a cohort table, similarly to how the OHDSI tools work. The COHORT table is defined in the CDM, and has a predefined set of fields that we will also use. We first must create the COHORT table in a database schema where we have write access, which likely is not the same as the database schema that holds the data in CDM format.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DatabaseConnector)}
\NormalTok{conn <-}\StringTok{ }\KeywordTok{connect}\NormalTok{(}\DataTypeTok{dbms =} \StringTok{"postgresql"}\NormalTok{,}
                \DataTypeTok{server =} \StringTok{"localhost/postgres"}\NormalTok{,}
                \DataTypeTok{user =} \StringTok{"joe"}\NormalTok{,}
                \DataTypeTok{password =} \StringTok{"secret"}\NormalTok{)}
\NormalTok{cdmDbSchema <-}\StringTok{ "cdm"}
\NormalTok{cohortDbSchema <-}\StringTok{ "scratch"}
\NormalTok{cohortTable <-}\StringTok{ "my_cohorts"}

\NormalTok{sql <-}\StringTok{ "}
\StringTok{CREATE TABLE @cohort_db_schema.@cohort_table (}
\StringTok{  cohort_definition_id INT,}
\StringTok{  cohort_start_date DATE,}
\StringTok{  cohort_end_date DATE,}
\StringTok{  subject_id BIGINT}
\StringTok{);}
\StringTok{"}
\KeywordTok{renderTranslateExecuteSql}\NormalTok{(conn, sql,}
                          \DataTypeTok{cohort_db_schema =}\NormalTok{ cohortDbSchema,}
                          \DataTypeTok{cohort_table =}\NormalTok{ cohortTable)}
\end{Highlighting}
\end{Shaded}

Here we have parameterized the database schema and table names, so we can easily adapt them to different environments. The result is an empty table on the database server.

\hypertarget{exposure-cohort}{%
\subsection{Exposure cohort}\label{exposure-cohort}}

Next we create our exposure cohort, and insert it into our COHORT table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sql <-}\StringTok{ "}
\StringTok{INSERT INTO @cohort_db_schema.@cohort_table (}
\StringTok{  cohort_definition_id,}
\StringTok{  cohort_start_date,}
\StringTok{  cohort_end_date,}
\StringTok{  subject_id}
\StringTok{)}
\StringTok{SELECT 1 AS cohort_definition_id,}
\StringTok{  cohort_start_date,}
\StringTok{  cohort_end_date,}
\StringTok{  subject_id}
\StringTok{FROM (}
\StringTok{  SELECT MIN(drug_exposure_start_date) AS cohort_start_date,}
\StringTok{    MIN(drug_exposure_end_date) AS cohort_end_date,}
\StringTok{    person_id AS subject_id}
\StringTok{  FROM @cdm_db_schema.drug_exposure}
\StringTok{  INNER JOIN @cdm_db_schema.concept_ancestor}
\StringTok{    ON drug_concept_id = descendant_concept_id}
\StringTok{  WHERE ancestor_concept_id IN (1335471, 1340128, 1341927,}
\StringTok{    1363749, 1308216, 1310756, 1373225, 1331235, 1334456,}
\StringTok{    1342439) -- ACE inhibitors}
\StringTok{  GROUP BY person_id}
\StringTok{) first_exposure}
\StringTok{INNER JOIN @cdm_db_schema.observation_period}
\StringTok{  ON subject_id = person_id}
\StringTok{    AND observation_period_start_date < cohort_start_date}
\StringTok{    AND observation_period_end_date > cohort_start_date}
\StringTok{WHERE DATEDIFF(DAY,}
\StringTok{               observation_period_start_date,}
\StringTok{               cohort_start_date) >= 365;}
\StringTok{"}

\KeywordTok{renderTranslateExecuteSql}\NormalTok{(conn, sql,}
                          \DataTypeTok{cohort_db_schema =}\NormalTok{ cohortDbSchema,}
                          \DataTypeTok{cohort_table =}\NormalTok{ cohortTable,}
                          \DataTypeTok{cdm_db_schema =}\NormalTok{ cdmDbSchema)}
\end{Highlighting}
\end{Shaded}

Here we use the DRUG\_EXPOSURE table, and join it the CONCEPT\_ANCESTOR table, thus allowing us to search for the ACEi ingredients and all their descendants, i.e.~all drugs containing an ACEi. We take the first drug exposure per person, and then join to the OBSERVATION\_PERIOD table, and because a person can have several observation periods we must make sure we only join to the period containing the drug exposure. We then require at least 365 days between the observation\_period\_start\_date and the cohort\_start\_date.

\hypertarget{outcome-cohort}{%
\subsection{Outcome cohort}\label{outcome-cohort}}

Finally, we must create our outcome cohort:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sql <-}\StringTok{ "}
\StringTok{INSERT INTO @cohort_db_schema.@cohort_table (}
\StringTok{ cohort_definition_id,}
\StringTok{ cohort_start_date,}
\StringTok{ cohort_end_date,}
\StringTok{subject_id}
\StringTok{)}
\StringTok{SELECT 2 AS cohort_definition_id,}
\StringTok{  cohort_start_date,}
\StringTok{  cohort_end_date,}
\StringTok{  subject_id}
\StringTok{FROM (}
\StringTok{  SELECT DISTINCT person_id AS subject_id,}
\StringTok{    condition_start_date AS cohort_start_date,}
\StringTok{    condition_end_date AS cohort_end_date}
\StringTok{  FROM @cdm_db_schema.condition_occurrence}
\StringTok{  INNER JOIN @cdm_db_schema.concept_ancestor}
\StringTok{    ON condition_concept_id = descendant_concept_id}
\StringTok{  WHERE ancestor_concept_id = 432791 -- Angioedema}
\StringTok{) distinct_occurrence}
\StringTok{INNER JOIN @cdm_db_schema.visit_occurrence}
\StringTok{  ON subject_id = person_id}
\StringTok{  AND visit_start_date <= cohort_start_date}
\StringTok{  AND visit_end_date >= cohort_start_date}
\StringTok{WHERE visit_concept_id IN (262, 9203,}
\StringTok{    9201) -- Inpatient or ER;}
\StringTok{"}

\KeywordTok{renderTranslateExecuteSql}\NormalTok{(conn, sql,}
                          \DataTypeTok{cohort_db_schema =}\NormalTok{ cohortDbSchema,}
                          \DataTypeTok{cohort_table =}\NormalTok{ cohortTable,}
                          \DataTypeTok{cdm_db_schema =}\NormalTok{ cdmDbSchema)}
\end{Highlighting}
\end{Shaded}

Here we join the CONDITION\_OCCURRENCE table to the CONCEPT\_ANCESTOR table to find all occurrences of angioedema or any of its descendants. We use DISTINCT to make sure we only select one record per day, as we believe multiple angioedema diagnoses on the same day are more likely to be the same occurrence rather than multiple angioedema events. We join these occurrences to the VISIT\_OCCURRENCE table to ensure the diagnose was made in and inpatient or ER setting.

\hypertarget{incidence-rate-calculation}{%
\subsection{Incidence rate calculation}\label{incidence-rate-calculation}}

Now that our cohorts are in place, we can compute the incidence rate, stratified by age and gender:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sql <-}\StringTok{ "}
\StringTok{WITH tar AS (}
\StringTok{  SELECT concept_name AS gender,}
\StringTok{    FLOOR((YEAR(cohort_start_date) -}
\StringTok{          year_of_birth) / 10) AS age,}
\StringTok{    subject_id,}
\StringTok{    cohort_start_date,}
\StringTok{    CASE WHEN DATEADD(DAY, 7, cohort_start_date) >}
\StringTok{      observation_period_end_date}
\StringTok{    THEN observation_period_end_date}
\StringTok{    ELSE DATEADD(DAY, 7, cohort_start_date)}
\StringTok{    END AS cohort_end_date}
\StringTok{  FROM @cohort_db_schema.@cohort_table}
\StringTok{  INNER JOIN @cdm_db_schema.observation_period}
\StringTok{    ON subject_id = observation_period.person_id}
\StringTok{      AND observation_period_start_date < cohort_start_date}
\StringTok{      AND observation_period_end_date > cohort_start_date}
\StringTok{  INNER JOIN @cdm_db_schema.person}
\StringTok{    ON subject_id = person.person_id}
\StringTok{  INNER JOIN @cdm_db_schema.concept}
\StringTok{    ON gender_concept_id = concept_id}
\StringTok{  WHERE cohort_definition_id = 1 -- Exposure}
\StringTok{)}
\StringTok{SELECT days.gender,}
\StringTok{    days.age,}
\StringTok{    days,}
\StringTok{    CASE WHEN events IS NULL THEN 0 ELSE events END AS events}
\StringTok{FROM (}
\StringTok{  SELECT gender,}
\StringTok{    age,}
\StringTok{    SUM(DATEDIFF(DAY, cohort_start_date,}
\StringTok{      cohort_end_date)) AS days}
\StringTok{  FROM tar}
\StringTok{  GROUP BY gender,}
\StringTok{    age}
\StringTok{) days}
\StringTok{LEFT JOIN (}
\StringTok{  SELECT gender,}
\StringTok{      age,}
\StringTok{      COUNT(*) AS events}
\StringTok{  FROM tar}
\StringTok{  INNER JOIN @cohort_db_schema.@cohort_table angioedema}
\StringTok{    ON tar.subject_id = angioedema.subject_id}
\StringTok{      AND tar.cohort_start_date <= angioedema.cohort_start_date}
\StringTok{      AND tar.cohort_end_date >= angioedema.cohort_start_date}
\StringTok{  WHERE cohort_definition_id = 2 -- Outcome}
\StringTok{  GROUP BY gender,}
\StringTok{    age}
\StringTok{) events}
\StringTok{ON days.gender = events.gender}
\StringTok{  AND days.age = events.age;}
\StringTok{"}

\NormalTok{results <-}\StringTok{ }\KeywordTok{renderTranslateQuerySql}\NormalTok{(conn, sql,}
                                   \DataTypeTok{cohort_db_schema =}\NormalTok{ cohortDbSchema,}
                                   \DataTypeTok{cohort_table =}\NormalTok{ cohortTable,}
                                   \DataTypeTok{cdm_db_schema =}\NormalTok{ cdmDbSchema,}
                                   \DataTypeTok{snakeCaseToCamelCase =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We first create ``tar'', a CTE that contains all exposures with the appropriate time-at-risk. Note that we truncate the time-at-risk at the observation\_period\_end\_date. We also compute the age in 10-year bins, and identify the gender. The advantage of using a CTE is that we can use the same set of intermediate results several times in a query. In this case we use it to count the total amount of time-at-risk, as well as the number of angioedema events that occur during the time-at-risk.

We use \texttt{snakeCaseToCamelCase\ =\ TRUE} because in SQL we tend to use snake\_case for field names (because SQL in case-insensitive), whereas in R we tend to use camelCase (because R is case-sensitive). The \texttt{results} data frame column names will now be in camelCase.

With the help of the ggplot2 package we can easily plot our results:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compute incidence rate (IR) :}
\NormalTok{results}\OperatorTok{$}\NormalTok{ir <-}\StringTok{ }\DecValTok{1000} \OperatorTok{*}\StringTok{ }\NormalTok{results}\OperatorTok{$}\NormalTok{events }\OperatorTok{/}\StringTok{ }\NormalTok{results}\OperatorTok{$}\NormalTok{days }\OperatorTok{/}\StringTok{ }\DecValTok{7}

\CommentTok{# Fix age scale:}
\NormalTok{results}\OperatorTok{$}\NormalTok{age <-}\StringTok{ }\NormalTok{results}\OperatorTok{$}\NormalTok{age }\OperatorTok{*}\StringTok{ }\DecValTok{10}

\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{(results, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ age, }\DataTypeTok{y =}\NormalTok{ ir, }\DataTypeTok{group =}\NormalTok{ gender, }\DataTypeTok{color =}\NormalTok{ gender)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Age"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Incidence (per 1,000 patient weeks)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{images/SqlAndR/ir} \end{center}

\hypertarget{clean-up}{%
\subsection{Clean up}\label{clean-up}}

Don't forget to clean up the table we created, and to close the connection:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sql <-}\StringTok{ "}
\StringTok{TRUNCATE TABLE @cohort_db_schema.@cohort_table;}
\StringTok{DROP TABLE @cohort_db_schema.@cohort_table;}
\StringTok{"}
\KeywordTok{renderTranslateExecuteSql}\NormalTok{(conn, sql,}
                          \DataTypeTok{cohort_db_schema =}\NormalTok{ cohortDbSchema,}
                          \DataTypeTok{cohort_table =}\NormalTok{ cohortTable)}

\KeywordTok{disconnect}\NormalTok{(conn)}
\end{Highlighting}
\end{Shaded}

\hypertarget{compatibility}{%
\subsection{Compatibility}\label{compatibility}}

Because we use OHDSI SQL together with DatabaseConnector and SqlRender throughout, the code we reviewed here will run on any database platform supported by OHDSI.

Note that for demonstration purposes we chose to create our cohorts using hand-crafted SQL. It would probably have been more convenient to construct cohort definition in ATLAS, and use the SQL generated by ATLAS to instantiate the cohorts. ATLAS also produced OHDSI SQL, and can therefore easily be used together with SqlRender and DatabaseConnector.

\hypertarget{summary-4}{%
\section{Summary}\label{summary-4}}

\BeginKnitrBlock{rmdsummary}
\begin{itemize}
\item
  \textbf{SQL} (Structured Query Language) is a standard language for querying databases, including those that conform to the Common Data Model (CDM).
\item
  Different database platforms have different SQL dialects, and require different tools to query them.
\item
  The \textbf{SqlRender} and \textbf{DatabaseConnector} R packages provide a unified way to query data in the CDM, allowing the same analysis code to be run in different environments without modificiation.
\item
  By using R and SQL together we can implement custom analyses that are not supported by the OHDSI tools.
\item
  The \textbf{QueryLibrary} provides a collection of re-usable SQL queries for the CDM.
\end{itemize}
\EndKnitrBlock{rmdsummary}

\hypertarget{exercises-2}{%
\section{Exercises}\label{exercises-2}}

\textbf{Prerequisites}

For these exercises we assume R, R-Studio and Java have been installed as described in Section \ref{installR}. Also required are the \href{https://ohdsi.github.io/SqlRender/}{SqlRender}, \href{https://ohdsi.github.io/DatabaseConnector/}{DatabaseConnector}, and \href{https://ohdsi.github.io/Eunomia/}{Eunomia} packages, which can be installed using:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"SqlRender"}\NormalTok{, }\StringTok{"DatabaseConnector"}\NormalTok{, }\StringTok{"devtools"}\NormalTok{))}
\NormalTok{devtools}\OperatorTok{::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"ohdsi/Eunomia"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The Eunomia package provides a simulated dataset in the CDM that will run inside your local R session. The connection details can be obtained using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{connectionDetails <-}\StringTok{ }\NormalTok{Eunomia}\OperatorTok{::}\KeywordTok{getEunomiaConnectionDetails}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The CDM database schema is ``main''.

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:exercisePeopleCount}{}{\label{exr:exercisePeopleCount} }Using SQL and R, compute how many people are in the database.
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:exerciseCelecoxibUsers}{}{\label{exr:exerciseCelecoxibUsers} }Using SQL and R, compute how many people have at least one prescription of celecoxib.
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:exerciseGiBleedsDuringCelecoxib}{}{\label{exr:exerciseGiBleedsDuringCelecoxib} }Using SQL and R, compute how many diagnoses of gastrointestinal haemorrhage occur during exposure to celecoxib. (Hint: the concept ID for gastrointestinal haemorrhage is \href{http://athena.ohdsi.org/search-terms/terms/192671}{192671}.)
\EndKnitrBlock{exercise}

Suggested answers can be found in Appendix \ref{SqlAndRanswers}.

\hypertarget{Cohorts}{%
\chapter{Building the building blocks: cohorts}\label{Cohorts}}

Contributors: Kristin Kostka, Patrick Ryan, Jon Duke, Juan Banda \& Joel Swerdel

Cohorts are used throughout OHDSI analytical tools and network studies as the primary building blocks for running high quality, systematic research. Cohort definitions vary from study to study depending on the research question of interest. Each cohort defines a specific way to represent a person with a condition or exposure using data in an observational health database. Thus, cohorts are an important component in documenting the methods of an observational research study.

The chapter serves to explain what is meant by creating and sharing cohort definitions, the methods for developing cohorts, and examples of how to build your own cohorts using ATLAS (see Chapter \ref{OhdsiAnalyticsTools}) and SQL queries against the Common Data Model (CDM).

\hypertarget{theory}{%
\section{Theory}\label{theory}}

\BeginKnitrBlock{rmdimportant}
\textbf{OHDSI Cohort Definition:} A cohort is defined as a set of persons who satisfy one or more inclusion criteria for a duration of time.
\EndKnitrBlock{rmdimportant}

In many peer-reviewed scientific manuscripts, a cohort definition is suggested to be analogous to a codeset of specific clinical codes (e.g.~ICD-9/ICD-10, NDC, HCPCS, etc). While codesets are an important piece to assembling a cohort, a cohort definition is not simply a codesets. A cohort definition requires logic for how to use the codeset in a criteria. A well documented cohort specifies how a patient enters a cohort, a patient exits a cohort and any additional inclusion criteria that impacts how to observe a patient's time-at-risk.

\BeginKnitrBlock{rmdimportant}
The term \emph{cohort} is often interchanged with the term phenotype. The term \emph{phenotype} is applied to patient characteristics inferred from electronic health record (EHR) data (Hripcsak citation). The goal is to draw conclusions about a target concept based on raw EHR data, claims data, or other clinically relevant data. Thus, a \emph{cohort} is a set of persons who satisfy one or more inclusion criteria (a phenotype) for a duration of time. A cohort in itself is not a phenotype but a phenotype can be used to create a cohort.
\EndKnitrBlock{rmdimportant}

There are two main approaches to constructing a cohort: \textbf{1) rules-based design} or \textbf{2) probabilistic design}. A rules-based cohort design relies heavily on the domain expertise of the individual designing the cohort to use their knowledge of the therapeutic area of interest to build rules to qualify potential cohort membership. Conversely, a probabilistic design mines already available data to identify and qualify potential cohort membership through machine-suggested patterns. The next sections will discuss these approaches in further detail.

\hypertarget{rules-based-cohort-design}{%
\subsection{Rules-based cohort design}\label{rules-based-cohort-design}}

A rules-based OHDSI cohort definition begins by an expert-consensus stating one or more inclusion criteria (e.g.~``people with angioedema'') in a specific duration of time (e.g.~``who developed this condition within the last 6 months'').

When creating a cohort definition, you need to ask yourself the following questions:

\begin{itemize}
\tightlist
\item
  \emph{What initial event(s) define cohort entry?}
\item
  \emph{What inclusion criteria are applied to the initial events?}
\item
  \emph{What defines a person's cohort exit?}
\end{itemize}

To visualize the importance of these criteria, think of how this information comes together in a person's timeline. The OBSERVATION\_PERIOD table creates the window for which we see the person in the data.

\begin{figure}
\includegraphics[width=1\linewidth]{images/Cohorts/cohort-build} \caption{Cohort Creation}\label{fig:cohortBuild}
\end{figure}

\emph{Cohort entry criteria:} The cohort entry event can be one or many clinical attributes which dictate an individual patient's eligibility to be included in a cohort. Events are recorded time-stamped observations for the persons, such as drug exposures, conditions, procedures, measurements and visits. The event index date is set to be equal to the event start date. Initial events defined by a domain, concept set, and any domain-specific attributes required.

\emph{Inclusion criteria:} The qualifying cohort will be defined as all persons who have an initial event and satisfy all qualifying inclusion criteria. Each inclusion criterion is defined by domain(s), concept set(s), domain-specific attributes, and the temporal logic relative to initial events. Each qualifying inclusion criterion can be evaluated to determine the impact of the criteria on the attrition of persons from the initial cohort.

\emph{Cohort exit criteria:} The cohort exit event signifies when a person no longer qualifies for cohort membership. Cohort exit can be defined in multiple ways such as the end of the observation period, a fixed time interval relative to the initial entry event, the last event in a sequence of related observations (e.g.~persistent drug exposure) or through other censoring of observation period. Cohort exit strategy will impact whether a person can belong to the cohort multiple times during different time intervals.

\emph{Time-at-risk:} In order to interpret risk of a specific outcome, which will be defined as a separate cohort definition, it is necessary to know the length of time that applies. A time-at-risk criteria states the period of time in which the cohort must be in the data following the cohort entry criteria. The time-at-risk will vary based on whether you're observing an acute/short term trend or a chronic/long term trend.

\begin{figure}
\includegraphics[width=1\linewidth]{images/Cohorts/cohort-TAR} \caption{Time-at-Risk Construction}\label{fig:cohortTar}
\end{figure}

In traditional study design, we would categorize time-at-risk for `on treatment' as the entirety of the time between when a person meets cohort entry through the cohort exit criteria. An `intent-to-treat' design would be the entirety of the time from the cohort start through the observation period ending (e.g.~when the person leaves the data because they've switched physicians, insurance carriers, etc).

The use of these criteria may present a number of unique nuances to an OHDSI cohort including:

\begin{itemize}
\tightlist
\item
  One person may belong to multiple cohorts
\item
  One person may belong to the same cohort at multiple different time periods
\item
  One person may not belong to the same cohort multiple times during the same period of time
\item
  One cohort may have zero or more members
\end{itemize}

Throughout the Book of OHDSI, we will detail how to address these consequences in your overall study design. In each respective methodology, we will discuss how you can configure a methods package to address how one person shows up in multiple cohorts being studied.

\hypertarget{probabilistic-cohort-design-using-aphrodite}{%
\subsection{Probabilistic cohort design using APHRODITE}\label{probabilistic-cohort-design-using-aphrodite}}

Rules-based cohort design are a popular method for assembling cohort definitions. However, assembling necessary expert consensus to create a study cohort can be prohibitively time consuming. Probabilistic cohort design is an alternative, machine-driven method to expedite the selection of cohort attributes. In this method, supervised learning allows a phenotyping algorithm to learn from a set of labeled examples (cases) of what attributes contribute to cohort membership. This algorithm can then be used to better ascertain the defining characteristics of a phenotype and what trade offs occur in overall study accuracy when choosing to modify phenotype criteria.

To apply this approach on OMOP data, OHDSI community researchers created Automated PHenotype Routine for Observational Definition, Identification, Training and Evaluation (APHRODITE), an R-package cohort building framework that combines the ability of learning from imperfectly labeled data and the Anchor learning framework for improving selected features in the phenotype models, for use with the OHDSI/OMOP CDM (reference: \url{https://www.ncbi.nlm.nih.gov/pubmed/28815104}). APHRODITE is an open-source package (\url{https://github.com/OHDSI/Aphrodite}) available for use which provides the OHDSI data network to the ability to start building electronic phenotype models that leverage machine learning techniques and go beyond traditional rule based approaches to cohort building.

\hypertarget{phenotype-evaluation}{%
\section{Phenotype Evaluation}\label{phenotype-evaluation}}

The systematic reuse of cohort definitions and the subsequent evaluation of phenotypes to characterize components of disease remains an ongoing piece of work within the OHDSI Community. A literature review of over 33 studies found significant heterogeneity in phenotype algorithms used, validation methods, and results (Swerdel reference). In general, the validation of a rules-based cohort definition or probabilistic algorithm can be thought of as a test of the proposed cohort compared to some form of ``gold standard'' reference (e.g.~manual chart review of cases).

\begin{figure}
\includegraphics[width=1\linewidth]{images/Cohorts/cohort-PPV} \caption{Algorithm Evaluation}\label{fig:cohortPpv}
\end{figure}

For a complete validation of an algorithm, we need to calculate:

\begin{itemize}
\item
  \textbf{Sensitivity} = True Positive (TP) / (True Positive + False Negative)
\item
  \textbf{Specificity} = True Negative (TN) / (True Negative + False Positive)
\item
  \textbf{Positive Predictive Value} = TP / (True Positive + False Positive)
\end{itemize}

This framework continues to be utilized across cohort definition research to evaluate the utility of reuse of cohorts across different electronic health data.

\hypertarget{ohdsi-gold-standard-phenotype-library}{%
\section{OHDSI Gold Standard Phenotype Library}\label{ohdsi-gold-standard-phenotype-library}}

To assist the community in evaluation of existing cohort definitions and algorithms, the OHDSI Gold Standard Phenotype Library (GSPL) Workgroup was formed. The purpose of the GSPL workgroup is to provide additional leadership to the development of community-backed cohort libraries from rules-based and probabilistic methods. The GPSL enable members of the OHDSI community to find, evaluate, and utilize community-validated cohort definitions for research and other activities. These ``gold standard'' definitions will reside in a library, the entries of which are held to specific standards of design and evaluation. For additional information related to the GSPL, consult the OHDSI workgroup page (\url{https://www.ohdsi.org/web/wiki/doku.php?id=projects:workgroups:gold-library-wg}).

\hypertarget{practice}{%
\section{Practice}\label{practice}}

Building a cohort starts with asking a question: ``I want to find patients who initiate ACE inhibitors monotherapy as first-line treatments for hypertension.''

Before you can define a cohort, you will need to construct OMOP concept sets. OMOP concept sets represent the sets of clinical codes that are strung together with other logical expressions to create your cohort. A detailed discussion of OMOP concept sets can be found \ref{StandardizedVocabularies}. Cohort inclusion criteria are created using specific attributes of data in the OMOP CDM (e.g.~condition occurrence, drug era, drug exposure, observation period, visit, etc). OHDSI domains are analogous to building blocks to contribute cohort attributes:

\begin{figure}
\includegraphics[width=1\linewidth]{images/Cohorts/cohort-legos} \caption{Building Blocks of Cohorts}\label{fig:cohortLegos}
\end{figure}

Prior to building a cohort, refer to the Common Data Model (Chapter \ref{CommonDataModel}) to understand what data elements are available for defining a cohort. When you are building a cohort, you should consider which of these is more important to you, finding all the eligible patients? \emph{vs.} Getting only the ones you are confident about?

Your strategy to construct your cohort will depend on your definition stringency. The right cohort design will depend on the question you're trying to answer. You may opt to build a cohort definition that: uses everything you can get, uses the lowest common denominator so you can share or is a compromise of the two. It is ultimately at the researcher's discretion what, threshold of stringency is necessary to adequately study the cohort of interest.

\hypertarget{using-atlas}{%
\subsection{Using ATLAS}\label{using-atlas}}

Missing: need to add high quality screenshots.

\hypertarget{using-sql}{%
\subsection{Using SQL}\label{using-sql}}

Missing: need to build tables for equivalent code.

\hypertarget{exercises-3}{%
\section{Exercises}\label{exercises-3}}

To be created.

\hypertarget{Characterization}{%
\chapter{Characterization}\label{Characterization}}

ATLAS' incidence rate calculator + cohort characterization tool

FeatureExtraction package: \url{https://github.com/OHDSI/FeatureExtraction}

Case study: characteristics + IRs of some cohorts

Example .. \url{http://www.pnas.org/content/113/27/7329}

\hypertarget{PopulationLevelEstimation}{%
\chapter{Population-level estimation}\label{PopulationLevelEstimation}}

\emph{Chapter leads: Martijn Schuemie, David Madigan, Marc Suchard \& Patrick Ryan}

\index{population-level estimation}

Observational healthcare data, such as administrative claims and electronic health records, offer opportunities to generate real-world evidence about the effect of treatments that can meaningfully improve the lives of patients. In this chapter we focus on population-level effect estimation, that is, the estimation of average causal effects of exposures (e.g.~medical interventions such as drug exposures or procedures) on specific health outcomes of interest. In what follows, we consider two different estimation tasks:

\begin{itemize}
\tightlist
\item
  \textbf{Direct effect estimation}: estimating the effect of an exposure on the risk of an outcome, as compared to no exposure. \index{direct effect estimation}
\item
  \textbf{Comparative effect estimation}: estimating the effect of an exposure (the target exposure) on the risk of an outcome, as compared to another exposure (the comparator exposure). \textbackslash index\{comparative effect estimation
\end{itemize}

In both cases, the patient-level causal effect contrasts a factual outcome, i.e., what happened to the exposed patient, with a counterfactual outcome, i.e., what would have happened had the exposure not occurred (direct) or had a different exposure occurred (comparative). Since any one patient reveals only the factual outcome (the fundamental problem of causal inference), the various effect estimation designs employ different analytic devices to shed light on the counterfactual outcomes.

Use-cases for population-level effect estimation include treatment selection, safety surveillance, and comparative effectiveness. Methods can test specific hypotheses one-at-a-time (e.g.~`signal evaluation') or explore multiple-hypotheses-at-once (e.g.~`signal detection'). In all cases, the objective remains the same: to produce a high-quality estimate of the causal effect.

In this chapter we first describe various population-level estimation study designs, all of which are implemented as R packages in the \href{https://ohdsi.github.io/MethodsLibrary/}{OHDSI Methods Library}. We then detail the design of an example estimation study, followed by step-by-step guides of how to implement the design using ATLAS and R. Finally, we review the various outputs generated by the study, including study diagnostics and effect size estimates.

\hypertarget{CohortMethod}{%
\section{The cohort method design}\label{CohortMethod}}

\index{cohort method}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/PopulationLevelEstimation/cohortMethod} 

}

\caption{The new-user cohort design. Subjects observed to initiate the target treatment are compared to those initiating the comparator treatment. To adjust for differences between the two treatment groups several adjustment strategies can be used, such as stratification, matching, or weighting by the propensity score, or by adding baseline characateristcs to the outcome model. The chararacteristics included in the propensity model or outcome model are captured prior to treatment initiation.}\label{fig:cohortMethod}
\end{figure}

The cohort method attempts to emulate a randomized clinical trial \citep{hernan_2016}. Subjects that are observed to initiate one treatment (the target) are compared to subjects initiating another treatment (the comparator) and are followed for a specific amount of time following treatment initiation, for example the time they stay on the treatment. We can specify the questions we wish to answer in a cohort study by making the five choices highlighted in Table \ref{tab:cmChoices}.

\begin{longtable}[]{@{}ll@{}}
\caption{\label{tab:cmChoices} Main design choices in a comparative cohort design.}\tabularnewline
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Target cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
A cohort representing the target treatment\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Comparator cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
A cohort representing the comparator treatment\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Outcome cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
A cohort representing the outcome of interest\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Time-at-risk\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
At what time (often relative to the target and comparator cohort start and end dates) do we consider the risk of the outcome?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Model\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
The model used to estimate the effect while adjusting for differences between the target and comparator\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

The choice of model specifies, among others, the type of model. For example, we could use a logistic regression, which evaluates whether or not the outcome has occurred, and produces an odds ratio. A logistic regression assumes the time-at-risk is of the same length for both target and comparator, or is irrelevant. Alternatively, we could choose a Poisson regression which estimates the incidence rate ratio, assuming a constant incidence rate. Often a Cox regression is used which considers time to first outcome to estimate the hazard ratio, assuming proportional hazards between target and comparator.

\BeginKnitrBlock{rmdimportant}
The new-user cohort method inherently is a method for comparative effect estimation, comparing one treatment to another. It is difficult to use this method to compare a treatment against no treatment, since it is hard to define a group of unexposed people that is comparable with the exposed group. If one wants to use this design for direct effect estimation, the preferred way is to select a comparator treatment for the same indication as the exposure of interest, where the comparator treatment is believed to have no effect on the outcome. Unfortunately, such a comparator might not always be available.
\EndKnitrBlock{rmdimportant}

A key concern is that the patients receiving the target treatment may systematically differ from those receiving the comparator treatment. For example, suppose the target cohort is, on average, 60 years old, whereas the comparator cohort is on average 40 years old. Comparing target to comparator with respect to any age-related health outcome (e.g.~stroke) might then show substantial differences. An uninformed investigator might reach the conclusion there is a causal association between the target treatment and stroke as compared to the comparator. More prosaically or commonplace, the investigator might conclude that there exist target patients that experienced stroke that would not have done so had they received the comparator. This conclusion could well be entirely incorrect! Maybe those target patients disproportionately experienced stroke simply because they are older; maybe the target patients that experienced stroke might well have done so even if they had received the comparator. In this context, age is a ``confounder''.

\hypertarget{propensity-scores}{%
\subsection{Propensity scores}\label{propensity-scores}}

\index{propensity score}

In a randomized trial, a (virtual) coin toss assigns patients to their respective groups. Thus, by design, the probability that a patient receives the target treatment as against the comparator treatment does not relate in any way to patient characteristics such as age. The coin has no knowledge of the patient, and, what's more, we know with certainty the exact probability that a patient receives the target exposure. As a consequence, and with increasing confidence as the number of patients in the trial increases, the two groups of patients essentially \emph{cannot} differ systematically with respect to \emph{any} patient characteristic. This guaranteed balance holds true for characteristics that the trial measured (such as age) as well as characteristics that the trial failed to measure.

For a given patient, the \emph{propensity score} (PS) is the probability that that patient received the target treatment as against the comparator. \citep{rosenbaum_1983} In a balanced two-arm randomized trial, the propensity score is 0.5 for every patient. In a propensity score-adjusted observational study, we estimate the probability of a patient receiving the target treatment based on what we can observe in the data on and before the time of treatment initiation (irrespective of the treatment they actually received). This a straightforward predictive modeling application; we fit a model (e.g.~a logistic regression) that predicts whether a subject receives the target treatment, and use this model to generate predicted probabilities (the PS) for each subject. Unlike in a standard randomized trial, different patients will have different probabilities of receiving the target treatment. The PS can be used in several ways, for example by matching target subjects to comparator subjects with similar PS, by stratifying the study population based on the PS, or by weighting subjects using Inverse Probability of Treatment Weighting (IPTW) derived from the PS. When matching we can select just one comparator subject for each target subject, or we can allow more than one comparator subject per target subject, a technique know as variable-ratio matching. \citep{rassen_2012}

For example, suppose we use one-on-one PS matching, and that Jan has a priori probability of 0.4 of receiving the target treatment and in fact receives the target treatment. If we can find a patient (named Jun) that also had an a priori probability of 0.4 of receiving the target treatment but in fact received the comparator, the comparison of Jan and Jun's outcomes is like a mini-randomized trial, at least with respect to measured confounders. This comparison will yield an estimate of the Jan-Jun causal contrast that is as good as the one randomization would have produced. Estimation then proceeds as follows: for every patient that received the target, find one or more matched patients that received the comparator but had the same a priori probability of receiving the target. Compare the outcome for the target patient with the outcomes for the comparator patients within each of these matched groups.

Propensity scoring controls for measured confounders. In fact, if treatment assignment is ``strongly ignorable'' given measured characteristics, propensity scoring will yield an unbiased estimate of the causal effect. ``Strongly ignorable'' essentially means that there are no unmeasured confounders, and that the measured confounders are adjusted for appropriately. Unfortunately this is not a testable assumption. See Chapter \ref{MethodValidity} for further discussion of this issue.

\hypertarget{VariableSelection}{%
\subsection{Variable selection}\label{VariableSelection}}

In the past, PS were computed based on manually selected characteristics, and although the OHDSI tools can support such practices, we prefer using many generic characteristics (i.e.~characteristics that are not selected based on the specific exposures and outcomes in the study). \citep{tian_2018} These characteristics include demographics, as well as all diagnoses, drug exposures, measurement, and medical procedures observed prior to and on the day of treatment initiation. A model typically involves 10,000 to 100,000 unique characteristics, which we fit using large-scale regularized regression \citep{suchard_2013} implemented in the \href{https://ohdsi.github.io/Cyclops/}{Cyclops} package. In essence, we let the data appropriately weigh the characteristics.

\BeginKnitrBlock{rmdimportant}
We typically include the day of treatment initiation in the covariate capture window because many relevant data points such as the diagnosis leading to the treatment are recorded on that date. This does require us to explicitly exclude the target and comparator treatment from the set of covariates, because these are the things we are trying to predict.
\EndKnitrBlock{rmdimportant}

Some have argued that a data-driven approach to covariate selection that does not depend on clinical expertise to specify the ``right'' causal structure runs the risk of erroneously including so-called instrumental variables and colliders, thus increasing variance and potentially introducing bias. \citep{hernan_2002} However, these concerns are unlikely to have a large impact in real-world scenarios. \citep{schneeweiss_2018} Furthermore, in medicine the true causal structure is rarely known, and when different researchers are asked to identify the `right' covariates to include for a specific research question, each researcher invariably comes up with a different list, thus making the process irreproducible. Most importantly, our diagnostics such as inspection of the propensity model, evaluating balance on all covariates, and including negative controls would identify most problems related to colliders and instrumental variables.

\hypertarget{caliper}{%
\subsection{Caliper}\label{caliper}}

\index{caliper}

Since propensity scores fall on a continuum from 0 to 1, exact matching is rarely possible. Instead, the matching process finds patients that match the propensity score of a target patient(s) within some tolerance known as a ``caliper.'' Following \citet{austin_2011}, we use a default caliper of 0.2 standard deviations on the logit scale.

\hypertarget{overlap-preference-scores}{%
\subsection{Overlap: preference scores}\label{overlap-preference-scores}}

\index{preference score}

The propensity method requires that matching patients exist! As such, a key diagnostic shows the distribution of the propensity scores in the two groups. To facilitate interpretation, the OHDSI tools plot a transformation of the propensity score called the ``preference score''. \citep{walker_2013} The preference score adjusts for the ``market share'' of the two treatments. For example, if 10\% of patients receive the target treatment (and 90\% receive the comparator treatment), then patients with a preference score of 0.5 have a 10\% probability of receiving the target treatment. Mathematically, the preference score is

\[\ln\left(\frac{F}{1-F}\right)=\ln\left(\frac{S}{1-S}\right)-\ln\left(\frac{P}{1-P}\right)\]

Where \(F\) is the preference score, \(S\) is the propensity score, and \(P\) is the proportion of patients receiving the target treatment.

\citet{walker_2013} discuss the concept of ``empirical equipoise''. They accept exposure pairs as emerging from empirical equipoise if at least half of the exposures are to patients with a preference score of between 0.3 and 0.7.

\hypertarget{balance}{%
\subsection{Balance}\label{balance}}

Good practice always checks that the PS adjustment succeeds in creating balanced groups of patients. Figure \ref{fig:balance} shows the standard OHDSI output for checking balance. For each patient characteristic, this plots the standardized difference between means between the two exposure groups before and after PS adjustment. Some guidelines recommend an after-adjustment standardized difference upper bound of 0.1. \citep{rubin_2001}

\hypertarget{the-self-controlled-cohort-design}{%
\section{The self-controlled cohort design}\label{the-self-controlled-cohort-design}}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/PopulationLevelEstimation/selfControlledCohort} 

}

\caption{The self-controlled cohort design. The rate of outcomes during exposure to the target is compared to the rate of outcomes in the time pre-exposure.}\label{fig:scc}
\end{figure}

The self-controlled cohort (SCC) design \citep{ryan_2013} compares the rate of outcomes during exposure to the rate of outcomes in the time just prior to the exposure. The four choices shown in Table \ref{tab:sccChoices} define a self-controlled cohort question.

\begin{longtable}[]{@{}ll@{}}
\caption{\label{tab:sccChoices} Main design choices in a self-controlled cohort design.}\tabularnewline
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Target cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
A cohort representing the treatment\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Outcome cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
A cohort representing the outcome of interest\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Time-at-risk\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
At what time (often relative to the target cohort start and end dates) do we consider the risk of the outcome?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Control time\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
The time period used as the control time\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Because the same subject that make up the exposed group are also used as the control group, no adjustment for between-person differences need to be made. However, the method is vulnerable to other differences, such as differences in the baseline risk of the outcome between different time periods.

\hypertarget{the-case-control-design}{%
\section{The case-control design}\label{the-case-control-design}}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/PopulationLevelEstimation/caseControl} 

}

\caption{The case-control design. Subjects with the outcome (cases) are compared to subjects without the outcome (controls) in terms of their exposure status. Often, cases and controls are matched on various characteristics such as age and sex.}\label{fig:caseControl}
\end{figure}

Case-control studies \citep{vandenbroucke_2012} consider the question ``are persons with a specific disease outcome exposed more frequently to a specific agent than those without the disease?'' Thus, the central idea is to compare ``cases'', i.e., subjects that experience the outcome of interest with ``controls'', i.e., subjects that did not experience the outcome of interest. The choices in Table \ref{tab:ccChoices} define a case-control question.

\begin{longtable}[]{@{}ll@{}}
\caption{\label{tab:ccChoices} Main design choices in a case-control design.}\tabularnewline
\toprule
\begin{minipage}[b]{0.22\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.22\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.22\columnwidth}\raggedright
Outcome cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
A cohort representing the cases (the outcome of interest)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
Control cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
A cohort representing the controls. Typically the control cohort is automatically derived from the outcome cohort using some selection logic\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
Target cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
A cohort representing the treatment\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
{[}Nesting cohort{]}\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
Optionally, a cohort defining the subpopulation from which cases and controls are drawn\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
Time-at-risk\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
At what time (often relative to the index date) do we consider exposure status?\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Often, one selects controls to match cases based on characteristics such as age and sex to make them more comparable. Another widespread practice is to nest the analysis within a specific subgroup of people, for example people that have all been diagnosed with one of the indications of the exposure of interest.

\hypertarget{the-case-crossover-design}{%
\section{The case-crossover design}\label{the-case-crossover-design}}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/PopulationLevelEstimation/caseCrossover} 

}

\caption{The case-crossover design. The time around the outcome is compared to a control date set at a predefined interval prior to the outcome date.}\label{fig:caseCrossover}
\end{figure}

The case-crossover \citep{maclure_1991} design evaluates whether the rate of exposure is different at the time of the outcome than at some predefined number of days prior to the outcome. It is trying to determine whether there is something special about the day the outcome occurred. Table \ref{tab:ccrChoices} shows the choices that define a case-crossover question:

\begin{longtable}[]{@{}ll@{}}
\caption{\label{tab:ccrChoices} Main design choices in a case-crossover design.}\tabularnewline
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Outcome cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
A cohort representing the cases (the outcome of interest)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Target cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
A cohort representing the treatment\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Time-at-risk\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
At what time (often relative to the index date) do we consider exposure status?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Control time\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
The time period used as the control time\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Since cases serve as their own control, it is a self-controlled design, and should therefore be robust to confounding due to between-person differences. One concern is that, because the outcome date is always later than the control date, the method will be positively biased if the overall frequency of exposure increases over time (or negatively biased if there is a decrease). To address this, the case-time-control design \citep{suissa_1995} was developed, which adds controls, matched for example on age and sex, to the case-crossover design to adjust for exposure trends.

\hypertarget{the-self-controlled-case-series-design}{%
\section{The self-controlled case series design}\label{the-self-controlled-case-series-design}}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/PopulationLevelEstimation/selfControlledCaseSeries} 

}

\caption{The Self-Controlled Case Series design. The rate of outcomes during exposure is compared to the rate of outcomes when not exposed.}\label{fig:selfControlledCaseSeries}
\end{figure}

The Self-Controlled Case Series (SCCS) design \citep{farrington_1995, whitaker_2006} compares the rate of outcomes during exposure to the rate of outcomes during all unexposed time, both before, between, and after exposures. It is a Poisson regression that is conditioned on the person. Thus, it seeks to answer the question: ``Given that a patient has the outcome, is the outcome more likely during exposed time compared to non-exposed time?''. The choices in Table \ref{tab:sccsChoices} define an SCCS question.

\begin{longtable}[]{@{}ll@{}}
\caption{\label{tab:sccsChoices} Main design choices in a self-controlled case series design.}\tabularnewline
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Target cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
A cohort representing the treatment\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Outcome cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
A cohort representing the outcome of interest\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Time-at-risk\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
At what time (often relative to the target cohort start and end dates) do we consider the risk of the outcome?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Model\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
The model to estimate the effect, including any adjustments for time-varying confounders\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Like other self-controlled designs, the SCCS is robust to confounding due to between-person differences, but vulnerable to confounding due to time-varying effects. Several adjustments are possible to attempt to account for these, for example by including age and season. A special variant of the SCCS includes not just the exposure of interest, but all other exposures to drugs recorded in the database, \citep{simpson_2013} potentially adding thousands of additional variables to the model. L1-regularization using cross-validation to select the regularization hyperparameter is applied to the coefficients of all exposures except the exposure of interest.

One important assumption underlying the SCCS is that the observation period end is independent of the date of the outcome. Because for some outcomes, especially ones that can be fatal such as stroke, this assumption can be violated. An extension to the SCCS has been developed that corrects for any such dependency. \citep{farrington_2011}

\hypertarget{designing-a-hypertension-study}{%
\section{Designing a hypertension study}\label{designing-a-hypertension-study}}

\hypertarget{problem-definition-1}{%
\subsection{Problem definition}\label{problem-definition-1}}

ACE inhibitors (ACEi) are widely used in patients with hypertension or ischemic heart disease, especially those with other comorbidities such as congestive heart failure, diabetes mellitus, or chronic kidney disease. \citep{zaman_2002} Angioedema, a serious and sometimes life-threatening adverse event that usually manifests as swelling of the lips, tongue, mouth, larynx, pharynx, or periorbital region, has been linked to the use of these medications. \citep{sabroe_1997} However, limited information is available about the absolute and relative risks for angioedema associated with the use of these medications. Existing evidence is primarily based on investigations of specific cohorts (e.g., predominantly male veterans or Medicaid beneficiaries), whose findings may not be generalizable to other populations, or based on investigations with few events, which provide unstable risk estimates \citep{powers_2012}. Several observational studies compare ACEi to beta-blockers for the risk of angioedema, \citep{magid_2010, toh_2012} but beta-blockers are no longer recommend as first-line treatment of hypertension. \citep{whelton_2018} A viable alternative treatment could be thiazides or thiazide-like diuretics (THZ), which could be just as effective in managing hypertension and its associated risks such as acute myocardial infarction (AMI), but without increasing the risk of angioedema.

The following will demonstrate how to apply our population-level estimation framework to observational healthcare data to address the following comparative estimation questions:

\begin{quote}
What is the risk of angioedema in new users of ACE inhibitors compared to new users of thiazide and thiazide-like diuretics?
\end{quote}

\begin{quote}
What is the risk of acute myocaridal infarction in new users of ACE inhibitors compared to new users of thiazide and thiazide-like diuretics?
\end{quote}

Since these are comparative effect estimation questions we will apply the cohort method as described in Section \ref{CohortMethod}.

\hypertarget{target-and-comparator}{%
\subsection{Target and comparator}\label{target-and-comparator}}

We consider patients new-users if their first observed treatment for hypertension was monotherapy with any active ingredient in either the ACEi or THZ class. We define mono therapy as not starting on any other anti-hypertensive drug in the seven days following treatment initiation. We require patients to have at least one year of prior continuous observation in the database before first exposure and a recorded hypertension diagnosis at or in the year preceding treatment initiation.

\hypertarget{outcome-1}{%
\subsection{Outcome}\label{outcome-1}}

We define angioedema as any occurrence of an angioedema condition concept during an inpatient or emergency room (ER) visit, and require there to be no angioedema diagnosis recorded in the seven days prior. We define AMI as any occurrence of an AMI condition concept during an inpatient or ER visit, and require there to be no AMI diagnosis record in the 180 days prior.

\hypertarget{time-at-risk-1}{%
\subsection{Time-at-risk}\label{time-at-risk-1}}

We define time-at-risk to start on the day after treatment initiation, and stop when exposure stops, allowing for a 30-day gap between subsequent drug exposures.

\hypertarget{model}{%
\subsection{Model}\label{model}}

We fit a PS model using the default set of covariates, including demographics, conditions, drugs, procedures, measurements, observations, and several co-morbidity scores. We exclude ACEi and THZ from the covariates. We perform variable-ratio matching and condition the Cox regression on the matched sets.

\hypertarget{study-summary}{%
\subsection{Study summary}\label{study-summary}}

\begin{longtable}[]{@{}ll@{}}
\caption{\label{tab:aceChoices} Main design choices four our comparative cohort study.}\tabularnewline
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Value\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Value\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Target cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
New users of ACE inhibitors as first-line monotherapy for hypertension.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Comparator cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
New users of thiazides or thiazide-like diuretics as first-line monotherapy for hypertension.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Outcome cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
Angioedema or acute myocardial infarction.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Time-at-risk\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
Starting the day after treatment initiation, stopping when exposure stops.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Model\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
Cox proportional hazards model using variable-ratio matching.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{control-questions}{%
\subsection{Control questions}\label{control-questions}}

To evaluate whether our study design produces estimates in line with the truth, we additionally include a set of control questions where the true effect size is known. Control questions can be divided in negative controls, having a hazard ratio of 1, and positive controls, having a known hazard ratio greater than 1. For several reasons we use real negative controls, and synthesize positive controls based on these negative controls. How to define and use control questions is discussed in detail in Chapter \ref{MethodValidity}.

\hypertarget{PleAtlas}{%
\section{Implementing the study using ATLAS}\label{PleAtlas}}

Here we demonstrate how this study can be implemented using the Estimation function in ATLAS. Click on \includegraphics{images/PopulationLevelEstimation/estimation.png} in the left bar of ATLAS, and create a new estimation study. Make sure to give the study an easy-to-recognize name. The study design can be saved at any time by clicking the \includegraphics{images/PopulationLevelEstimation/save.png} button.

In the Estimation design function, there are three sections: Comparisons, Analysis Settings, and Evaluation Settings. We can specify multiple comparisons and multiple analysis settings, and ATLAS will execute all combinations of these as separate analyses. Here we discuss each section:

\hypertarget{ComparisonSettings}{%
\subsection{Comparative cohort settings}\label{ComparisonSettings}}

A study can have one or more comparisons. Click on ``Add Comparison'', which will open a new dialog. Click on \includegraphics{images/PopulationLevelEstimation/open.png} to the select the target and comparator cohorts. By clicking on ``Add Outcome'' we can add our two outcome cohorts. We assume the cohorts have already been created in ATLAS as described in Chapter \ref{Cohorts}. The Appendix provides the full definitions of the target (Appendix \ref{AceInhibitorsMono}) , comparator (Appendix \ref{ThiazidesMono}), and outcome (Appendix \ref{Angioedema} and Appendix \ref{Ami}) cohorts. When done, the dialog should look like Figure \ref{fig:comparisons}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/PopulationLevelEstimation/comparisons} 

}

\caption{The comparison dialog}\label{fig:comparisons}
\end{figure}

Note that we can select multiple outcomes for a target-comparator pair. Each outcome will be treated independently, and will result in a separate analysis.

\textbf{Negative control outcomes}

Negative controls outcomes are outcomes that are not believed to be caused by either the target or the comparator, and where therefore the true hazard ratio equals 1. Ideally, we would have proper cohort definitions for each outcome cohort. However, typically, we only have a concept set, with one concept per negative control outcome, and some standard logic to turn these into outcome cohorts. Here we assume the concept set has already been created as described in Chapter \ref{MethodValidity} and can simply be selected. The negative control concept set should contain a concept per negative control, and not include descendants. Figure \ref{fig:ncConceptSet} shows the negative control concept set used for this study.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/PopulationLevelEstimation/ncConceptSet} 

}

\caption{Negative Control concept set.}\label{fig:ncConceptSet}
\end{figure}

\textbf{Concepts to include}

TODO: Update these sections when ATLAS interface has been updated.

When selecting concept to include, we can specify which covariates we would like to generate, for example to use in a propensity model. When specifying covariates here, all other covariates (aside from those you specified) are left out. We usually want to include all baseline covariates, letting the regularized regression build a model that balances all covariates. The only reason we might want to specify particular covariates is to replicate an existing study that manually picked covariates. These inclusions can be specified in this comparison section or in the analysis section, because sometimes they pertain to a specific comparison (e.g.~know confounders in a comparison), or sometimes they pertain to an analysis (e.g.~when evaluating a particular covariate selection strategy).

\textbf{Concepts to exclude}

Rather than specifying which concepts to include, we can instead specify concepts to \emph{exclude}. When we submit a concept set in this field, we use every covariate except for those that we submitted. When using the default set of covariates, which includes all drugs and procedures occurring on the day of treatment initiation, we must exclude the target and comparator treatment, and any concepts that are directly related to these. For example, if the target exposure is an injectable, we should not only exclude the drug, but also the injection procedure from the propensity model. In this example, the covariates we want to exclude are ACEi and THZ. Figure \ref{fig:covsToExclude} shows we select a concept set that includes all these concepts.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/PopulationLevelEstimation/covsToExclude} 

}

\caption{The concept set defining the concepts to exclude.}\label{fig:covsToExclude}
\end{figure}

After selecting the negative controls and covariates to exclude, the lower half of the comparisons dialog should look like Figure \ref{fig:comparisons2}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/PopulationLevelEstimation/comparisons2} 

}

\caption{The comparison window showing concept sets for negative controls and concepts to exclude.}\label{fig:comparisons2}
\end{figure}

\hypertarget{effect-estimation-analysis-settings}{%
\subsection{Effect estimation analysis settings}\label{effect-estimation-analysis-settings}}

After closing the comparisons dialog we can click on ``Add Analysis Settings''. In the box labeled ``Analysis Name'', we can give the analysis a unique name that is easy to remember and locate in the future. For example, we could set the name to ``Propensity score matching''.

\textbf{Study population}

There are a wide range of options to specify the study population; the set of subjects that will enter the analysis. Many of these overlap with options available when designing the target and comparator cohorts in the cohort definition tool. One reason for using the options in Estimation instead of in the cohort definition is re-usability: We can define the target, comparator, and outcome cohorts completely independently, and add dependencies between these at a later point in time. For example, if we wish to remove people who had the outcome before treatment initiation, we could do so in the definitions of the target and comparator cohort, but then we would need to create separate cohorts for every outcome! Instead, we can choose to have people with prior outcomes be removed in the analysis settings, and now we can reuse our target and comparator cohorts for our two outcomes of interest (as well as our negative control outcomes).

The \textbf{study start and end dates} can be used to limit the analyses to a specific period. The study end date also truncates risk windows, meaning no outcomes beyond the study end date will be considered. One reason for selecting a study start date might be that one of the drugs being studied is new and did not exist in an earlier time. Automatically adjusting for this can be done by answering ``yes'' to the question ``\textbf{Restrict the analysis to the period when both exposures are observed?}''. Another reason to adjust study start and end dates might be that medical practice changed over time (e.g., due to a drug warning) and we are only interested in the time where medicine was practiced a specific way.

The option ``\textbf{Should only the first exposure per subject be included?}'' can be used to restrict to the first exposure per patient. Often this is already done in the cohort definition, as is the case in this example. Similarly, the option ``\textbf{The minimum required continuous observation time prior to index date for a person to be included in the cohort}'' is often already set in the cohort definition, and can therefore be left at 0 here. Having observed time (as defined in the OBSERVATION\_PERIOD table) before the index date ensures that there is sufficient information about the patient to calculate a propensity score, and is also often used to ensure the patient is truly a new user, and therefore was not exposed before.

``\textbf{Remove subjects that are in both the target and comparator cohort?}'' defines, together with the option ``\textbf{If a subject is in multiple cohorts, should time-at-risk be censored when the new time-at-risk starts to prevent overlap?}'' what happens when a subject is in both target and comparator cohort. The first setting has three choices:

\begin{itemize}
\tightlist
\item
  ``\textbf{Keep All}'' indicating to keep the subjects in both cohorts. With this option it might be possible to double-count subjects and outcomes.
\item
  ``\textbf{Keep First}'' indicating to keep the subject in the first cohort that occurred.
\item
  ``\textbf{Remove All}'' indicating to remove the subject from both cohorts.
\end{itemize}

If the options ``keep all'' or ``keep first'' are selected, we may wish to censor the time when a person is in both cohorts. This is illustrated in Figure \ref{fig:tar}. By default, the time-at-risk is defined relative to the cohort start and end date. In this example, the time-at-risk starts one day after cohort entry, and stops at cohort end. Without censoring the time-at-risk for the two cohorts might overlap. This is especially problematic if we choose to keep all, because any outcome that occurs during this overlap (as shown) will be counted twice. If we choose to censor, the first cohort's time-at-risk ends when the second cohort's time-at-risk starts.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PopulationLevelEstimation/tar} 

}

\caption{Time-at-risk (TAR) for subjects who are in both cohorts, assuming time-at-risk starts the day after treatment initiation, and stops at exposure end.}\label{fig:tar}
\end{figure}

We can choose to \textbf{remove subjects that have the outcome prior to the risk window start}, because often a second outcome occurrence is the continuation of the first one. For instance, when someone develops heart failure, a second occurrence is likely, which means the heart failure probably never fully resolved in between. On the other hand, some outcomes are episodic, and it would be expected for patients to have more than one independent occurrence, like an upper respiratory infection. If we choose to remove people that had the outcome before, we can select \textbf{how many days we should look back when identifying prior outcomes}.

Our choices for our example study are shown in Figure \ref{fig:studyPopulation}. Because our target and comparator cohort definitions already restrict to the first exposure and require observation time prior to treatment initiation, we do not apply these criteria here.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/PopulationLevelEstimation/studyPopulation} 

}

\caption{Study population settings..}\label{fig:studyPopulation}
\end{figure}

\textbf{Covariate settings}

Here we specify the covariates to construct. These covariates are typically used in the propensity model, but can also be included in the outcome model (the Cox proporitional hazards model in this case). If we \textbf{click to view details} of our covariate settings, we can select which sets of covariates to construct. However, the recommendation is to use the default set, which constructs covariates for demographics, all conditions, drugs, procedures, measurements, etc.

We can modify the set of covariates by specifying concepts to \textbf{include} and/or \textbf{exclude}. These settings are the same as the ones found in Section \ref{ComparisonSettings} on comparison settings. The reason why they can be found in two places is because sometimes these settings are related to a specific comparison, as is the case here because we wish to exclude the drugs we are comparing, and sometimes the settings are related to a specific analysis. When executing an analysis for a specific comparison using specific analysis settings, the OHDSI tools will take the union of these sets.

The choice to \textbf{add descendants to include or exclude} affects this union of the two settings. So in this example we specified only the ingredients to exclude when defining the comparisons. Here we set ``Should descendant concepts be added to the list of excluded concepts?'' to ``Yes'' to also add all descendants.

Figure \ref{fig:covariateSettings} shows our choices for this study. Note that we have selected to add descendants to the concept to exclude, which we defined in the comparison settings in Figure \ref{fig:comparisons2}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/PopulationLevelEstimation/covariateSettings} 

}

\caption{Covariate settings.}\label{fig:covariateSettings}
\end{figure}

\textbf{Time at risk}

Time-at-risk is defined relative to the start and end dates of our target and comparator cohorts. In our example, we had set the cohort start date to start on treatment initiation, and cohort end date when exposure stops (for at least 30 days). We set the start of time-at-risk to one day after cohort start, so one day after treatment initiation. A reason to set the time-at-risk start to be later than the cohort start is because we may want to exclude outcome events that occur on the day of treatment initiation if we do not believe it biologically plausible they can be caused by the drug.

We set the end of the time-at-risk to the cohort end, so when exposure stops. We could choose to set the end date later if for example we believe events closely following treatment end may still be attributable to the exposure. In the extreme we could set the time-at-risk end to a large number of days (e.g.~99999) after the cohort end date, meaning we will effectively follow up subjects until observation end. Such a design is sometimes referred to as an \emph{intent-to-treat} design.

A patient with zero days at risk adds no information, so the \textbf{minimum days at risk} is normally set at one day. If there is a known latency for the side effect, then this may be increased to get a more informative proportion. It can also be used to create a cohort more similar to that of a randomized trial it is being compared to (e.g., all the patients in the randomized trial were observed for at least N days).

\BeginKnitrBlock{rmdimportant}
A golden rule in designing a cohort study is to never use information that falls after the cohort start date to define the study population, as this may introduce bias. For example, if we require everyone to have at least a year of time-at-risk, we will likely have limited our analyses to those who tolerate the treatment well. This setting should therefore be used with extreme care.
\EndKnitrBlock{rmdimportant}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/PopulationLevelEstimation/timeAtRisk} 

}

\caption{Time-at-risk settings.}\label{fig:timeAtRisk}
\end{figure}

\textbf{Propensity score adjustment}

We can opt to \textbf{trim} the study population, removing people with extreme PS values. We can choose to remove the top and bottom percentage, or we can remove subjects whose preference score falls outside the range we specify. Trimming the cohorts is generally not recommended because it requires discarding observations, which reduces statistical power. It may be desirable to trim in some cases, for example when using IPTW.

In addition to, or instead of trimming, we can choose to \textbf{stratify} or \textbf{match} on the propensity score. When stratifying we need to specify the \textbf{number of strata} and whether to select the strata based on the target, comparator, or entire study population. When matching we need to specify the \textbf{maximum number of people from the comparator group to match to each person in the target group}. Typical values are 1 for one-on-one matching, or a large number (e.g.~100) for variable-ratio matching. We also need to specify the \textbf{caliper}: the maximum allowed difference between propensity scores to allow a match. The caliper can be defined on difference \textbf{caliper scales}:

\begin{itemize}
\tightlist
\item
  \textbf{The propensity score scale}: the PS itself
\item
  \textbf{The standardized scale}: in standard deviations of the PS distributions
\item
  \textbf{The standardized logit scale}: in standard deviations of the PS distributions after the logit transformation to make the PS more normally distributed.
\end{itemize}

In case of doubt, we suggest using the default values, or consult the work on this topic by \citet{austin_2011}.

Fitting large-scale propensity models can be computationally expensive, so we may want to restrict the data used to fit the model to just a sample of the data. By default the maximum size of the target and comparator cohort is set to 250,000. In most studies this limit will not be reached. It is also unlikely that more data will lead to a better model. Note that although a sample of the data may be used to fit the model, the model will be used to compute PS for the entire population.

\textbf{Test each covariate for correlation with the target assignment?} If any covariate has an unusually high correlation (either positive or negative), this will throw an error. This avoids lengthy calculation of a propensity model only to discover complete separation. Finding very high univariate correlation allows you to review the covariate to determine why it has high correlation and whether it should be dropped.

Figure \ref{fig:psSettings} shows our choices for this study. Note that we select variable-ratio matching by setting the maximum number of people to match to 100.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/PopulationLevelEstimation/psSettings} 

}

\caption{Propenstity score adjustment settings.}\label{fig:psSettings}
\end{figure}

\textbf{Outcome model settings}

First, we need to \textbf{specify the statistical model we will use to estimate the relative risk of the outcome between target and comparator cohorts}. We can choose between Cox, Poisson, and logistic regression, as discussed briefly in Section \ref{CohortMethod}. For our example we choose a Cox proportional hazards model, which considers time to first event with possible censoring. Next, we need to specify \textbf{whether the regression should be conditioned on the strata}. One way to understand conditioning is to imagine a separate estimate is produced in each stratum, and then combined across strata. For one-to-one matching this is likely unnecessary and would just lose power. For stratification or variable-ratio matching it is required.

We can also choose to \textbf{add all covariates to the outcome model} to adjust the analysis. This can be done in addition or instead of using a propensity model. However, whereas there usually is ample data to fit a propensity model, with many people in both treatment groups, there is typically very little data to fit the outcome model, with only few people having the outcome. We therefore recommend keeping the outcome model as simple as possible and not include additional covariates.

Instead of stratifying or matching on the propensity score we can also choose to \textbf{use inverse probability of treatment weighting} (IPTW). If weighting is used it is often recommended to use some form of trimming to avoid extreme weights and therefore unstable estimates.

Figure \ref{fig:psSettings} shows our choices for this study. Because we use variable-ratio matching, we must condition the regression on the strata (i.e.~the matched sets).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/PopulationLevelEstimation/outcomeModelSettings} 

}

\caption{Outcome model settings.}\label{fig:outcomeModelSettings}
\end{figure}

\hypertarget{evaluationSettings}{%
\subsection{Evaluation settings}\label{evaluationSettings}}

As described in Chapter \ref{MethodValidity}, negative and positive controls should be included in our study to evaluate the operating characteristics, and perform empirical calibration.

\textbf{Negative control outcome cohort definition}

In Section \ref{ComparisonSettings} we selected a concept set representing the negative control outcomes. However, we need logic to convert concepts to cohorts to be used as outcomes in our analysis. ATLAS provides standard logic with three choices The first choice is whether to \textbf{use all occurrences} or just the \textbf{first occurrence} of the concept. The second choice determines \textbf{whether occurrences of descendant concepts should be considered}. For example, occurrences of the descendant ``ingrown nail of foot'' can also be counted as an occurrence of the ancestor ``ingrown nail''. The third choice specifies which domains should be considered when looking for the concepts.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/PopulationLevelEstimation/ncSettings} 

}

\caption{Negative control outcome cohort definition settings.}\label{fig:ncSettings}
\end{figure}

\textbf{Positive control synthesis}

In addition to negative controls we can also include positive controls, which are exposure-outcome pairs where a causal effect is believed to exist with known effect size. For various reasons real positive controls are problematic, so instead we rely on synthetic positive controls, derived from negative controls as described in Chapter \ref{MethodValidity}. Positive control synthesis is an advanced topic that we will skip for now.

TODO: Add positive control synthesis settings when ATLAS interface is updated.

\hypertarget{running-the-study-package}{%
\subsection{Running the study package}\label{running-the-study-package}}

Now that we have fully defined our study, we can export it as an executable R package. This package contains everything that is needed to execute the study at a site that has data in CDM. This includes the cohort definitions that can be used to instantiate the target, comparator and outcome cohorts, the negative control concept set and logic to create the negative control outcome cohorts, as well as the R code to execute the analysis. Before generating the package make sure to save your study, then click on the \textbf{Utilities} tab. Here we can review the set of analyses that will be performed. As mentioned before, every combination of a comparison and an analysis setting will results in a separate analysis. In our example we have specified two analyses: ACEi versus THZ for AMI, and ACEi versus THZ for angioedema, both using propensity score matching.

We must provide a name for our package, after which we can click on ``Download'' to download the zip file. The zip file contains an R package, with the usual required folder structure for R packages. \citep{Wickham_2015} To use this package we recommend using R Studio. If you are running R Studio locally, unzip the file, and double click the .Rproj file to open it in R Studio. If you are running R Studio on an R studio server, click \includegraphics{images/PopulationLevelEstimation/upload.png} to upload and unzip the file, then click on the .Rproj file to open the project.

Once you have opened the project in R Studio, you can open the README file, and follow the instructions. Make sure to change all file paths to existing paths on your system.

A common error message that may appear when running the study is ``High correlation between covariate(s) and treatment detected''. This indicates that when fitting the propensity model, some covariates were observed to be highly correlated with the exposure. Please review the covariates mentioned in the error message, and exclude them from the set of covariates if appropriate (see Section \ref{VariableSelection}).

\hypertarget{pleR}{%
\section{Implementing the study using R}\label{pleR}}

Instead of using ATLAS to write the R code that executes the study, we can also write the R code ourselves. One reason we might want to do this is because R offers far greater flexibility than is exposed in ATLAS. If we for example wish to use custom covariates, or a linear outcome model, we will need to write some custom R code, and combine it with the functionality provided by the OHDSI R packages.

For our example study we will rely on the \href{https://ohdsi.github.io/CohortMethod/}{CohortMethod} package to execute our study. CohortMethod extracts the necessary data from a database in the CDM and can use a large set of covariates for the propensity model. In the following example we first only consider angioedema as outcome. In Section \ref{MultipleAnalyses} we then describe how this can be extended to include AMI and the negative control outcomes.

\hypertarget{cohort-instantiation}{%
\subsection{Cohort instantiation}\label{cohort-instantiation}}

We first need to instantiate the target and outcome cohorts. Instantiating cohorts is described in Chapter \ref{Cohorts}. The Appendix provides the full definitions of the target (Appendix \ref{AceInhibitorsMono}), comparator (Appendix \ref{ThiazidesMono}), and outcome (Appendix \ref{Angioedema} ) cohorts. We will assume the ACEi, THZ, and angioedema cohorts have been instantiated in a table called \texttt{scratch.my\_cohorts} with cohort definition IDs 1,2, and 3 respectively.

\hypertarget{data-extraction}{%
\subsection{Data extraction}\label{data-extraction}}

We first need to tell R how to connect to the server. \href{https://ohdsi.github.io/CohortMethod/}{CohortMethod} uses the \href{https://ohdsi.github.io/DatabaseConnector/}{DatabaseConnector} package, which provides a function called \texttt{createConnectionDetails}. Type \texttt{?createConnectionDetails} for the specific settings required for the various database management systems (DBMS). For example, one might connect to a PostgreSQL database using this code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(CohortMethod)}
\NormalTok{connDetails <-}\StringTok{ }\KeywordTok{createConnectionDetails}\NormalTok{(}\DataTypeTok{dbms =} \StringTok{"postgresql"}\NormalTok{,}
                                       \DataTypeTok{server =} \StringTok{"localhost/ohdsi"}\NormalTok{,}
                                       \DataTypeTok{user =} \StringTok{"joe"}\NormalTok{,}
                                       \DataTypeTok{password =} \StringTok{"supersecret"}\NormalTok{)}

\NormalTok{cdmDbSchema <-}\StringTok{ "my_cdm_data"}
\NormalTok{cohortDbSchema <-}\StringTok{ "scratch"}
\NormalTok{cohortTable <-}\StringTok{ "my_cohorts"}
\NormalTok{cdmVersion <-}\StringTok{ "5"}
\end{Highlighting}
\end{Shaded}

The last four lines define the \texttt{cdmDbSchema}, \texttt{cohortDbSchema}, and \texttt{cohortTable} variables, as well as the CDM version. We will use these later to tell R where the data in CDM format live, where the cohorts of interest have been created, and what version CDM is used. Note that for Microsoft SQL Server, database schemas need to specify both the database and the schema, so for example \texttt{cdmDbSchema\ \textless{}-\ "my\_cdm\_data.dbo"}.

Now we can tell CohortMethod to extract the cohorts, construct covariates, and extract all necessary data for our analysis:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# target and comparator ingredient concepts:}
\NormalTok{aceI <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1335471}\NormalTok{,}\DecValTok{1340128}\NormalTok{,}\DecValTok{1341927}\NormalTok{,}\DecValTok{1363749}\NormalTok{,}\DecValTok{1308216}\NormalTok{,}\DecValTok{1310756}\NormalTok{,}\DecValTok{1373225}\NormalTok{,}
          \DecValTok{1331235}\NormalTok{,}\DecValTok{1334456}\NormalTok{,}\DecValTok{1342439}\NormalTok{)}
\NormalTok{thz <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1395058}\NormalTok{,}\DecValTok{974166}\NormalTok{,}\DecValTok{978555}\NormalTok{,}\DecValTok{907013}\NormalTok{)}

\CommentTok{# Define which types of covariates must be constructed:}
\NormalTok{cs <-}\StringTok{ }\KeywordTok{createDefaultCovariateSettings}\NormalTok{(}\DataTypeTok{excludedCovariateConceptIds =} \KeywordTok{c}\NormalTok{(aceI, }
\NormalTok{                                                                     thz),}
                                     \DataTypeTok{addDescendantsToExclude =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{#Load data:}
\NormalTok{cmData <-}\StringTok{ }\KeywordTok{getDbCohortMethodData}\NormalTok{(}\DataTypeTok{connectionDetails =}\NormalTok{ connectionDetails,}
                                \DataTypeTok{cdmDatabaseSchema =}\NormalTok{ cdmDatabaseSchema,}
                                \DataTypeTok{oracleTempSchema =} \OtherTok{NULL}\NormalTok{,}
                                \DataTypeTok{targetId =} \DecValTok{1}\NormalTok{,}
                                \DataTypeTok{comparatorId =} \DecValTok{2}\NormalTok{,}
                                \DataTypeTok{outcomeIds =} \DecValTok{3}\NormalTok{,}
                                \DataTypeTok{studyStartDate =} \StringTok{""}\NormalTok{,}
                                \DataTypeTok{studyEndDate =} \StringTok{""}\NormalTok{,}
                                \DataTypeTok{exposureDatabaseSchema =}\NormalTok{ cohortDbSchema,}
                                \DataTypeTok{exposureTable =}\NormalTok{ cohortTable,}
                                \DataTypeTok{outcomeDatabaseSchema =}\NormalTok{ cohortDbSchema,}
                                \DataTypeTok{outcomeTable =}\NormalTok{ cohortTable,}
                                \DataTypeTok{cdmVersion =}\NormalTok{ cdmVersion,}
                                \DataTypeTok{firstExposureOnly =} \OtherTok{FALSE}\NormalTok{,}
                                \DataTypeTok{removeDuplicateSubjects =} \OtherTok{FALSE}\NormalTok{,}
                                \DataTypeTok{restrictToCommonPeriod =} \OtherTok{FALSE}\NormalTok{,}
                                \DataTypeTok{washoutPeriod =} \DecValTok{0}\NormalTok{,}
                                \DataTypeTok{covariateSettings =}\NormalTok{ cs)}
\NormalTok{cmData}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## CohortMethodData object
## 
## Treatment concept ID: 1
## Comparator concept ID: 2
## Outcome concept ID(s): 3
\end{verbatim}

There are many parameters, but they are all documented in the \href{https://ohdsi.github.io/CohortMethod/reference/}{CohortMethod manual}. The \texttt{createDefaultCovariateSettings} function is described in the \href{https://ohdsi.github.io/FeatureExtraction/}{FeatureExtraction} package. In short, we are pointing the function to the table containing our cohorts and specify which cohort definition IDs in that table identify the target, comparator and outcome. We instruct that the default set of covariates should be constructed, including covariates for all conditions, drug exposures, and procedures that were found on or before the index date. As mentioned in Section \ref{CohortMethod} we must exclude the target and comparator treatments from the set of covariates, and here we achieve this by listing all ingredients in the two classes, and tell FeatureExtraction to also exclude all descendants, thus excluding all drugs that contain these ingredients.

All data about the cohorts, outcomes, and covariates are extracted from the server and stored in the \texttt{cohortMethodData} object. This object uses the package \texttt{ff} to store information in a way that ensures R does not run out of memory, even when the data are large, as mentioned in Section \ref{BigDataSupport}.

We can use the generic \texttt{summary()} function to view some more information of the data we extracted:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(cmData)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## CohortMethodData object summary
## 
## Treatment concept ID: 1
## Comparator concept ID: 2
## Outcome concept ID(s): 3
## 
## Treated persons: 67166
## Comparator persons: 35333
## 
## Outcome counts:
##          Event count Person count
## 3               980          891
## 
## Covariates:
## Number of covariates: 58349
## Number of non-zero covariate values: 24484665
\end{verbatim}

Creating the \texttt{cohortMethodData} file can take considerable computing time, and it is probably a good idea to save it for future sessions. Because \texttt{cohortMethodData} uses \texttt{ff}, we cannot use R's regular save function. Instead, we'll have to use the \texttt{saveCohortMethodData()} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{saveCohortMethodData}\NormalTok{(cmData, }\StringTok{"AceiVsThzForAngioedema"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can use the \texttt{loadCohortMethodData()} function to load the data in a future session.

\textbf{Defining new users}

Typically, a new user is defined as first time use of a drug (either target or comparator), and typically a washout period (a minimum number of days prior first use) is used to increase the probability that it is truly first use. When using the CohortMethod package, you can enforce the necessary requirements for new use in three ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When defining the cohorts.
\item
  When loading the cohorts using the \texttt{getDbCohortMethodData} function, you can use the \texttt{firstExposureOnly}, \texttt{removeDuplicateSubjects}, \texttt{restrictToCommonPeriod}, and \texttt{washoutPeriod} arguments.
\item
  When defining the study population using the \texttt{createStudyPopulation} function (see below) using the \texttt{firstExposureOnly}, \texttt{removeDuplicateSubjects}, \texttt{restrictToCommonPeriod}, and \texttt{washoutPeriod} arguments.
\end{enumerate}

The advantage of option 1 is that the input cohorts are already fully defined outside of the CohortMethod package, and external cohort characterization tools can be used on the same cohorts used in this analysis. The advantage of options 2 and 3 is that they save you the trouble of limiting to first use yourself, for example allowing you to directly use the DRUG\_ERA table in the CDM. Option 2 is more efficient than 3, since only data for first use will be fetched, while option 3 is less efficient but allows you to compare the original cohorts to the study population.

\hypertarget{defining-the-study-population}{%
\subsection{Defining the study population}\label{defining-the-study-population}}

Typically, the exposure cohorts and outcome cohorts will be defined independently of each other. When we want to produce an effect size estimate, we need to further restrict these cohorts and put them together, for example by removing exposed subjects that had the outcome prior to exposure, and only keeping outcomes that fall within a defined risk window. For this we can use the \texttt{createStudyPopulation} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{studyPop <-}\StringTok{ }\KeywordTok{createStudyPopulation}\NormalTok{(}\DataTypeTok{cohortMethodData =}\NormalTok{ cmData,}
                                  \DataTypeTok{outcomeId =} \DecValTok{3}\NormalTok{,}
                                  \DataTypeTok{firstExposureOnly =} \OtherTok{FALSE}\NormalTok{,}
                                  \DataTypeTok{restrictToCommonPeriod =} \OtherTok{FALSE}\NormalTok{,}
                                  \DataTypeTok{washoutPeriod =} \DecValTok{0}\NormalTok{,}
                                  \DataTypeTok{removeDuplicateSubjects =} \StringTok{"remove all"}\NormalTok{,}
                                  \DataTypeTok{removeSubjectsWithPriorOutcome =} \OtherTok{TRUE}\NormalTok{,}
                                  \DataTypeTok{minDaysAtRisk =} \DecValTok{1}\NormalTok{,}
                                  \DataTypeTok{riskWindowStart =} \DecValTok{1}\NormalTok{,}
                                  \DataTypeTok{addExposureDaysToStart =} \OtherTok{FALSE}\NormalTok{,}
                                  \DataTypeTok{riskWindowEnd =} \DecValTok{0}\NormalTok{,}
                                  \DataTypeTok{addExposureDaysToEnd =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that we've set \texttt{firstExposureOnly} and \texttt{removeDuplicateSubjects} to FALSE, and \texttt{washoutPeriod} to 0 because we already applied those criteria in the cohort definitions. We specify the outcome ID we will use, and that people with outcomes prior to the risk window start date will be removed. The risk window is defined as starting on the day after the cohort start date (\texttt{riskWindowStart\ =\ 1} and \texttt{addExposureDaysToStart\ =\ FALSE}), and the risk windows ends when the cohort exposure ends (\texttt{riskWindowEnd\ =\ 0} and \texttt{addExposureDaysToEnd\ =\ TRUE}), which was defined as the end of exposure in the cohort definition. Note that the risk windows are automatically truncated at the end of observation or the study end date. We also remove subjects who have no time at risk. To see how many people are left in the study population we can always use the \texttt{getAttritionTable} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{getAttritionTable}\NormalTok{(studyPop)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                    description targetPersons comparatorPersons           ...
## 1             Original cohorts         67212             35379           ...
## 2 Removed subs in both cohorts         67166             35333           ...
## 3             No prior outcome         67061             35238           ...
## 4 Have at least 1 days at risk         66780             35086           ...
\end{verbatim}

\hypertarget{propensity-scores-1}{%
\subsection{Propensity scores}\label{propensity-scores-1}}

We can fit a propensity model using the covariates constructed by the \texttt{getDbcohortMethodData()} function, and compute a PS for each person:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ps <-}\StringTok{ }\KeywordTok{createPs}\NormalTok{(}\DataTypeTok{cohortMethodData =}\NormalTok{ cmData, }\DataTypeTok{population =}\NormalTok{ studyPop)}
\end{Highlighting}
\end{Shaded}

The \texttt{createPs} function uses the \href{https://ohdsi.github.io/Cyclops/}{Cyclops} package to fit a large-scale regularized logistic regression. To fit the propensity model, Cyclops needs to know the hyperparameter value which specifies the variance of the prior. By default Cyclops will use cross-validation to estimate the optimal hyperparameter. However, be aware that this can take a really long time. You can use the \texttt{prior} and \texttt{control} parameters of the \texttt{createPs} function to specify Cyclops' behavior, including using multiple CPUs to speed-up the cross-validation.

Here we use the PS to perform variable-ratio matching:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matchedPop <-}\StringTok{ }\KeywordTok{matchOnPs}\NormalTok{(}\DataTypeTok{population =}\NormalTok{ ps, }\DataTypeTok{caliper =} \FloatTok{0.2}\NormalTok{, }
                        \DataTypeTok{caliperScale =} \StringTok{"standardized logit"}\NormalTok{, }\DataTypeTok{maxRatio =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Alternatively, we could have used the PS in the \texttt{trimByPs}, \texttt{trimByPsToEquipoise}, or \texttt{stratifyByPs} functions.

\hypertarget{outcome-models}{%
\subsection{Outcome models}\label{outcome-models}}

The outcome model is a model describing which variables are associated with the outcome. Under strict assumptions, the coefficient for the treatment variable can be interpreted as the causal effect. In this case we fit a Cox proportional hazards model, conditioned (stratified) on the matched sets:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{outcomeModel <-}\StringTok{ }\KeywordTok{fitOutcomeModel}\NormalTok{(}\DataTypeTok{population =}\NormalTok{ matchedPop,}
                                \DataTypeTok{modelType =} \StringTok{"cox"}\NormalTok{,}
                                \DataTypeTok{stratified =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{outcomeModel}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Model type: cox
## Stratified: TRUE
## Use covariates: FALSE
## Use inverse probability of treatment weighting: FALSE
## Status: OK
## 
##           Estimate lower .95 upper .95   logRr seLogRr
## treatment   4.3203    2.4531    8.0771 1.4633   0.304
\end{verbatim}

\hypertarget{MultipleAnalyses}{%
\subsection{Running multiple analyses}\label{MultipleAnalyses}}

Often we want to perform more than one analyses, for example for multiple outcomes including negative controls. The \href{https://ohdsi.github.io/CohortMethod/}{CohortMethod} offers functions for performing such studies efficiently. This is described in detail in the \href{https://ohdsi.github.io/CohortMethod/articles/MultipleAnalyses.html}{package vignette on running multiple analyses}. Briefly, assuming the outcome of interest and negative control cohorts have already been created, we can specify all target-comparator-outcome combinations we wish to analyse:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Outcomes of interest:}
\NormalTok{ois <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{) }\CommentTok{# Angioedema, AMI}

\CommentTok{# Negative controls:}
\NormalTok{ncs <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{434165}\NormalTok{,}\DecValTok{436409}\NormalTok{,}\DecValTok{199192}\NormalTok{,}\DecValTok{4088290}\NormalTok{,}\DecValTok{4092879}\NormalTok{,}\DecValTok{44783954}\NormalTok{,}\DecValTok{75911}\NormalTok{,}\DecValTok{137951}\NormalTok{,}\DecValTok{77965}\NormalTok{,}
         \DecValTok{376707}\NormalTok{,}\DecValTok{4103640}\NormalTok{,}\DecValTok{73241}\NormalTok{,}\DecValTok{133655}\NormalTok{,}\DecValTok{73560}\NormalTok{,}\DecValTok{434327}\NormalTok{,}\DecValTok{4213540}\NormalTok{,}\DecValTok{140842}\NormalTok{,}\DecValTok{81378}\NormalTok{,}\DecValTok{432303}\NormalTok{,}
         \DecValTok{4201390}\NormalTok{,}\DecValTok{46269889}\NormalTok{,}\DecValTok{134438}\NormalTok{,}\DecValTok{78619}\NormalTok{,}\DecValTok{201606}\NormalTok{,}\DecValTok{76786}\NormalTok{,}\DecValTok{4115402}\NormalTok{,}\DecValTok{45757370}\NormalTok{,}\DecValTok{433111}
         \DecValTok{433527}\NormalTok{,}\DecValTok{4170770}\NormalTok{,}\DecValTok{4092896}\NormalTok{,}\DecValTok{259995}\NormalTok{,}\DecValTok{40481632}\NormalTok{,}\DecValTok{4166231}\NormalTok{,}\DecValTok{433577}\NormalTok{,}\DecValTok{4231770}\NormalTok{,}\DecValTok{440329}\NormalTok{,}
         \DecValTok{4012570}\NormalTok{,}\DecValTok{4012934}\NormalTok{,}\DecValTok{441788}\NormalTok{,}\DecValTok{4201717}\NormalTok{,}\DecValTok{374375}\NormalTok{,}\DecValTok{4344500}\NormalTok{,}\DecValTok{139099}\NormalTok{,}\DecValTok{444132}\NormalTok{,}\DecValTok{196168}\NormalTok{,}
         \DecValTok{432593}\NormalTok{,}\DecValTok{434203}\NormalTok{,}\DecValTok{438329}\NormalTok{,}\DecValTok{195873}\NormalTok{,}\DecValTok{4083487}\NormalTok{,}\DecValTok{4103703}\NormalTok{,}\DecValTok{4209423}\NormalTok{,}\DecValTok{377572}\NormalTok{,}\DecValTok{40480893}\NormalTok{,}
         \DecValTok{136368}\NormalTok{,}\DecValTok{140648}\NormalTok{,}\DecValTok{438130}\NormalTok{,}\DecValTok{4091513}\NormalTok{,}\DecValTok{4202045}\NormalTok{,}\DecValTok{373478}\NormalTok{,}\DecValTok{46286594}\NormalTok{,}\DecValTok{439790}\NormalTok{,}\DecValTok{81634}\NormalTok{,}
         \DecValTok{380706}\NormalTok{,}\DecValTok{141932}\NormalTok{,}\DecValTok{36713918}\NormalTok{,}\DecValTok{443172}\NormalTok{,}\DecValTok{81151}\NormalTok{,}\DecValTok{72748}\NormalTok{,}\DecValTok{378427}\NormalTok{,}\DecValTok{437264}\NormalTok{,}\DecValTok{194083}\NormalTok{,}
         \DecValTok{140641}\NormalTok{,}\DecValTok{440193}\NormalTok{,}\DecValTok{4115367}\NormalTok{)}

\NormalTok{tcos <-}\StringTok{ }\KeywordTok{createTargetComparatorOutcomes}\NormalTok{(}\DataTypeTok{targetId =} \DecValTok{1}\NormalTok{,}
                                       \DataTypeTok{comparatorId =} \DecValTok{2}\NormalTok{,}
                                       \DataTypeTok{outcomeIds =} \KeywordTok{c}\NormalTok{(ois, ncs))}

\NormalTok{tcosList <-}\StringTok{ }\KeywordTok{list}\NormalTok{(tcos)}
\end{Highlighting}
\end{Shaded}

Next, we specify what arguments should be used when calling the various functions described previously in our example with one outcome:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aceI <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1335471}\NormalTok{,}\DecValTok{1340128}\NormalTok{,}\DecValTok{1341927}\NormalTok{,}\DecValTok{1363749}\NormalTok{,}\DecValTok{1308216}\NormalTok{,}\DecValTok{1310756}\NormalTok{,}\DecValTok{1373225}\NormalTok{,}
          \DecValTok{1331235}\NormalTok{,}\DecValTok{1334456}\NormalTok{,}\DecValTok{1342439}\NormalTok{)}
\NormalTok{thz <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1395058}\NormalTok{,}\DecValTok{974166}\NormalTok{,}\DecValTok{978555}\NormalTok{,}\DecValTok{907013}\NormalTok{)}

\NormalTok{cs <-}\StringTok{ }\KeywordTok{createDefaultCovariateSettings}\NormalTok{(}\DataTypeTok{excludedCovariateConceptIds =} \KeywordTok{c}\NormalTok{(aceI, }
\NormalTok{                                                                     thz),}
                                     \DataTypeTok{addDescendantsToExclude =} \OtherTok{TRUE}\NormalTok{)}

\NormalTok{cmdArgs <-}\StringTok{ }\KeywordTok{createGetDbCohortMethodDataArgs}\NormalTok{(}
  \DataTypeTok{studyStartDate =} \StringTok{""}\NormalTok{,}
  \DataTypeTok{studyEndDate =} \StringTok{""}\NormalTok{,}
  \DataTypeTok{firstExposureOnly =} \OtherTok{FALSE}\NormalTok{,}
  \DataTypeTok{removeDuplicateSubjects =} \OtherTok{FALSE}\NormalTok{,}
  \DataTypeTok{restrictToCommonPeriod =} \OtherTok{FALSE}\NormalTok{,}
  \DataTypeTok{washoutPeriod =} \DecValTok{0}\NormalTok{,}
  \DataTypeTok{covariateSettings =}\NormalTok{ cs)}

\NormalTok{spArgs <-}\StringTok{ }\KeywordTok{createCreateStudyPopulationArgs}\NormalTok{(}
  \DataTypeTok{firstExposureOnly =} \OtherTok{FALSE}\NormalTok{,}
  \DataTypeTok{restrictToCommonPeriod =} \OtherTok{FALSE}\NormalTok{,}
  \DataTypeTok{washoutPeriod =} \DecValTok{0}\NormalTok{,}
  \DataTypeTok{removeDuplicateSubjects =} \StringTok{"remove all"}\NormalTok{,}
  \DataTypeTok{removeSubjectsWithPriorOutcome =} \OtherTok{TRUE}\NormalTok{,}
  \DataTypeTok{minDaysAtRisk =} \DecValTok{1}\NormalTok{,}
  \DataTypeTok{riskWindowStart =} \DecValTok{1}\NormalTok{,}
  \DataTypeTok{addExposureDaysToStart =} \OtherTok{FALSE}\NormalTok{,}
  \DataTypeTok{riskWindowEnd =} \DecValTok{0}\NormalTok{,}
  \DataTypeTok{addExposureDaysToEnd =} \OtherTok{TRUE}\NormalTok{)}

\NormalTok{psArgs <-}\StringTok{ }\KeywordTok{createCreatePsArgs}\NormalTok{()}

\NormalTok{matchArgs <-}\StringTok{ }\KeywordTok{createMatchOnPsArgs}\NormalTok{(}
  \DataTypeTok{caliper =} \FloatTok{0.2}\NormalTok{, }
  \DataTypeTok{caliperScale =} \StringTok{"standardized logit"}\NormalTok{, }
  \DataTypeTok{maxRatio =} \DecValTok{100}\NormalTok{)}

\NormalTok{fomArgs <-}\StringTok{ }\KeywordTok{createFitOutcomeModelArgs}\NormalTok{(}
  \DataTypeTok{modelType =} \StringTok{"cox"}\NormalTok{,}
  \DataTypeTok{stratified =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We then combine these into a single analysis settings object, which we provide a unique analysis ID and some description. We can combine one or more analysis settings objects into a list:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cmAnalysis <-}\StringTok{ }\KeywordTok{createCmAnalysis}\NormalTok{(}
  \DataTypeTok{analysisId =} \DecValTok{1}\NormalTok{,}
  \DataTypeTok{description =} \StringTok{"Propensity score matching"}\NormalTok{,}
  \DataTypeTok{getDbCohortMethodDataArgs =}\NormalTok{ cmdArgs,}
  \DataTypeTok{createStudyPopArgs =}\NormalTok{ spArgs,}
  \DataTypeTok{createPs =} \OtherTok{TRUE}\NormalTok{,}
  \DataTypeTok{createPsArgs =}\NormalTok{ psArgs,}
  \DataTypeTok{matchOnPs =} \OtherTok{TRUE}\NormalTok{,}
  \DataTypeTok{matchOnPsArgs =}\NormalTok{ matchArgs}
  \DataTypeTok{fitOutcomeModel =} \OtherTok{TRUE}\NormalTok{,}
  \DataTypeTok{fitOutcomeModelArgs =}\NormalTok{ fomArgs)}

\NormalTok{cmAnalysisList <-}\StringTok{ }\KeywordTok{list}\NormalTok{(cmAnalysis)}
\end{Highlighting}
\end{Shaded}

We can now run the study including all comparisons and analysis settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result <-}\StringTok{ }\KeywordTok{runCmAnalyses}\NormalTok{(}\DataTypeTok{connectionDetails =}\NormalTok{ connectionDetails,}
                        \DataTypeTok{cdmDatabaseSchema =}\NormalTok{ cdmDatabaseSchema,}
                        \DataTypeTok{exposureDatabaseSchema =}\NormalTok{ cohortDbSchema,}
                        \DataTypeTok{exposureTable =}\NormalTok{ cohortTable,}
                        \DataTypeTok{outcomeDatabaseSchema =}\NormalTok{ cohortDbSchema,}
                        \DataTypeTok{outcomeTable =}\NormalTok{ cohortTable,}
                        \DataTypeTok{cdmVersion =}\NormalTok{ cdmVersion,}
                        \DataTypeTok{outputFolder =}\NormalTok{ outputFolder,}
                        \DataTypeTok{cmAnalysisList =}\NormalTok{ cmAnalysisList,}
                        \DataTypeTok{targetComparatorOutcomesList =}\NormalTok{ tcosList)}
\end{Highlighting}
\end{Shaded}

The \texttt{result} object contains references to all the artifacts that were created. For example, we can retrieve the outcome model for AMI:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{omFile <-}\StringTok{ }\NormalTok{result}\OperatorTok{$}\NormalTok{outcomeModelFile[result}\OperatorTok{$}\NormalTok{targetId }\OperatorTok{==}\StringTok{ }\DecValTok{1} \OperatorTok{&}\StringTok{ }
\StringTok{                                    }\NormalTok{result}\OperatorTok{$}\NormalTok{comparatorId }\OperatorTok{==}\StringTok{ }\DecValTok{2} \OperatorTok{&}\StringTok{ }
\StringTok{                                    }\NormalTok{result}\OperatorTok{$}\NormalTok{outcomeId }\OperatorTok{==}\StringTok{ }\DecValTok{4} \OperatorTok{&}
\StringTok{                                    }\NormalTok{result}\OperatorTok{$}\NormalTok{analysisId }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{]}
\NormalTok{outcomeModel <-}\StringTok{ }\KeywordTok{readRDS}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(outputFolder, omFile))}
\NormalTok{outcomeModel}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Model type: cox
## Stratified: TRUE
## Use covariates: FALSE
## Use inverse probability of treatment weighting: FALSE
## Status: OK
## 
##           Estimate lower .95 upper .95   logRr seLogRr
## treatment   1.1338    0.5921    2.1765 0.1256   0.332
\end{verbatim}

We can also retrieve the effect size estimates for all outcomes with one command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summ <-}\StringTok{ }\KeywordTok{summarizeAnalyses}\NormalTok{(result, }\DataTypeTok{outputFolder =}\NormalTok{ outputFolder)}
\KeywordTok{head}\NormalTok{(summ)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     analysisId targetId comparatorId outcomeId        rr    ci95lb  ...
## 1            1        1            2     72748 0.9734698 0.5691589  ...
## 2            1        1            2     73241 0.7067981 0.4009951  ...
## 3            1        1            2     73560 1.0623951 0.7187302  ...
## 4            1        1            2     75911 0.9952184 0.6190344  ...
## 5            1        1            2     76786 1.0861746 0.6730408  ...
## 6            1        1            2     77965 1.1439772 0.5173222  ...
\end{verbatim}

\hypertarget{studyOutputs}{%
\section{Study outputs}\label{studyOutputs}}

Our estimates are only valid if several assumptions have been met. We use a wide set of diagnostics to evaluate whether this is the case. These are available in the results produced by the R package generated by ATLAS, or can be generated on the fly using specific R functions.

\hypertarget{propensity-scores-and-model}{%
\subsection{Propensity scores and model}\label{propensity-scores-and-model}}

We first need to evaluate whether the target and comparator cohort are to some extend comparable. For this we can compute the Area Under the Receiver Operator Curve (AUC) statistic for the propensity model. An AUC of 1 indicates the treatment assignment was completely predictable based on baseline covariates, and that the two groups are therefore incomparable. We can use the \texttt{computePsAuc} function to compute the AUC, which in our example is 0.79. Using the \texttt{plotPs} function, we can also generate the preference score distribution as shown in Figure \ref{fig:ps}. Here we see that for many people the treatment they received was predictable, but there is also a large amount of overlap, indicating that adjustment can be used to select comparable groups.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PopulationLevelEstimation/ps} 

}

\caption{Preference score distribution.}\label{fig:ps}
\end{figure}

In general it is a good idea to also inspect the propensity model itself, and especially so if the model is very predictive. That way we may discover which variables are most predictive. Table \ref{tab:psModel} shows the top predictors in our propensity model. Note that if a variable is too predictive, the CohortMethod package will throw an informative error rather than attempt to fit a model that is already known to be perfectly predictive.

\begin{longtable}[]{@{}rl@{}}
\caption{\label{tab:psModel} Top 10 predictors in the propensity model for ACEi and THZ. Positive values mean subjects with the covariate are more likely to receive the target treatment.}\tabularnewline
\toprule
\begin{minipage}[b]{0.06\columnwidth}\raggedleft
Beta\strut
\end{minipage} & \begin{minipage}[b]{0.88\columnwidth}\raggedright
Covariate\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.06\columnwidth}\raggedleft
Beta\strut
\end{minipage} & \begin{minipage}[b]{0.88\columnwidth}\raggedright
Covariate\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.06\columnwidth}\raggedleft
-1.42\strut
\end{minipage} & \begin{minipage}[t]{0.88\columnwidth}\raggedright
condition\_era group during day -30 through 0 days relative to index: Edema\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.06\columnwidth}\raggedleft
-1.11\strut
\end{minipage} & \begin{minipage}[t]{0.88\columnwidth}\raggedright
drug\_era group during day 0 through 0 days relative to index: Potassium Chloride\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.06\columnwidth}\raggedleft
0.68\strut
\end{minipage} & \begin{minipage}[t]{0.88\columnwidth}\raggedright
age group: 05-09\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.06\columnwidth}\raggedleft
0.64\strut
\end{minipage} & \begin{minipage}[t]{0.88\columnwidth}\raggedright
measurement during day -365 through 0 days relative to index: Renin\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.06\columnwidth}\raggedleft
0.63\strut
\end{minipage} & \begin{minipage}[t]{0.88\columnwidth}\raggedright
condition\_era group during day -30 through 0 days relative to index: Urticaria\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.06\columnwidth}\raggedleft
0.57\strut
\end{minipage} & \begin{minipage}[t]{0.88\columnwidth}\raggedright
condition\_era group during day -30 through 0 days relative to index: Proteinuria\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.06\columnwidth}\raggedleft
0.55\strut
\end{minipage} & \begin{minipage}[t]{0.88\columnwidth}\raggedright
drug\_era group during day -365 through 0 days relative to index: INSULINS AND ANALOGUES\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.06\columnwidth}\raggedleft
-0.54\strut
\end{minipage} & \begin{minipage}[t]{0.88\columnwidth}\raggedright
race = Black or African American\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.06\columnwidth}\raggedleft
0.52\strut
\end{minipage} & \begin{minipage}[t]{0.88\columnwidth}\raggedright
(Intercept)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.06\columnwidth}\raggedleft
0.50\strut
\end{minipage} & \begin{minipage}[t]{0.88\columnwidth}\raggedright
gender = MALE\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\BeginKnitrBlock{rmdimportant}
If a variable is found to be highly predictive, there are two possible conclusions: Either we find that the variable is clearly part of the exposure itself and should be removed before fitting the model, or else we must conclude that the two populations are truly incomparable, and the analysis must be stopped.
\EndKnitrBlock{rmdimportant}

\hypertarget{covariate-balance}{%
\subsection{Covariate balance}\label{covariate-balance}}

The goal of using PS is to make the two groups comparable (or at least to select comparable groups). We must verify whether this is achieved, for example by checked whether the baseline covariates are indeed balanced after adjustment. We can use the \texttt{computeCovariateBalance} and \texttt{plotCovariateBalanceScatterPlot} functions to generate Figure \ref{fig:balance}. One rule-of-thumb to use is that no covariate may have an absolute standardized difference of means greater than 0.1 after propensity score adjustment. Here we see that although there was substantial imbalance before matching, after matching we meet this criterion.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/PopulationLevelEstimation/balance} 

}

\caption{Covariate balance, showing the absolute standardized difference of mean before and after propensity score matching. Each blue dot represents a covariate.}\label{fig:balance}
\end{figure}

\hypertarget{follow-up-and-power}{%
\subsection{Follow up and power}\label{follow-up-and-power}}

Before fitting an outcome model, we might be interested to know whether we have sufficient power to detect a particular effect size. It makes sense to perform these power calculations once the study population has been fully defined, so taking into account loss to the various inclusion and exclusion criteria (such as no prior outcomes), and loss due to matching and/or trimming. We can view the attrition of subjects in our study using the \texttt{drawAttritionDiagram} function as shown in Figure \ref{fig:attrition}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/PopulationLevelEstimation/attrition} 

}

\caption{Attrition diagram. The counts shown at the top are those that meet our target and comparator cohort definitions. The counts at the bottom are those that enter our outcome model, in  this case a Cox regression.}\label{fig:attrition}
\end{figure}

Since the sample size is fixed in retrospective studies (the data has already been collected), and the true effect size is unknown, it is therefore less meaningful to compute the power given an expected effect size. Instead, the CohortMethod package provides the \texttt{computeMdrr} function to compute the minimum detectable relative risk (MDRR). In our example study the MDRR is 1.69.

To gain a better understanding of the amount of follow-up available we can also inspect the distribution of follow-up time. We defined follow-up time as time at risk, so not censored by the occurrence of the outcome. The \texttt{getFollowUpDistribution} can provide a simple overview as shown in Figure \ref{fig:followUp}, which suggests the follow-up time for both cohorts is comparable.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PopulationLevelEstimation/followUp} 

}

\caption{Distribution of follow-up time for the target and comparator cohorts.}\label{fig:followUp}
\end{figure}

\hypertarget{kaplan-meier}{%
\subsection{Kaplan Meier}\label{kaplan-meier}}

One last check is to review the Kaplan Meier plot, showing the survival over time in both cohorts. Using the \texttt{plotKaplanMeier} function we can create \ref{fig:kmPlot}, which we can check for example if our assumption of proportionality of hazards holds. The Kaplan-Meier plot automatically adjusts for stratification or weighting by PS. In this case, because variable-ratio matching is used, the survival curve for the comparator groups is adjusted to mimick what the curve had looked like for the target group had they been exposued to the comparator instead.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/PopulationLevelEstimation/kmPlot} 

}

\caption{Kaplan Meier plot.}\label{fig:kmPlot}
\end{figure}

\hypertarget{effect-size-estimate}{%
\subsection{Effect size estimate}\label{effect-size-estimate}}

We observe a hazard ratio of 4.32 (95\% confidence interval: 2.45 - 8.08) for angioedema, which tells us that ACEi appear to increase the risk of angioedema compared to THZ. Similarly, we observe a hazard ratio of 1.13 (95\% confidence interval: 0.59 - 2.18) for AMI, suggesting little or no effect for AMI. Our diagnostics, as reviewed earlier, give no reason for doubt. However, ultimately the quality of this evidence, and whether we choose to trust it, depends on many factors that are not covererd by the study diagnostics as described in Chapter \ref{EvidenceQuality}.

\hypertarget{summary-5}{%
\section{Summary}\label{summary-5}}

\BeginKnitrBlock{rmdsummary}
\begin{itemize}
\item
  Population-level estimation aims to infer causal effects from observational data.
\item
  The \textbf{counterfactual}, what would have happened if the subject had received an alternative exposure or no exposure, cannot be observed.
\item
  Different designs aim to construct the counterfactual in different ways.
\item
  The various designs as implemented in the OHDSI Methods Library provide diagnostics to evaluate whether the assumptions for creating an appropriate counterfactual have been met.
\end{itemize}
\EndKnitrBlock{rmdsummary}

\hypertarget{excercises}{%
\section{Excercises}\label{excercises}}

Note: The excercises still have to be defined. The idea is to require readers to define a study that estimates the effect of celecoxib on GI bleed, compared to diclofenac. For this they must use the Eunomia package, which is still under development.

\hypertarget{PatientLevelPrediction}{%
\chapter{Patient Level Prediction}\label{PatientLevelPrediction}}

\emph{Chapter leads: Peter Rijnbeek \& Jenna Reps}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

It is widely excepted in medicine that prevention is better than cure. If we could identify patients who are at a high risk of developing some illness, then it might be possible to intervene and slow down the progression of, or even prevent, the illness. Big observational healthcare data capture rich information across a diverse and often large patient population. These data are collected retrospectively. It may be possible to learn associations from the data that can be used to calculate patient-level predictions for various illnesses or health outcomes. In this chapter we consider predicting the general tasks:

\begin{itemize}
\tightlist
\item
  Disease onset and progression

  \begin{itemize}
  \tightlist
  \item
    \textbf{Structure}: Amongst patients who are newly diagnosed with \emph{{[}a disease{]}}, who will go on to have \emph{{[}another disease or complication{]}} within \emph{{[}time horizon from diagnosis{]}}?
  \item
    \textbf{Example}: Among newly diagnosed atrial fibrilation patients, who will go on to have ischemic stroke in the next three years?
  \end{itemize}
\item
  Treatment choice

  \begin{itemize}
  \tightlist
  \item
    \textbf{Structure}: Amongst patients with \emph{{[}indicated disease{]}} who are treated with either \emph{{[}treatment 1{]}} or \emph{{[}treatment 2{]}}, which patients were treated with \emph{{[}treatment 1{]}} (on day 0).
  \item
    \textbf{Example}: Among patients with atrial fibrilation who took either warfarin or rivaroxaban, which patients gets warfarin? (e.g.~for a propensity model)
  \end{itemize}
\item
  Treatment response

  \begin{itemize}
  \tightlist
  \item
    \textbf{Structure}: Amongst new users of \emph{{[}a treatment{]}}, who will experience \emph{{[}some effect{]}} in \emph{{[}time window{]}} ?
  \item
    \textbf{Example}: Which patients with diabetes who start on metformin stay on metform for three years?
  \end{itemize}
\item
  Treatment safety

  \begin{itemize}
  \tightlist
  \item
    \textbf{Structure}: Amongst new users of \emph{{[}a treatment{]}}, who will experience \emph{{[}adverse event{]}} in \emph{{[}time window{]}}?
  \item
    \textbf{Example}: Amongst new users of warfarin, who will have a GI bleed in one year?
  \end{itemize}
\item
  Treatment adherence

  \begin{itemize}
  \tightlist
  \item
    \textbf{Structure}: Amongst new users of \emph{{[}a treatment{]}}, who will achieve \emph{{[}adherence metric{]}} at \emph{{[}time window{]}}?
  \item
    \textbf{Example}: Which patients with diabetes who start on metformin achieve \textgreater=80\% proportion of days covered at one year?
  \end{itemize}
\end{itemize}

All these examples answer the question, within some target population predict the occurrence of some outcome during some time interval relative to the target population index. Prediction models can be used to find groups of patients that are at a high or low risk of some health outcome or to calculate a patient's personalized risk. However, prediction does not tell us anything about the causality relationship between the predictors and the outcome. If you are interested in causal inference, then you want to read Chapter {[}add link to Population level estimation{]}.

In this chapter we first describe the theory behind patient-level prediction, including a \protect\hyperlink{progress}{summary of current progress in patient-level prediction}, details about \protect\hyperlink{extracted}{how labeled data are extracted} from observational databases, an \protect\hyperlink{supervised}{overview of supervised learning} and the various classifiers and how to \protect\hyperlink{evaluation}{evaluate the performance of a model}. There are two ways to use the OHDSI tools for patient-level prediction. The first approach is to use the atlas interface to design a study and then atlas creates an R package that goes end to end from data extraction to model develoment and evaluation. This requires only basic R knowledge. The second approach is to manually write R code using our PatientLevelPrediction R library. This requires intermediate R knowledge at a minimum, but enables greater flexibility in the prediction analysis study design. In sections - we provide a complete walk-through demonstrating how to correctly specify a patientl-level prediction problem, how to use Atlas and/or R to develop the prediction model and how to evaluate the model.

\newpage

\hypertarget{progress}{%
\section{Current Progress in Patient-Level Prediction}\label{progress}}

Clinical decision making is a complicated task in which the clinician has to infer a diagnosis or treatment pathway based on the available medical history of the patient and the current clinical guidelines. Clinical prediction models have been developed to support this decision making process and are used in clinical practice in a wide spectrum of specialties. These models predict a diagnostic or prognostic outcome based on a combination of patient characteristics, e.g.~demographic information, disease history, treatment history. The number of publications describing clinical prediction models has increased strongly over the last 10 years. An example is the Garvan model that predicts the 5-years and 10-years fractures risk in any elderly man or woman based on age, fracture history, fall history, bone mass density or weight \citep{nguyen2008}. Many prediction models have been developed in patient subgroups at higher risk that need more intensive monitoring, e.g.~the prediction of 30-day mortality after an acute myocardial described by \citet{lee1995}. Also, many models have been developed for asymptomatic subjects in the population, e.g.~the famous Framingham risk functions for cardiovascular disease \citep{wilson1998}, or the models for breast cancer screening \citep{engel2015}.

Surprisingly, most currently used models are estimated using small datasets and contain a limited set of patient characteristics. For example, in a review of 102 prognostic models in traumatic brain injury showed that three quarters of the models were based on samples with less than 500 patients \citep{perel2006}. This low sample size, and thus low statistical power, forces the data analyst to make stronger modelling assumptions. The selection of the often limited set of patient characteristics is strongly guided by the expert knowledge at hand. This contrasts sharply with the reality of modern medicine wherein patients generate a rich digital trail, which is well beyond the power of any medical practitioner to fully assimilate. Presently, health care is generating huge amount of patient-specific information contained in the Electronic Health Record (EHR). This includes structured data in the form of diagnose, medication, laboratory test results, and unstructured data contained in clinical narratives. Currently, it is unknown how much predictive accuracy can be gained by leveraging the large amount of data originating from the complete EHR of a patient.

Massive-scale, patient-specific predictive modeling has become reality due the OHDSI initiative in which the common data model (CDM) allows for uniform and transparent analysis at an unprecedented scale. These large standardized populations contain rich data to build highly predictive large-scale models and also provide immediate opportunity to serve large communities of patients who are in most need of improved quality of care. Such models can inform truly personalized medical care leading hopefully to sharply improved patient outcomes. Furthermore, these models could assist in the design and analysis of randomized controlled trials (RCT) by enabling a better patient stratification or can be utilized to adjust for confounding variables in observational research. More accurate prediction models contribute to targeting of treatment and to increasing cost-effectiveness of medical care.

Advances in machine learning for large dataset analysis have led to increased interest in applying patient-level prediction on this type of data. However, many published efforts in patient-level-prediction do not follow the model development guidelines, fail to perform extensive external validation, or provide insufficient model details that limits the ability of independent researchers to reproduce the models and perform external validation. This makes it hard to fairly evaluate the predictive performance of the models and reduces the likelihood of the model being used appropriately in clinical practice. To improve standards, several papers have been written detailing guidelines for best practices in developing and reporting prediction models.

The Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) statement \footnote{\url{https://www.equator-network.org/reporting-guidelines/tripod-statement/}} provides clear recommendations for reporting prediction model development and validation and addresses some of the concerns related to transparency. However, data structure heterogeneity and inconsistent terminologies still make collaboration and model sharing difficult as different researchers are often required to write new code to extract the data from their databases and may define variables differently.

In our paper \citep{reps2018}, we propose a standardised framework for patient-level prediction that utilizes the OMOP Common Data Model (CDM) and standardized vocabularies, and describe the open-source software that we developed implementing the framework's pipeline. The framework is the first to support existing best practice guidelines and will enable open dissemination of models that can be extensively validated across the network of OHDSI collaborators.

Figure \ref{fig:figure1}, illustrates the prediction problem we address. Among a population at risk, we aim to predict which patients at a defined moment in time (t = 0) will experience some outcome during a time-at-risk. Prediction is done using only information about the patients in an observation window prior to that moment in time.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/Figure1} \caption{The prediction problem.}\label{fig:figure1}
\end{figure}

As shown in Table \ref{tab:plpDesign}, to define a prediction problem we have to define t=0 by a target Cohort (T), the outcome we like to predict by an outcome cohort (O), and the time-at-risk (TAR). We define the standard prediction question as:

\BeginKnitrBlock{rmdimportant}
Amongst {[}add Target cohort definition{]}, who will go on to have {[}add outcome definition{]} within {[}add time at risk period{]}
\EndKnitrBlock{rmdimportant}

Furthermore, we have to make design choices for the model we like to develop, and determine the observational datasets to perform internal and external validation.

\begin{longtable}[]{@{}ll@{}}
\caption{\label{tab:plpDesign} Main design choices in a prediction design.}\tabularnewline
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Target cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
A cohort for whom we wish to predict\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Outcome cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
A cohort representing the outcome we wish to predict\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Time-at-risk\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
For what time relative to t=0 do we want to make the prediction?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Model\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
What algorithms using which parameters do we want use, and what predictor variables do we want to include?\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

This conceptual framework works for all type of prediction problems, see {[}introduction link{]}

\hypertarget{extracted}{%
\section{Creating Labelled Data}\label{extracted}}

The observational datasets we use in OHDSI consist of timestamped records of patient medical interactions. These are represented by tables containing anonymised patient details such as gender and year of birth in addition to tables containing date stamped medical records.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/theory/patienttimeline} \caption{Observational data.}\label{fig:figuretheory1}
\end{figure}

Applying supervised learning techniques for prediction requires having covariate and label pairs for a sufficient number of patients. The covariates (also referred to as features or independant variables) describe a patient. Example covariates could be: the patient's gender, age and health state based on the presence or absense of medical conditions. Many of these covariates are time dependant, for example age changes over time, as do some medical conditions. The labels correspond to whether a patient has a outcome of interest during some time interval. The label is also time dependant.

To convert the observational data into labelled data consisting of covariate and label pairs for a set of patients, we need to specify a point in time for each patient that will be used as a pivot. Covariates can be constructed at that pivot point in time (using all records up to that point), and we can can determine whether a patient has the outcome of interest during some time interval relative to the pivot point in time (the time at risk).

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/theory/dataplot1} \caption{Create laelled data from observational data.}\label{fig:figuretheory2}
\end{figure}

This will then provide us with labelled data. The definition of the target cohort population is what we use to define this pivot point in time. For example, if the target cohort was new users of drug A, then the pivot point in time is the date a patient first had drug A recorded in the database. Alternatively, if the target cohort was diagnoses of cardiovascular disease, then the pivot point in time is the date a patient first has a record indicating cardiovascular disease is present. Our prediction specification directly links to how the labelled data are constructed.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/theory/dataplot2} \caption{Create laelled data from observational data.}\label{fig:figuretheory3}
\end{figure}

\hypertarget{supervised}{%
\section{Supervised learning}\label{supervised}}

The idea of supervised learning is to be able to generalise what is observed in the labelled data so that when a new patient's covariates are known but their label is unknown, we can predict their label.

If we consider the situation where we have two covariates, then we can represent each patient as a plot in two dimensional space. The shape/color of a data points corresponds to the patient's label. The idea of supervised learning is to generalise the what we see and fill in where there are no current data points. A supervised learning model will try to partition the space via a decision boundary, as seen in Figure \ref{fig:figuretheory4} that aims to minimise the cases where the data point labels do not match the models prediction. Different supervised learning techniques lead to different decision boundaries and there are often hyper-parameters that can impact the complexity of the decision boundary.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/theory/learning} \caption{Decision boundary.}\label{fig:figuretheory4}
\end{figure}

In Figure Figure \ref{fig:figuretheory4} you can see three different decision boundaries. The boundaries are used to infer the class of any new data point. In figure Figure \ref{fig:figuretheory5} the decision boundaries are used to shade the 2 dimentional space into red regions and green regions. If a new data point falls into the green shaded area then the model will predict `no outcome', otherwise it will predict `has outcome'.

Ideally a decision boundary should partition the two classes with no error. However, generlizability is an issue, as complex models can `overfit' where they can correctly partition each data points in the labelled data by using very complex boundaries:

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/theory/noise} \caption{Overfitting issues.}\label{fig:figuretheory5}
\end{figure}

The issue here is that these boundaries may be fit too closely to the labelled data used to learn them and may not work for new data. For example, noise causing incorrectly positioned data points can cause issues. This is shown in Figure Figure \ref{fig:figuretheory5} where the decision boundary goes around a data point that was incorrectly positioned due to noise and this impacts predictions near this region.

Therefore, you want a model that appears to partition the labelled data well but is also as simple as possible. Techniques such as regularization aim to maximise model performance on the labelled data while minimising complexity. Complexity can also be controlled by picking classifier hyper-parameters such that a simpler decision boundary is used.

Another way to think about supervised learning is finding a function that maps from a patient's covariates to their label. {[}add function{]}. Each supervised learning model has a different way to learn the mapping function and the no free lunch theorem states that no one algorithm is always going to outperform the others. The performance of each type of supervised learning algorithm depends on how the labelled data points are distributed in space. Therefore we recommend trying multiple supervised learning techniques when developing patient-level prediction models.

\hypertarget{regularized-logistic-regression}{%
\subsection{Regularized Logistic Regression}\label{regularized-logistic-regression}}

Lasso logistic regression belongs to the family of generalized linear models, where a linear combination of the variables is learned and finally a logistic function maps the linear combination to a value between 0 and 1. The lasso regularization adds a cost based on model complexity to the objective function when training the model. This cost is the sum of the absolute values of the linear combination of the coefficients. The model automatically performs feature selection by minimizing this cost. We use the \href{https://ohdsi.github.io/Cyclops/}{Cyclops} (Cyclic coordinate descent for logistic, Poisson and survival analysis) package to perform large-scale regularized logistic regression. \textbf{Hyper-parameters}: var (starting variance), seed.

\hypertarget{gradient-boosting-machines}{%
\subsection{Gradient boosting machines}\label{gradient-boosting-machines}}

Gradient boosting machines is a boosting ensemble technique and in our framework it combines multiple decision trees. Boosting works by iteratively adding decision trees but adds more weight to the data-points that are misclassified by prior decision trees in the cost function when training the next tree. We use Extreme Gradient Boosting, which is an efficient implementation of the gradient boosting framework implemented in the xgboost R package available from CRAN. \textbf{Hyper-parameters}: ntree (number of trees), max depth (max levels in tree), min rows (minimum data points in in node), learning rate, seed \textbar{} mtry (number of features in each tree),ntree (number of trees), maxDepth (max levels in tree), minRows (minimum data points in in node),balance (balance class labels), seed.

\hypertarget{random-forest}{%
\subsection{Random forest}\label{random-forest}}

Random forest is a bagging ensemble technique that combines multiple decision trees. The idea behind bagging is to reduce the likelihood of overfitting, by using weak classifiers, but combining multiple diverse weak classifiers into a strong classifier. Random forest accomplishes this by training multiple decision trees but only using a subset of the variables in each tree and the subset of variables differ between trees. Our packages uses the sklearn learn implementation of Random Forest in python. \textbf{Hyper-parameters}: mtry (number of features in each tree),ntree (number of trees), maxDepth (max levels in tree), minRows (minimum data points in in node),balance (balance class labels), seed.

\hypertarget{k-nearest-neighbors}{%
\subsection{K-nearest neighbors}\label{k-nearest-neighbors}}

K-nearest neighbors (KNN) is an algorithm that uses some metric to find the K closest labelled data-points, given the specified metric, to a new unlabelled data-point. The prediction of the new data-points is then the most prevalent class of the K-nearest labelled data-points. There is a sharing limitation of KNN, as the model requires labelled data to perform the prediction on new data, and it is often not possible to share this data across data sites. We included the \href{https://github.com/OHDSI/BigKnn}{BigKnn} package developed in OHDSI which is a large scale k-nearest neighbor classifier. \textbf{Hyper-parameters}: k (number of neighbours), weighted (weight by inverse frequency).

\hypertarget{naive-bayes}{%
\subsection{Naive Bayes}\label{naive-bayes}}

The Naive Bayes algorithm applies the Bayes theorem with the naive assumption of conditional independence between every pair of features given the value of the class variable. Based on the likelihood the data belongs to a class and the prior distribution of the class, a posterior distribution is obtained. \textbf{Hyper-parameters}: none.

\hypertarget{adaboost}{%
\subsection{AdaBoost}\label{adaboost}}

AdaBoost is a boosting ensemble technique. Boosting works by iteratively adding classifiers but adds more weight to the data-points that are misclassified by prior classifiers in the cost function when training the next classifier. We use the sklearn AdaboostClassifier implementation in Python. \textbf{Hyper-parameters}: nEstimators (the maximum number of estimators at which boosting is terminated), learningRate (learning rate shrinks the contribution of each classifier by learning\_rate. There is a trade-off between learningRate and nEstimators).

\hypertarget{decision-tree}{%
\subsection{Decision Tree}\label{decision-tree}}

A decision tree is a classifier that partitions the variable space using individual tests selected using a greedy approach. It aims to find partitions that have the highest information gain to separate the classes. The decision tree can easily overfit by enabling a large number of partitions (tree depth) and often needs some regularization (e.g., pruning or specifying hyper-parameters that limit the complexity of the model). We use the sklearn DecisionTreeClassifier implementation in Python. \textbf{Hyper-parameters}: maxDepth (the maximum depth of the tree), minSamplesSplit,minSamplesLeaf, minImpuritySplit (threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.), seed,classWeight (``Balance''" or ``None'').

\hypertarget{multilayer-perception}{%
\subsection{Multilayer Perception}\label{multilayer-perception}}

Neural networks containing multiple layers that weight their inputs using a non-linear function. The first layer is the input layer, the last layer is the output layer the between are the hidden layers. Neural networks are generally trained using feed forward back-propagation. This is when you go through the network with a data-point and calculate the error between the true label and predicted label, then go backwards through the network and update the linear function weights based on the error. \textbf{Hyper-parameters}: size (the number of hidden nodes), alpha (the l2 regularisation), seed.

\hypertarget{deep-learning}{%
\subsection{Deep Learning}\label{deep-learning}}

Deep learning such as deep nets, convolutional neural networks or recurrent neural networks are similar to a neural network but have multiple hidden layers that aim to learn latent representations useful for prediction. In a seperate vignette in the \href{https://ohdsi.github.io/PatientLevelPrediction/}{PatientLevelPrediction} package we describe these models and hyper-parameters in more detail.

\hypertarget{evalaution}{%
\section{Evaluating Patient-Level Prediction Models}\label{evalaution}}

\hypertarget{evaluation-types}{%
\subsection{Evaluation Types}\label{evaluation-types}}

There are various ways to evaluate a prediction model: internal validation, external valdiation, temporal validation and spatial validation.

Internal validation is when a prediction model is evaluated using the same dataset used to develop the model. There are three ways to perform internal validation: using a holdout set, using cross validation and using bootstrapping. In the patient-level prediction framework we use a holdout set for internal validation.

A holdout set approach simply splits the labelled data into two independent sets, a train set and a test set (the hold out set). The train set is used to learn the model and the test set is used to evaluate it.

Cross validation is useful when the data are limited. A user needs to specify the number of folds, such as 10, then the data are split into that number of independent sets. For each data split, a model is trained on all other data splits and then applied to the split to obtain the predicted risks for each patient in the split. The model performance can then be estimated using the predicted risk obtained when the patients were held out from model training. A form of cross validation is leave one out validation, where for each patient, the model is trained using all other data except that patient's data and then applied to the patient to obtain their predicted risk. This is repeated for each patient to obtain risks for all the patients which can be used to evaluate the model. In the patient-level prediction framework we use cross validation to pick the optimal hyper-parameters on the train set.

Bootstrapping is useful when calculating confidence intervals. In bootstrapping multiple sample sets are drawn with replacement from the whole labelled dataset to generate hold out sets, the unsampled patient data are used to develop the models and then evaluated on the sample sets. This gives a range for each metric. We currently do not use bootstrapping in the patient-level prediction framework.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/theory/validationTypes} \caption{Types of internal valdiation.}\label{fig:figuretheoryval}
\end{figure}

External validation is when a model trained on one dataset is validated on a new dataset or set of patients. This is important as it helps model developers understand which types of patients the model will transport to.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/theory/externalValidation} \caption{Visulisation of external validation.}\label{fig:figuretheoryextval}
\end{figure}

Temporal validation is a type of validation where a model is validated on data that were collected after the data used to develop the model. This can help identify situations where they may be temporal shifts in the data that impact the transportability of the model across time. Another type of validation, spatial validation, is location based where a model is developed on patients for some locations (perhaps certain hospitals or doctor surgeries) and validated on patients at a different location.

\hypertarget{performance}{%
\subsection{Performance Metrics}\label{performance}}

** Threhold measures **
A prediction model assigned a value between 0 and 1 for each patient corresponding to the risk of the patient having the outcome during the time at risk. A value of 0 means 0\% risk, a value of 0.5 means 50\% risk and a value of 1 means 100\% risk. Common metrics such as accuracy, sensitivity, specificity, positivity predictive value can be calculated by first specifying a threshold that is used to class patients as having the outcome or not having the outcome during the time at risk. For example, given Table \ref{tab:tabletheorytab}, if we set the threshold as 0.5, the patients 1,3,7 and 10 have a predicted risk greater than or equal to the threshold of 0.5 so they would be predicted to have the outcome. All other patients had a predicted risk less than 0.5, so would be predicted to not have the outcome.

\begin{longtable}[]{@{}lcrrr@{}}
\toprule
\begin{minipage}[b]{0.25\columnwidth}\raggedright
Patient ID\strut
\end{minipage} & \begin{minipage}[b]{0.25\columnwidth}\centering
Predicted risk\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedleft
Predicted class at 0.5 theshold\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedleft
Has outcome during TAR\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedleft
Type\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.25\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.8\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
1\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
1\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
TP\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.1\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
TN\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
3\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.7\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
1\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
FP\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
4\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
TN\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
5\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.05\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
TN\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
6\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.1\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
TN\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
7\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.9\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
1\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
1\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
TP\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
8\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.2\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
1\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
FN\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
9\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.3\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
TN\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
10\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.5\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
1\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft
FP\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

If a patient is predicted to have the outcome and has the outcome during TAR then this is called as a true positive (TP). If a patient is predicted to have the outcome but does not have the outcome during TAR then this I called a false positive (FP). If a patient is predicted to not have the outcome and does not have the outcome during TAR then this is called a true negative (TN). Finally, if a patient is predicted to not have the outcome but does have the outcome during TAR then this is called a false negative (FN).

The following threshold based metrics are:

\begin{itemize}
\tightlist
\item
  accuracy: (TP+TN)/(TP+TN+FP+FN)
\item
  sensitivity: TP/(TP+FN)
\item
  specificity: TN/(TN+FP)
\item
  positive predictive value: TP/(TP+FP)
\end{itemize}

Note that these values can either decrease or increase if the threshold is lowered. Lowering the threshold of a classifier may increase the denominator, by increasing the number of results returned. If the threshold was previously set too high, the new results may all be true positives, which will increase positive predictive value. If the previous threshold was about right or too low, further lowering the threshold will introduce false positives, decreasing positive predictive value. For sensitivity the denominator does not depend on the classifier threshold (TP+FN is a constant). This means that lowering the classifier threshold may increase sensitivity, by increasing the number of true positive results. It is also possible that lowering the threshold may leave sensitivty unchanged, while the positive predictive value fluctuates.

Figure \ref{fig:plpPrecisionRecall} shows the tradeoff between positive predictive value (precision) and sensitivity (recall).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/precisionRecall} 

}

\caption{Positive predictive value-sensitivy aka Precision-recall plot.}\label{fig:plpPrecisionRecall}
\end{figure}

** Overal Performance **
There are two main criteria used to assess the overal performance of a prediction model; discrimination and calibration.

** Discrimiantion **

Discrimination is the ability to assign a higher risk to patients who will experience the outcome during the time at risk. The Receiver Operating Characteristics (ROC) is determined by plotting 1 -- specificity on the x-axis and sensitivity on the y-axis at all possible thresholds, see Figure \ref{fig:figuretheoryroc}. The dashed diagonal line in Figure \ref{fig:figuretheoryroc} is the performance of a model that randomly assigns predictions. The area under the receiver operating characteristic curve (AUROC) gives an overall measure of discrimination where a value of 0.5 corresponds to randomly assigning the risk and a value of 1 means perfect discrimination. In reality, most prediction models obtain AUCs between 0.6-0.8. The AUROC is invariant to class imbalance, unlike accuracy, but for rare outcomes even a model with a high AUROC may not be practical. When the outcome is rare another measure known as the area under the precision recall curve (AUPRC) is recommended. A model may obtain a high AUC when the outcome is rare but have a very low positive predictive value. This would mean may false positives. Depending on the severity of the outcome and cost (health risk and/or monetary) of some intervention, a low false positive rate may result in a non-practical model. The AUPRC is the area under the line generated by plotting the sensitivity on the x-axis (also known as the recall) and the positive predictive value (also known as the precision) on the y-axis.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/theory/discrimination} \caption{Example discrimination plots generated by the patient-level prediction framework.}\label{fig:figuretheoryroc}
\end{figure}

The AUROC provides a way to determine how different the predicted risk distributions are between the patients who experience the outcome during the time at risk and those who do not. If the AUROC is high, then the distributions will be mostly disjointed, whereas when there is a lot of overlap, the AUROC will be closer to 0.5, see Figure \ref{fig:figuretheoryroctheory}.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/theory/roctheory} \caption{How the ROC plots are linked to discrimination.}\label{fig:figuretheoryroctheory}
\end{figure}

\emph{Preference distribution}

The preference distribution plot (Figure \ref{fig:plpPreference}) shows the preference score distributions for people in the test set with the outcome (red) without the outcome (blue).

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/PatientLevelPrediction/preferencePDF} 

}

\caption{Preference distribution plot.}\label{fig:plpPreference}
\end{figure}

\emph{Predicted probability distribution}

The prediction distribution box plot shows the predicted risks of the people in the test set with the outcome (blue) and without the outcome (red).

The box plots in Figure \ref{fig:plpPredProb} show that the predicted probability of the outcome is indeed higher for those with the outcome but there is also overlap between the two distribution which lead to an imperfect discrimination.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/PatientLevelPrediction/predictionDistribution} 

}

\caption{Predicted probability distribution.}\label{fig:plpPredProb}
\end{figure}

** Calibration **

Calibration is the ability of the model to assign a correct risk. For example, if the model assigned one hundred patients a risk of 10\% then ten of the patients should experience the outcome during the time at risk. If the model assigned 100 patients a risk of 80\% then eighty of the patients should experience the outcome during the time at risk. The calibration is generally calculated by partitioning the patients into deciles based on the predicted risk and in each group calculating the mean predicted risk and the fraction of the patients who experienced the outcome during the time at risk. We then plot these ten points (predicted risk on the y-axis and observed risk on the x-axis) and see whether they fall on the x = y line, indicating the model is well calibrated. We also fit a linear model using the points to calculate the intercept (which should be close to 0) and the gradient (which should be close to 1). If the gradient is greater than 1 then the model is assigning a higher risk than the true risk and if the gradient is less than 1 the model is assigning a lower risk than the true risk.

AN example calibration plot generated by the Patient-level prediction package is shown in Figure \ref{fig:figuretheorycal}. The diagonal dashed line thus indicates a perfectly calibrated model. The ten (or fewer) dots represent the mean predicted values for each quantile plotted against the observed fraction of people in that quantile who had the outcome (observed fraction). The straight black line is the linear regression using these 10 plotted quantile mean predicted vs observed fraction points. The straight vertical lines represented the 95\% lower and upper confidence intervals of the slope of the fitted line.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/theory/calibration} \caption{Example calibration plots generated by the patient-level prediction framework.}\label{fig:figuretheorycal}
\end{figure}

\emph{Smooth Calibration}

Similar to the traditional calibration shown above the Smooth Calibration plot shows the relationship between predicted and observed risk. the major difference is that the smooth fit allows for a more fine grained examination of this. Whereas the traditional plot will be heavily influenced by the areas with the highest density of data the smooth plot will provide the same information for this region as well as a more accurate interpretation of areas with lower density. the plot also contains information on the distribution of the outcomes relative to predicted risk.

Figure \ref{fig:plpSmoothCal} shows an example that better demonstrates the impact of using a smooth calibration plot. The default line fit would not highlight the miss-calibration at the lower predicted probability levels that well.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/PatientLevelPrediction/smoothCalibration} 

}

\caption{Smooth calibration plot.}\label{fig:plpSmoothCal}
\end{figure}

\emph{Demographic summary}
It can also be useful to determine how well calibrated a model is for different demographics (age and gender groups). This can be calculated by partitioning the patients into groups of similar age and gender and comparing the mean predicted risk within the group with the observed fraction of the patients who experience the outcome during the time at risk. This can identify demographic groups where the model does not perform well when applied. Figure \ref{fig:plpDemoSummary} shows for females and males the expected and observed risk in different age groups together with a confidence area. This example shows that the model is well calibrated across gender and age groups.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/PatientLevelPrediction/demographicSummary} 

}

\caption{Precision-recall plot.}\label{fig:plpDemoSummary}
\end{figure}

\hypertarget{inspecting-the-model}{%
\subsection{Inspecting the model}\label{inspecting-the-model}}

\emph{Test-Train similarity}

The test-train similarity is assessed by plotting the mean covariate values in the train set against those in the test set for people with and without the outcome. This can be a useful way to see how similar the test and train sets are.

The results in Figure \ref{fig:plpTestTrain} show that the mean values of the covariates in both the test and train sets are comparable as the points are on the diagonal. If some of the points were off the diagonal then this would tell us the test and train datasets differed.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/PatientLevelPrediction/generalizability} 

}

\caption{Predicted probability distribution.}\label{fig:plpTestTrain}
\end{figure}

\emph{Variable scatter plot}

The variable scatter plot shows the mean covariate value for the people with the outcome against the mean covariate value for the people without the outcome. The color of the dots corresponds to the inclusion (green) or exclusion in the model (blue), respectively. Figure \ref{fig:plpVarScatter} shows that the mean of most of the covariates is higher for subjects with the outcome compared to those without. This would indicate that the patients with the outcome are generally sicker.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/PatientLevelPrediction/variableScatterplot} 

}

\caption{Predicted probability distribution.}\label{fig:plpVarScatter}
\end{figure}

\newpage

\hypertarget{specfifying-a-patient-level-prediction-study}{%
\section{Specfifying a Patient-level Prediction Study}\label{specfifying-a-patient-level-prediction-study}}

In this section we will demonstrate how to define a prediction problem using an example for hypertension.

The first step is to clearly define the prediction problem. Interestingly, in many published papers the prediction problem is poorly defined, e.g.~it is unclear how the index date (start of the target Cohort) is defined. A poorly defined prediction problem does not allow for external validation by others let alone implementation in clinical practice. In the PLP framework we have enforced that we have to define the prediction problem we like to address, in which population we will build the model, which model we will build and how we will evaluate its performance. In this section we will guide you through this process and we will use a ``Treatment safety'' prediction type as an example.

\hypertarget{problem-definition-2}{%
\subsection{Problem definition}\label{problem-definition-2}}

Angioedema is a well known side-effect of ACE inhibitors, and the incidence of angioedema reported in the labeling for ACE inhibitors is in the range of 0.1\% to 0.7\% \citep{byrd_2006}. Monitoring patients for this adverse effect is important, because although angioedema is rare, it may be life-threatening, leading to respiratory arrest and death \citep{norman_2013}. Further, if angioedema is not initially recognized, it may lead to extensive and expensive workups before it is identified as a cause \citep{norman_2013, thompson_1993}. Other than the higher risk among African-American patients, there are no known predisposing factors for the development of ACE inhibitor related angioedema \citep{byrd_2006}. Most reactions occur within the first week or month of initial therapy and often within hours of the initial dose \citep{circardi_2004}. However, some cases may occur years after therapy has begun \citep{mara_1996}.No diagnostic test is available that specifically identifies those at risk. If we could identify those at risk, doctors could act, for example by discontinuing the ACE inhibitorin favor of another hypertension drug.

We will apply the PLP framework to observational healthcare data to address the following patient-level prediction question:

\begin{quote}
Amongst patients who have just started on an ACE inhibitor for the first time, who will experience angioedema in the following year?
\end{quote}

\hypertarget{study-population-definition}{%
\subsection{Study population definition}\label{study-population-definition}}

The final study population in which we will develop our model is often a subset of the target population, because we will e.g.~apply criteria that are dependent on T and O or we want to do sensitivity analyses with subpopulations of T. For this we have to answer the following questions:

\begin{itemize}
\item
  \emph{What is the minimum amount of observation time we require before the start of the target cohort?} This choice could depend on the available patient time in your training data, but also on the time you expect to be available in the data sources you want to apply the model on in the future. The longer the minimum observation time, the more baseline history time is available for each person to use for feature extraction, but the fewer patients will qualify for analysis. Moreover, there could be clinical reasons to choose a short or longer lookback period. For our example, we will use a prior history as lookback period (washout period).
\item
  \emph{Can patients enter the target cohort multiple times?} In the target cohort definition, a person may qualify for the cohort multiple times during different spans of time, for example if they had different episodes of a disease or separate periods of exposure to a medical product. The cohort definition does not necessarily apply a restriction to only let the patients enter once, but in the context of a particular patient-level prediction problem, a user may want to restrict the cohort to the first qualifying episode. In our example, a person can only enter the target cohort once since our criteria was based on first use of an ACE inhibitor.
\item
  \emph{Do we allow persons to enter the cohort if they experienced the outcome before?} Do we allow persons to enter the target cohort if they experienced the outcome before qualifying for the target cohort? Depending on the particular patient-level prediction problem, there may be a desire to predict incident first occurrence of an outcome, in which case patients who have previously experienced the outcome are not at-risk for having a first occurrence and therefore should be excluded from the target cohort. In other circumstances, there may be a desire to predict prevalent episodes, whereby patients with prior outcomes can be included in the analysis and the prior outcome itself can be a predictor of future outcomes. For our prediction example, we will choose not to include those with prior angioedema.
\item
  \emph{How do we define the period in which we will predict our outcome relative to the target cohort start?} We actually have to make two decisions to answer that question. First, does the time-at-risk window start at the date of the start of the target cohort or later? Arguments to make it start later could be that you want to avoid outcomes that were entered late in the record that actually occurred before the start of the target cohort or you want to leave a gap where interventions to prevent the outcome could theoretically be implemented. Second, you need to define the time-at-risk by setting the risk window end, as some specification of days offset relative to the target cohort start or end dates. For our problem we will predict in a time-at-risk window starting 1 day after the start of the target cohort up to 365 days later.
\item
  \emph{Do we require a minimum amount of time-at-risk?} We have to decide if we want to include patients that did not experience the outcome but did leave the database earlier than the end of our time-at-risk period. These patients may experience the outcome when we do not observe them. For our prediction problem we decide to answer this question with Yes, require a mimimum time-at-risk for that reason. Furthermore, we have to decide if this constraint also applies to persons who experienced the outcome or we will include all persons with the outcome irrespective of their total time at risk. For example, if the outcome is death, then persons with the outcome are likely censored before the full time-at-risk period is complete.
\end{itemize}

\hypertarget{model-development-settings}{%
\subsection{Model development settings}\label{model-development-settings}}

To develop the model we have to decide which algorithm(s) we like to train. We see the selection of the best algorithm for a certain prediction problem as an empirical question, i.e.~you need to let the data speak for itself and try different approaches to find the best one. There is no algorithm that will work best for all problems (no free lunch). In our framework we therefore aim to implement many algorithms. Furthermore, we made the system modular so you can add your own custom algorithms. This out-of-scope for this chapter but mode details can be found in the \emph{AddingCustomAlgorithms} vignette in the \href{https://ohdsi.github.io/PatientLevelPrediction/}{PatientLevelPrediction} package.

Our framework currently contains the following algorithms to choose from:

Furthermore, we have to decide on the \textbf{covariates} that we will use to train our model. In our example, we like to add gender, age, all conditions, drugs and drug groups, and visit counts. We also have to specify in which time windows we will look and we decide to look in year before and any time prior.

\hypertarget{model-evaluation}{%
\subsection{Model evaluation}\label{model-evaluation}}

Finally, we have to define how we will train and test our model on our data, i.e.~how we perform \textbf{internal validation}. For this we have to decide how we divide our dataset in a training and testing dataset and how we randomly assign patients to these two sets. Dependent on the size of the training set we can decide how much data we like to use for training, typically this is a 75\% - 25\% split. If you have very large datasets you can use more data for training. To randomly assign patients to the training and testing set, there are two commonly used approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  split by person. In this case a random seed is used to assign the patient to either sets.
\item
  split by time. In this case a time point is used to split the persons, e.g.~75\% of the data is before and 25\% is after this date. The advantage of this is that you take into consideration that the health care system has changed over time.
\end{enumerate}

For our prediction model we decide to start with a Regularized Logistic Regression and will use the default parameters. We will do a 75\%-25\% split by person.

\hypertarget{study-summary-1}{%
\subsection{Study summary}\label{study-summary-1}}

We now completely defined our study as shown in Table \ref{tab:plpSummary}.

\begin{longtable}[]{@{}ll@{}}
\caption{\label{tab:plpSummary} Main design choices for our study.}\tabularnewline
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Value\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
Choice\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Value\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Target cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
Patients who have just started on an ACE inhibitor for the first time.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Outcome cohort\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
Angioedema.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Time-at-risk\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
1 day till 365 days from cohort start. We will require at least 364 days at risk.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
Model\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
Gradient Boosting Machine with hyper-parameters ntree: 5000, max depth: 4 or 7 or 10 and learning rate: 0.001 or 0.01 or 0.1 or 0.9. Covariates will include gender, age, conditions, drugs, drug groups, and visit count. Data split: 75\% train - 25\% test, randomly assigned by person.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

We define the target cohort as the first exposure to any ACE inhibitor. Patients are excluded if they have less than 365 days of prior observation time or have prior angioedema.

\newpage

\hypertarget{implementing-the-study-in-atlas}{%
\section{Implementing the study in Atlas}\label{implementing-the-study-in-atlas}}

\hypertarget{introduction-2}{%
\subsection{Introduction}\label{introduction-2}}

The atlas interface to patient-level prediction enables a users to design a prediction study analysis containing multiple prediction questions and analyses settings. The atlas interface creates a prediction study R package populated with all the code ready to develop and evaluate the specified models. All a user needs to develop the models is R studio with:

\begin{itemize}
\tightlist
\item
  OHDSI's PatientLevelPrediction R package installed
\item
  devtools R package installed
\item
  connection details for the OMOP CDM databases
\end{itemize}

The atlas created prediction study R package has additional functionality to:

\begin{itemize}
\tightlist
\item
  Create a study protocol template
\item
  Create a shiny app for interactively exploring the results
\item
  Create a validation study R package that can be shared to externally validate the developed models
\end{itemize}

In this section we will detail the design choices for the prediction problem specification, the analysis settings and the executing settings. We will then guide the user through the process of reviewing analysis, downloading and running the prediction study package and interpreting the results via the shiny app.

\hypertarget{the-atlas-layout}{%
\subsection{The Atlas layout}\label{the-atlas-layout}}

The interface for designing a prediction study can be opened by clicking on the `Prediction' button in the left hand side atlas menu.

Once in the `Prediction' view you should see Figure \ref{fig:figure2b}

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/prediction_page} \caption{The Atlas prediction page.}\label{fig:figure2b}
\end{figure}

You can create a new study by clicking on the blue `New Patient Level Prediction'' button or by clicking on a row in the table with the name of the study you want to open. Once inside the prediction study (either by clicking the blue `New Patient Level Prediction'' button or an existing row in the table) you should see a specification options as shown in \ref{fig:specification1}-\ref{fig:specification2} with the top stating `Patient Level Prediction' with a number such as `\#46', as highlighed by the red A. This tells us the cohort definition id is 46.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/specification_p1} \caption{The atlas prediction specification part 1.}\label{fig:specification1}
\end{figure}

To the right of the `Patient Level Prediction \#46' there are buttons to save (green button), exit (blue button with x), copy (blue button with double paper) and delete (red button with bin) the current study highlighted by C-F respectively in Figure \ref{fig:specification1}.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/specification_p2} \caption{The atlas prediction specification part 2}\label{fig:specification2}
\end{figure}

Below these is a white text form where you can name the study (B in Figure \ref{fig:specification1}). The `Specification' tab (G in Figure \ref{fig:specification1}) contains all the settings a user needs to define for the prediction study. The first part is the `Prediction Problem Settings' (J in Figure \ref{fig:specification1}), this is where the user defines the Target cohorts and Outcome cohorts for the prediction analyses. These cohorts need to be created in atlas using the `Cohort Definition' view and can then be imported into the Prediction study. Instantiating cohorts is described in Chapter \ref{Cohorts}.

The next part of the `Specification' is the `Analysis Settings' (N in Figure \ref{fig:specification1}). This is where the user specifies the models to train (classifiers or survival models), the candidate covariates (these are standard OHDSI covariates), the time-at-risk and additional inclusion criteria.

Then the `Execution Settings' (R in Figure \ref{fig:specification2}) define how many patients to extract for the model development, whether to remove rare covariates and whether to normalise the covariates.

Finally, the last part in the `Specification' is the `Training Settings' (S in Figure \ref{fig:specification2}) which specifies' how to split the labelled data into data used to develop the model (including how many folds you want to use when applying cross validation) and validate the model.

Each of the `Specification' settings are described in more detail in the following sections. We also describe the `Utilities' tab (H in Figure \ref{fig:specification1}) where a user can review, import/export and download their study as an executional R library.

\hypertarget{atlas-specification-tab}{%
\subsection{Atlas Specification Tab}\label{atlas-specification-tab}}

The specification section is where a user can specify her prediction question, covariates, additional study population inclusion criteria, model type and hyper-parameters and execution settings.

\hypertarget{prediction-problem-settings}{%
\subsection{Prediction Problem Settings}\label{prediction-problem-settings}}

The prediction problem settings enables you to select the target population cohorts and outcome cohorts for the analysis. A prediction model will be developed for all combinations of the target population cohorts and the outcome cohorts.

For example, if you specify two target populations:

\begin{itemize}
\tightlist
\item
  `T1: new users of ACE inhibitors'
\item
  `T2: new users of ACE inhibitors with no prior anti-hypertensive'
\end{itemize}

and three outcomes:

\begin{itemize}
\tightlist
\item
  `O1: angioedema'
\item
  `O2: stroke'
\item
  `O3: myocardial infarction'
\end{itemize}

then six prediction problems will be investigated in the study:

\begin{itemize}
\tightlist
\item
  `In T1: new users of ACE inhibitors predict O1: angioedema during TAR'
\item
  `In T1: new users of ACE inhibitors predict O1: stroke during TAR'
\item
  `In T1: new users of ACE inhibitors predict O1: myocardial infarction during TAR'
\item
  `In T2: new users of ACE inhibitors with no prior anti-hypertensive predict O1: angioedema during TAR'
\item
  `In T2: new users of ACE inhibitors with no prior anti-hypertensive predict O1: stroke during TAR'
\item
  `In T2: new users of ACE inhibitors with no prior anti-hypertensive predict O1: myocardial infarction during TAR'
\end{itemize}

To select a target population cohort you need to have previously defined it atlas. Instantiating cohorts is described in Chapter \ref{Cohorts}. The Appendix provides the full definitions of the target (Appendix \ref{AceInhibitors}) and outcome (Appendix \ref{Angioedema}) cohorts used in this example. To add a target population to the cohort you then need to click on the blue `+ Add Target Cohort' button, see K in Figure \ref{fig:specification1}.

This will open up a table of cohorts that have been created in atlas, see Figure \ref{fig:figure2k}.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/target_pop_cohort_table} \caption{The Training Settings area}\label{fig:figure2k}
\end{figure}

You can simple click on any row in the table to add that cohort. If you have many cohorts, using the filter option on the top right may help (just make sure to remember the cohort name). We filtered the book of ohdsi cohorts by adding `book' to the filter as the cohort names all included the work `book', see Figure \ref{fig:figure2l}.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/target_pop_cohort_filter} \caption{The Training Settings area}\label{fig:figure2l}
\end{figure}

By clicking on the row `{[}BookOfOHDSI{]} New users of ACE inhibitors as first-line monotherapy for hypertension' this is now added as a target population cohort in the study. This process can be repeated to add more target population cohorts. Adding outcome cohorts is a similar process, but requires click on the blue `+ Add Outcome Cohort' button (M in Figure \ref{fig:specification1}).

You need to specify, at minimum, one target population cohort and one outcome cohort. Once you have added all the target population cohorts and outcome cohorts you are now ready to procede to the analysis settings.

\hypertarget{analysis-settings}{%
\subsection{Analysis Settings}\label{analysis-settings}}

The analysis settings enables you to pick the supervised learning models, the covariates and population settings.

\hypertarget{model-settings}{%
\subsection{Model Settings}\label{model-settings}}

You can pick one or more supervised learning models to investigate using for model development. To add a supervised learning model click on the blue `+ Add Model Settings' button (O in Figure \ref{fig:specification1}). A dropdown containing all the models currently supported in the Atlas interface will appear (note: more models may be available outside of Atlas).

You can select the supervised learning model you want to include in the study by clicking on the name in the dropdown menu. This will then take you to a view for that specific model and the hyper-parameters you can include into a grid search. For example, if I click on `Lasso Logistic Regression' the following view shown in Figure \ref{fig:figureAS2} will appear.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/analysis_lasso_lr_view} \caption{The lasso logistic regression view}\label{fig:figureAS2}
\end{figure}

As the Lasso Logistic Regression model only has one hyper-parameter, we do an automatic search for the optimal value rather than a grid search so a user just needs to specify the starting value, see Figure \ref{fig:figureAS2}. Once you are happy with the hyper-parameter settings you can return to the main settings view by clicking on the grey `\textless{}' button.

You will now see your chosen supervised learning model added tosection N in Figure \ref{fig:specification1}. To edit the model you added, click on the corresponding row and it will take you back to the model view where you can edit the hyper-parameter settings.

To add a gradient boosting machine model we can follow the same process and click on `Gradient Boosting Machine' in the drop down menu. This will take us into the gradient boosting machine view:

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/analysis_gbm_view} \caption{The gradient boosting machine view}\label{fig:figureAS5}
\end{figure}

The gradient boosting machine model has four hyper-parameters you can define a grid search for (boosting learn rate, maximum number of interactions, minimum number of trees and number of trees to build). Initially the default values are shown, but a user can add a new value by typing it into the text field at the bottom of the hyper-parameter box and clicking on the blue `Add' button.\\

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/analysis_adding_hyper} \caption{Adding a hyper-parameter value into the grid search}\label{fig:figureAS6}
\end{figure}

It is also possible to remove a hyper-parameter value from the grid search by clicking on `Remove' for the corresponding row:

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/analysis_removing_hyper} \caption{Removing a hyper-parameter value into the grid search}\label{fig:figureAS7}
\end{figure}

Once happy with the hyper-parameters, click on the grey `\textless{}' button on the top left to add the model into the prediction study. You will now see you model and hyper-parameter settings in the `Model Setttings' table. Repeat the process to include all the supervised learning models you want to investigate.

\hypertarget{covariate-settings}{%
\subsection{Covariate Settings}\label{covariate-settings}}

We have defined a set of \emph{standard} covariates that can be extracted from the observational data in the OMOP CDM format. In the covariate settings view, it is possible to select which of the standard covariates to include. It is possible to add many different types of covariate settings.

To add a covariate setting into the study, click on the blue `+ Add Covariate Settings' button (P in Figure \ref{fig:specification2}). This will take you into the covariate setting view:

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/covariates_p1} \caption{The covariate settings view part 1}\label{fig:covariate1}
\end{figure}

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/covariates_p2} \caption{The covariate settings view part 2}\label{fig:covariate2}
\end{figure}

The \emph{standard} OHDSI covariates includes indicator covariates corresponding to any concept id that is recorded in the database. The indicator covariates are binary and indicate whether a patient had a concept id recorded during some time interval relative to the target cohort start date. The user can specify up to three time intervals, longterm, mediumterm and shortterm in addition to using anytime prior. There is also the option of whether to include the target cohort start date.

Although the \emph{standard} OHDSI covariates iclude 4 time intervals (all time prior, longterm, mediumterm and short term) and all concept ids, generally only a subset of these covariates will be chosen. The concept ids can be restricted by OHDSI vocabulary domain (condition, drug, procedure, measurement and observation). Generally, a user will select one or two time intervals and some of the domains. For example, if a user selects long term (using default set to 365 days prior) conditions and drugs and anytime prior measurements with end days set to 0, then there could be covariates for any condition or drug concept id record 365 days prior to and up to the cohort start day for any patient in the target cohort and covariates for any measurement concept id recorded on the cohort start day or anytime prior.

Age group and gender are also binary covariates, with age group covariates for every 5 years (0-4, 5-9, 10-14, \ldots, 95+).

Non binary covariates include age, domain counts, such as the number of condition concept ids that were recorded for each time interval per patient or the number of inpatient visits a patient had during the time interval. Measurement covaraites can be binary (indicating a measurement was taken or whether it was abnormal) or non-binary (the value of the measurement). Existing risk scores can also be chosen.

\textbf{Include/Exclude options}

The first part is of the covariate settings is the exclude/include option, see A Figure \ref{fig:covariate1}. Previously we mentioned that covariates are generally constructed for any concept id in the chosen time intervals and domains. However, you may be in a situation where you only want to include certain concept ids or you may want to exlcude concept ids (e.g., if the concept id is linked to the target cohort definition).

To only include certain concepts, create a concept set in atlas and then under the ``What concepts do you want to include in baseline covariates in the patient-level prediction model? (Leave blank if you want to include everything)'' select the concept set by clicking on the blue button with a folder icon (see A in Figure \ref{fig:covariate1}). This will then open up a table with all the concept sets, select the one you want. You can include the concept ids in the concept set and all descendants by select `yes' to the ``Should descendant concepts be added to the list of included concepts?'' option. This option will mean after you select the covariates you want, only covariates corresponding to these included concept ids will be included.

The same process can be repeated for the ``What concepts do you want to exclude in baseline covariates in the patient-level prediction model? (Leave blank if you want to include everything)'' but this will mean after you select the covariates you want, any covariates corresponding to these concept ids will be removed.

To remove any include/exclude setting, click on the red button with an X (see B in Figure \ref{fig:covariate1}).

The final option ``A comma delimited list of covariate IDs that should be restricted to:'' (see C in Figure \ref{fig:covariate1}) enables you to add a set of covariate ids (rather than concept ids) comma seperated that will only be included in the model. For example if you wanted covariate ids 340504504 and 8373747504 then you would type ``340504504,8373747504'' into the text box. You must ensure the domain/time interval corresponding to these covariates are selected below.

\textbf{Non time bound options}

The next section enables the selection of non-time bound variables (see D in Figure \ref{fig:covariate1}).

\begin{itemize}
\tightlist
\item
  Gender: a binary variable indicating male or female gender
\item
  Age: a continuous variable corresponding to age in years
\item
  Age group: binary variables for every 5 years of age (0-4, 5-9, 10-14, \ldots, 95+)
\item
  Race: a binary variable for each race, 1 means the patient has that race recorded, 0 otherwise
\item
  Ethnicity: a binary variable for each ethnicity, 1 means the patient has that ethnicity recorded, 0 otherwise
\item
  Index year: {[}Not recommended for prediction{]} a binary variable for each cohort start date year, 1 means that was the patients cohort start date year, 0 otherwise
\item
  Index month - a binary variable for each cohort start date month, 1 means that was the patients cohort start date month, 0 otherwise
\item
  Prior observation time: {[}Not recommended for prediction{]} a continuous variable corresponding to how long in days the patient was in the database prior to the cohort start date
\item
  Post observation time: {[}Not recommended for prediction{]} a continuous variable corresponding to how long in days the patient was in the database post cohort start date
\item
  Time in cohort: a continuous variable corresponding to how long in days the patient was in the cohort (cohort end date minus cohort start date)
\item
  Index year and month: {[}Not recommended for prediction{]} a binary variable for each cohort start date year and month combination, 1 means that was the patients cohort start date year and month, 0 otherwise
\end{itemize}

To include any of these variables, click the corresponding unticked box to add a tick (clicking a ticked box will remove the variable).

\textbf{Time interval options}

The standard covariates enable three flexible time intervals for the covariates:

\begin{itemize}
\tightlist
\item
  end days: when to end the time intervals relative to the cohort start date {[}default is 0{]}
\item
  long term {[}default -365 days to end days prior to cohort start date{]}
\item
  medium term {[}default -180 days to end days prior to cohort start date{]}
\item
  short term {[}default -30 days to end days prior to cohort start date{]}
\end{itemize}

These settings can be input into the text boxes to update them at E-H in Figure \ref{fig:covariate1}.

\textbf{Domain covariates}

The next option is the covariates extracted from the era tables (see J in Figure \ref{fig:covariate1}):

\begin{itemize}
\tightlist
\item
  Condition: Construct covariates for each condition concept id and time interval selected and if a patient has the concept id with an era (i.e., the condition starts or ends during the time interval or starts before and ends after the time interval) during the specified time interval prior to the cohort start date in the condition era table, the covariate value is 1, otherwise 0.
\item
  Condition group: Construct covariates for each condition concept id and time interval selected and if a patient has the concept id \textbf{or any descendant concept id} with an era during the specified time interval prior to the cohort start date in the condition era table, the covariate value is 1, otherwise 0.
\item
  Drug: Construct covariates for each drug concept id and time interval selected and if a patient has the concept id with an era during the specified time interval prior to the cohort start date in ths drug era table, the covariate value is 1, otherwise 0.
\item
  Drug group: Construct covariates for each drug concept id and time interval selected and if a patient has the concept id \textbf{or any descendant concept id} with an era during the specified time interval prior to the cohort start date in ths drug era table, the covariate value is 1, otherwise 0.
\end{itemize}

Click on a box with no tick to add a tick and select that covariate into the covariate settings. Clicking on a box with a tick with untick it and remove that covariate from the covaraite settings.

{[}need to check this{]} Overlapping time inverval setting means you want the drug or condition to start prior to the cohort start date and end after the cohort start date (so it overlaps with the cohort start date). The \textbf{era start} option restricts to finding condition or drug eras that start during the time interval selected. These options are at K in Figure \ref{fig:covariate1}.

The domain tables covariates enable you to pick whether to include covariates corresponding to concept ids in each domain for the various time intervals (see L in Figure \ref{fig:covariate2}):

\begin{itemize}
\tightlist
\item
  Condition: Construct covariates for each condition concept id and time interval selected and if a patient has the concept id recorded during the specified time interval prior to the cohort start date in the condition occurrence table, the covariate value is 1, otherwise 0.
\item
  Condition Primary Inpatient: ?
\item
  Drug: Construct covariates for each drug concept id and time interval selected and if a patient has the concept id recorded during the specified time interval prior to the cohort start date in the drug exposure table, the covariate value is 1, otherwise 0.
\item
  Procedure: Construct covariates for each procedure concept id and time interval selected and if a patient has the concept id recorded during the specified time interval prior to the cohort start date in the procedure occurrence table, the covariate value is 1, otherwise 0.
\item
  Measurement: Construct covariates for each measurement concept id and time interval selected and if a patient has the concept id recorded during the specified time interval prior to the cohort start date in the measurement table, the covariate value is 1, otherwise 0.
\item
  Measurement Value: Construct covariates for each measurement concept id with a value and time interval selected and if a patient has the concept id recorded during the specified time interval prior to the cohort start date in the measurement table, the covariate value is the measurement value, otherwise 0.
\item
  Measurement range group: ?
\item
  Observation: Construct covariates for each observation concept id and time interval selected and if a patient has the concept id recorded during the specified time interval prior to the cohort start date in the observation table, the covariate value is 1, otherwise 0.
\item
  Device: Construct covariates for each device concept id and time interval selected and if a patient has the concept id recorded during the specified time interval prior to the cohort start date in the device table, the covariate value is 1, otherwise 0.
\item
  Visit Count: Construct covariates for each visit and time interval selected and count the number of visits recorded during the time interval as the covariate value
\item
  Visit Concept Count: Construct covariates for each visit, domain and time interval selected and count the number of records per domain recorded during the visit type and time interval as the covariate value
\end{itemize}

The distinct count option counds the number of records per domain and time interval {[}expand{]}.

\textbf{Risk score covariates}

The final option is whether to include commonly used risk scores as covariate, M in Figure \ref{fig:covariate2}.

Once happy with the covariate settings, click the `\textless{}' button (see N Figure \ref{fig:covariate1}) on the top left corner to return to the main prediction settings. Your covariate options you picked will now show in the covaraite settings table. You can edit an exisitng setting by clicking on the corresponding row or add more covariate settings by clicking on the blue `+ Add Covariate Settings' button again.

\hypertarget{population-settings}{%
\subsection{Population Settings}\label{population-settings}}

The population settings is where addition inclusion criteria can be applied to the target population (this may be useful for sensitivity investigations) and is also where the time-at-risk is defined. To add a population setting into the study, click on the blue `+ Add Population Settings' button (Q in Figure \ref{fig:specification2}).

This will open up the population setting view containing various setting to define, see Figure \ref{fig:figureAS19}.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/analysis_population_settings} \caption{The population setting options}\label{fig:figureAS19}
\end{figure}

The first set of options, A and B, enable the user to specify the time-at-risk period. This is a time interval relative to the target cohort dates where we look to see whether the outcome of interest occurs. If a patient has the outcome during the time at risk period then we will class them as `outcome', otherwise they are classed as `non-outcome'.

The first option labelled with a red A is: ``Define the time-at-risk window start, relative to target cohort entry:'' - this settings lets you define the start of the time-at-risk. It is relative to the target cohort dates (cohort start date or cohort end date). You can pick an offset corresponding to the number of days and whether it is relative to the target cohort start date or the target cohort end date.

The second option labelled with a red B is: ``Define the time-at-risk window end:'' - this settings lets you define the end of the time-at-risk. It is relative to the target cohort dates (cohort start date or cohort end date). You can pick an offset corresponding to the number of days and whether it is relative to the target cohort start date or the target cohort end date.

See Figure \ref{fig:figureAS20} for an illustration of how these settings define the time-at-risk period:

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/analysis_population_plot} \caption{How the population setting options define the time-at-risk}\label{fig:figureAS20}
\end{figure}

The next option, marked by the red C, is ``Minimum lookback period applied to target cohort:''. This is where you can specify the minimum baseline period, specifically the minimum number of days prior to the cohort start date that a patient has been continuously observed. The default is 365 days. Expanding the minimum lookback will give a more complete picture of a patient (as they must have been observed for longer) but will filter many patienst (who do not have the minimum number of days prior observation).

The option maked by the red D is ``Should subjects without time at risk be removed?''. If this is set to yes, then a value for ``Minimum time at risk:'' is also required. This option lets you deal with people who are lost to follow-up (e.g., they leave the database during the time-at-risk period). If you select `yes' then you need to specify the minimum time a patient needs to be in the time-at-risk period for them to be included in the labelled data (if they do not have the minimum time they are excluded from the population). For example, if the time-at-risk period was 1 day from cohort start until 365 days from cohort start, then the full time-at-risk interval is 364 days (365-1). If you only want to include patients who are observed the whole interval, then set the minimum time at risk to be 364. If you are happy as long as people are in the time-at-risk for the first 100 days, then select minimum time at risk to be 100. In this case as the time-at-risk start as 1 day from the cohort start, a patient will be include if they remain in the database for at least 101 days from the cohort start date. If you set ``Should subjects without time at risk be removed?'' to `No', then this will keep every patient, even those who drop out from the database during the time-at-risk.

The option E ``Include people with outcomes who are not observed for the whole at risk period?'' is also linked to D. This option lets you treat people with the outcome who drop out of the database during time-at-risk differently to those who do not have the outcome observed before dropping out. If ``Include people with outcomes who are not observed for the whole at risk period?'' is set to `No', then people who are not observed for the whole time-at-risk are include/excluded depending on your settings for D. However, if ``Include people with outcomes who are not observed for the whole at risk period?'' is set to `Yes', then this means people who have the outcome recorded during the time-at-risk interval are included in the labelled data even if they drop out from the database before the end of the time-at-risk interval.

The option ``Should only the first exposure per subject be included?'' labelled in F is only useful if you have a target cohort that contains patients multiple times but with different cohort start dates. In this situation, picking `yes' for ``Should only the first exposure per subject be included?'' will result in only keeping the earliest target cohort date per patient in the analysis (i.e., unique patients); otherwise a patient can be in the labbelled dataset multiple times but the covariates and time-at-risk will be at different time points in the patients observation.

The final option G is ``Remove patients who have observed the outcome prior to cohort entry?''. Selecting `Yes' to this option will remove patients who have the outcome prior to the time-at-risk start date, so the model is in patients who have never experience the outcome prior. If `No' is selected, then patients could have had the outcome prior. Generally, having the outcome prior is very predictive of having the outcome during the time-at-risk.

Once you are happy with the population settings, click on the grey `\textless{}' button in the top left and this will return you to the main setting view. You will now see your population settings as a new row in the population settings table. To edit the settings click on the corresponding row. This will take you to the population setting view where you can change any of the settings.

To add more population settings, repeat the process detailed in this section.

\hypertarget{execution-settings}{%
\subsection{Execution settings}\label{execution-settings}}

Execution settings (R in Figure \ref{fig:specification2}) determine whether to use sampling, how to manage rare events, and whether to normalize covariates. Sampling can be an efficient means to determine if a model for a large population (i.e.~10 million patients) is accurate, by creating and testing the model with a subgroup of patients (e.g.~if AUC is close to 0.5 on your sampling, you might abandon the model). The user specifies the size of the subgroup to be sampled. A minimum threshold value for covariate occurrence is necessary to remove rare events that are not representative of the overall population. Normalization of the covariates is usually necessary for successful implementation of a LASSO model.

There are three options:

\begin{itemize}
\tightlist
\item
  ``Perform sampling'': here you can choosen whether to perform sampling (default = `No'). If you set this to `yes', another option will appear ``How many patients to use for a subset?'', here you can add the sample size you wish to extract.
\item
  ``Minimum covariate occurrence: If a covariate occurs in a fraction of the target population less than this value, it will be removed:'': here you can choose then minimum covariate occurrence (default = 0.001)
\item
  ``Normalize covariate'': here you can choose whether to normalize covariates (default = `Yes)
\end{itemize}

\hypertarget{training-settings}{%
\subsection{Training settings}\label{training-settings}}

Training settings (S in Figure \ref{fig:specification2}) determine how to distribute the data between training and testing groups. Most of the data will be used to train the model and the rest will be used to test it. The data can be divided by either unit person or time. The percentage of data attributed to training or testing the model is specified by the user. Additionally, the number of folds for cross-validation is specified, which partitions the training data for hyper-parametric analysis. The user has the option of specifying the seed used to split the training and testing data for consistent distribution of the outcomes between the groups. This option is only needed for person based splitting.

There are four options:

\begin{itemize}
\tightlist
\item
  "Specify how to split the test/train set:: Select whether to differentiate the train/test data by person (stratified by outcome) or by time (older data to train the model, later data to evaluate the model)
\item
  ``Percentage of the data to be used as the test set (0-100\%)'': Select the percentage of data to be used as test data (default = 25\%)
\item
  ``The number of folds used in the cross validation'': Select the number of folds for cross-validation (default = 3)
\item
  ``The seed used to split the test/train set when using a person type testSplit (optional):'': Select the seed used to split the train/test set when using a person type test split
\end{itemize}

\hypertarget{atlas-utilities-tab}{%
\subsection{Atlas Utilities Tab}\label{atlas-utilities-tab}}

The Utilities tab (H in Figure \ref{fig:specification1}) is where a user can review the prediction study (once minimum required settings are defined), export/import existing atlas prediction studies and download the prediction study R package.

\textbf{Review and Download Tab}

If you have not completed all pre-requisites needed to run the study, you will see the same as Figure \ref{fig:insuf}.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/utilities_review_insuf} \caption{Reviewing when insufficient design}\label{fig:insuf}
\end{figure}

Assuming your study contains all necessary components, you will see Figure \ref{fig:utilities}, showing the tabs Full Analysis List, Prediction Problem Settings, and Analysis Settings.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/utilities_p1} \caption{The utilities tab reviewing valid study}\label{fig:utilities}
\end{figure}

Clicking on the Prediction Problem Settings (see Y in Figure \ref{fig:utilities}) will show all the combinations of the Target Cohort and Outcome Cohort names specfied in the analysis.

Finally, clicking on the Analysis Settings tab (see Z in Figure \ref{fig:utilities}) shows a table allowing you to review all of the Model Names, Model Settings, Covariate Settings, Risk Window Start and Risk Window End combinations.

\hypertarget{how-to-importexport-study}{%
\subsection{How to import/export study}\label{how-to-importexport-study}}

To export a study, click on the Export tab under utilities (see V in Figure \ref{fig:utilities}). ATLAS will produce JSON file that can be directly copied and pasted into a file that contains all of the data (study name, cohort definitions, models selected, covariates, settings, etc.) needed to run the study. This is displayed in Figure \ref{fig:export}.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/utilities_export} \caption{Exporting a study design}\label{fig:export}
\end{figure}

To import a study, first go back to the main ATLAS menu and click on Prediction. Click on the New Patient Level Prediction button, give your study a name, and Save. Next, click on the Utilities tab, then the Import tab (see U in Figure \ref{fig:utilities}). Paste the contents of a Patient Level Prediction JSON file into this window, then click on the Import button below the other tab buttons.

\hypertarget{how-to-download-package}{%
\subsection{How to download package}\label{how-to-download-package}}

The Download Study button is available at the bottom of the Utilities screen (see X in Figure \ref{fig:utilities}). Enter a descriptive name for the R package, noting that any illegal characters in R will automatically be removed from the file name by ATLAS.

ATLAS will generate an R package for the study, see Figure \ref{fig:figureU9}.

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/utilities_downloading} \caption{The downloaded study design R package}\label{fig:figureU9}
\end{figure}

\hypertarget{building-atlas-created-prediction-study-r-package}{%
\subsection{Building Atlas created prediction study R package}\label{building-atlas-created-prediction-study-r-package}}

\textbf{Setting up R}

To run the atlas generated prediction R package study requires having R studio () installed, the devtools R package {[}in R run: install.packages(`devtools'){]} and the OHDSI PatientLevelPrediction package installed (see \ldots).

\textbf{Unzipping atlas compressed folder}

Atlas generates a zipped directory containing the R package. This zipped directory needs to be extracted. Once extracted the directory will look like:

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/download_folder} \caption{The directory of study design R package}\label{fig:figureU10}
\end{figure}

\textbf{Opening package project in R}

The easiest way to open the atlas created package in R is to double click on the project file:

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/download_folder_project} \caption{Opening the study design R package}\label{fig:figureU11}
\end{figure}

This will then open a new R studio session:

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/rstudio_start} \caption{Rstudio open with the study design project}\label{fig:figureU12}
\end{figure}

\textbf{Building project}

Once R studio has opened the project, you can then build the package by clicking on the `build' option in the top right hand side:

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/building} \caption{Building the R project into a local R library}\label{fig:figureU13}
\end{figure}

If you find a message like (but with the text in red matching the name you called your study):

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/buildComplete} \caption{Building the R project completed}\label{fig:figureU14}
\end{figure}

Your package has now been created and will be available to run. If you have a message with an error then there was an issue with building the package and the package did not get built. Common issues causing the build to fail are missing dependencies, to find out the R packages required for your built, open the `DESCRIPTION' file in the main directory:

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/download_folder_desc} \caption{Finding the DECSRIPTION file}\label{fig:figureU15}
\end{figure}

This will open up in R studio and show what R packages are required (the packages in the Imports section)

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/description} \caption{The DECSRIPTION file content}\label{fig:figureU16}
\end{figure}

If you do not have any of the packages listed in `Imports:' then you will need to install them before building the atlas generated package.

\hypertarget{running-study}{%
\subsection{Running Study}\label{running-study}}

\textbf{Readme and extras/codetorun.R}

The key file in the atlas generated package directory is the one that contains code for running the study, the CodeToRun.R file found in the extras directory:

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/download_folder_extras} \caption{The CodeToRun.R file is in the extras folder}\label{fig:figureU17}
\end{figure}

We recommend opening the file CodeToRun.R

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/code_to_run} \caption{The CodeToRun.R file}\label{fig:figureU18}
\end{figure}

\textbf{CodeToRun.R Settings}

The final step to running the study is to connect to the database through R and specify where the results should be saved.,

The CodeToRun.R file looks like:

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/code_to_run_open} \caption{The CodeToRun.R default setting}\label{fig:figureU19}
\end{figure}

The inputs for the CodeToRun file are:

\begin{itemize}
\tightlist
\item
  outputFolder: This is a string specifying where in your computer to save the results. This location needs to have sufficient space as data will be extracted from the database into this location and the lcoation must have read/write access.
\item
  options(fftempdir = ''): this is a location in your computer that must have read/write access and large amounts of space. It will be used to store temporary data.
\item
  dbms: The database management system you use
\item
  user: Your username for the database connection (contact database administrator if unknown)
\item
  pw: Your password for the database connection (contact database administrator if unknown)
\item
  server: a string specifying the database server (contact database administrator if unknown)
\item
  port: (optional) the port number (contact database administrator if unknown)
\item
  cdmDatabaseSchema: a string specifying the database schema containing the OMOP CDM instance
\item
  cohortDatabaseSchema: a string specifying the database schema either containing the cohorts or where to create the cohorts.
\item
  oracleTempSchema: if using oracle, this is your temp database schema
\item
  cohortTable: the name of the cohort table (if using atlas cohorts then this will be `cohort')
\end{itemize}

Once the settings are filled out, the final step is to pick what parts of the study to execute:

\begin{figure}
\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/atlasImplementation/execute} \caption{Executing the study}\label{fig:figureU20}
\end{figure}

The following options specify:

\begin{itemize}
\tightlist
\item
  A: createProtocol - set to `True' if you want to create a word document protocol template that automatically inserts the study design settings. This can be shared if creating a network study.
\item
  B: createCohorts - do you need to create the cohorts for this study? If you are using atlas cohorts you can set this to `False' otherwise set this to `True' and the cohorts you picked for the study will all be generated.
\item
  C: runAnalyses - setting this to `True' will result in models being developed and evaluated for each setting you specified in the study design. This requires cohorts to have been generated (in atlas or using B createCohorts set to `True').
\item
  D: createResultsDoc - if you set A: createProtocol to `True' and generated a protocol and also ran the analysis by setting C: runAnalyses to `True' then you can add the results into the protocol to create a word document with the protocol and results.
\item
  E: packageResults - if you set C: runAnalyses to `True' and have results, you can set packageResults to `True' to create a zipped folder containing your results with any sensitive data removed. This can be easily shared with other OHDSI colabortors.
\item
  F: createValidationPackage - if a model seems to do, we can use this option to create a new R package for validating the model. Set to `True' to create a validation package containing all the models for external validation. In later Atlas versions there is another input where you can specify the analysis id of a model rather than validating all models.
\item
  G: minCellCount - this is linked to E: packageResults and F: createValidationPackage and specifies the minimum cell count for any result to be included when sharing the models. For example, if the minCellCount is 5, then any count with a value less than 5 will be removed.
\end{itemize}

\textbf{Viewing the Results}

After running the R package analysis you can view the results in an interactive shiny app by running:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PatientLevelPrediction}\OperatorTok{::}\KeywordTok{viewMultiplePlp}\NormalTok{(outputFolder)}
\end{Highlighting}
\end{Shaded}

\newpage

\hypertarget{implementing-the-study-in-r}{%
\section{Implementing the study in R}\label{implementing-the-study-in-r}}

Now we have completely designed our study we have to implement the study in R. This will be done using the \href{https://ohdsi.github.io/PatientLevelPrediction/}{PatientLevelPrediction} package to build patient-level predictive models. The package enables data extraction, model building, and model evaluation using data from databases that are translated into the OMOP CDM.

\hypertarget{cohort-instantiation-1}{%
\subsection{Cohort instantiation}\label{cohort-instantiation-1}}

We first need to instantiate the target and outcome cohorts. Instantiating cohorts is described in Chapter \ref{Cohorts}. The Appendix provides the full definitions of the target (Appendix \ref{AceInhibitors}) and outcome (Appendix \ref{Angioedema}) cohorts. In this example we will assume the ACE inhibitors cohort has ID 1, and the angioedema cohort has ID 2.

\hypertarget{data-extraction-1}{%
\subsection{Data extraction}\label{data-extraction-1}}

We first need to tell R how to connect to the server. \href{https://ohdsi.github.io/PatientLevelPrediction/}{\texttt{PatientLevelPrediction}} uses the \href{https://ohdsi.github.io/DatabaseConnector/}{\texttt{DatabaseConnector}} package, which provides a function called \texttt{createConnectionDetails}. Type \texttt{?createConnectionDetails} for the specific settings required for the various database management systems (DBMS). For example, one might connect to a PostgreSQL database using this code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(PatientLevelPrediction)}
\NormalTok{connDetails <-}\StringTok{ }\KeywordTok{createConnectionDetails}\NormalTok{(}\DataTypeTok{dbms =} \StringTok{"postgresql"}\NormalTok{,}
                                       \DataTypeTok{server =} \StringTok{"localhost/ohdsi"}\NormalTok{,}
                                       \DataTypeTok{user =} \StringTok{"joe"}\NormalTok{,}
                                       \DataTypeTok{password =} \StringTok{"supersecret"}\NormalTok{)}

\NormalTok{cdmDbSchema <-}\StringTok{ "my_cdm_data"}
\NormalTok{cohortsDbSchema <-}\StringTok{ "scratch"}
\NormalTok{cohortsDbTable <-}\StringTok{ "my_cohorts"}
\NormalTok{cdmVersion <-}\StringTok{ "5"}
\end{Highlighting}
\end{Shaded}

The last four lines define the \texttt{cdmDbSchema}, \texttt{cohortsDbSchema}, and \texttt{cohortsDbTable} variables, as well as the CDM version. We will use these later to tell R where the data in CDM format live, where the cohorts of interest have been created, and what version CDM is used. Note that for Microsoft SQL Server, database schemas need to specify both the database and the schema, so for example \texttt{cdmDbSchema\ \textless{}-\ "my\_cdm\_data.dbo"}.

First it makes sense to verify that the cohort creation has succeeded, by counting the number of cohort entries:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sql <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"SELECT cohort_definition_id, COUNT(*) AS count"}\NormalTok{,}
\StringTok{"FROM @cohortsDbSchema.cohortsDbTable"}\NormalTok{,}
\StringTok{"GROUP BY cohort_definition_id"}\NormalTok{)}
\NormalTok{conn <-}\StringTok{ }\KeywordTok{connect}\NormalTok{(connDetails)}
\KeywordTok{renderTranslateQuerySql}\NormalTok{(}\DataTypeTok{connection =}\NormalTok{ conn, }
                        \DataTypeTok{sql =}\NormalTok{ sql,}
                        \DataTypeTok{cohortsDbSchema =}\NormalTok{ cohortsDbSchema,}
                        \DataTypeTok{cohortsDbTable =}\NormalTok{ cohortsDbTable)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   cohort_definition_id  count
## 1                    1 527616
## 2                    2   3201
\end{verbatim}

Now we can tell \href{https://ohdsi.github.io/PatientLevelPrediction/}{PatientLevelPrediction} to extract all necessary data for our analysis. Covariates are extracted using the \href{https://ohdsi.github.io/FeatureExtraction/}{\texttt{FeatureExtraction}} package. For more detailed information on the FeatureExtraction package see its vignettes. For our example study we decided to use these settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{covSettings <-}\StringTok{ }\KeywordTok{createCovariateSettings}\NormalTok{(}\DataTypeTok{useDemographicsGender =} \OtherTok{TRUE}\NormalTok{,}
                                       \DataTypeTok{useDemographicsAge =} \OtherTok{TRUE}\NormalTok{,}
                                       \DataTypeTok{useConditionGroupEraLongTerm =} \OtherTok{TRUE}\NormalTok{,}
                                       \DataTypeTok{useConditionGroupEraAnyTimePrior =} \OtherTok{TRUE}\NormalTok{,}
                                       \DataTypeTok{useDrugGroupEraLongTerm =} \OtherTok{TRUE}\NormalTok{,}
                                       \DataTypeTok{useDrugGroupEraAnyTimePrior =} \OtherTok{TRUE}\NormalTok{,}
                                       \DataTypeTok{useVisitConceptCountLongTerm =} \OtherTok{TRUE}\NormalTok{,}
                                       \DataTypeTok{longTermStartDays =} \DecValTok{-365}\NormalTok{,}
                                       \DataTypeTok{endDays =} \DecValTok{-1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The final step for extracting the data is to run the \texttt{getPlpData} function and input the connection details, the database schema where the cohorts are stored, the cohort definition ids for the cohort and outcome, and the washoutPeriod which is the minimum number of days prior to cohort index date that the person must have been observed to be included into the data, and finally input the previously constructed covariate settings.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plpData <-}\StringTok{ }\KeywordTok{getPlpData}\NormalTok{(}\DataTypeTok{connectionDetails =}\NormalTok{ connDetails,}
                      \DataTypeTok{cdmDatabaseSchema =}\NormalTok{ cdmDbSchema,}
                      \DataTypeTok{cohortDatabaseSchema =}\NormalTok{ cohortsDbSchema,}
                      \DataTypeTok{cohortTable =}\NormalTok{ cohortsDbSchema,}
                      \DataTypeTok{cohortId =} \DecValTok{1}\NormalTok{,}
                      \DataTypeTok{covariateSettings =}\NormalTok{ covariateSettings,}
                      \DataTypeTok{outcomeDatabaseSchema =}\NormalTok{ cohortsDbSchema,}
                      \DataTypeTok{outcomeTable =}\NormalTok{ cohortsDbSchema,}
                      \DataTypeTok{outcomeIds =} \DecValTok{2}\NormalTok{,}
                      \DataTypeTok{sampleSize =} \DecValTok{10000}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There are many additional parameters for the \texttt{getPlpData} function which are all documented in the \href{https://ohdsi.github.io/PatientLevelPrediction/}{PatientLevelPrediction} manual. The resulting \texttt{plpData} object uses the package \texttt{ff} to store information in a way that ensures R does not run out of memory, even when the data are large.

Creating the \texttt{plpData} object can take considerable computing time, and it is probably a good idea to save it for future sessions. Because \texttt{plpData} uses \texttt{ff}, we cannot use R's regular save function. Instead, we'll have to use the \texttt{savePlpData()} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{savePlpData}\NormalTok{(plpData, }\StringTok{"angio_in_ace_data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can use the \texttt{loadPlpData()} function to load the data in a future session.

\hypertarget{additional-inclusion-criteria}{%
\subsection{Additional inclusion criteria}\label{additional-inclusion-criteria}}

To completely define the prediction problem the final study population is obtained by applying additional constraints on the two earlier defined cohorts, e.g., a minumim time at risk can be enforced (\texttt{requireTimeAtRisk,\ minTimeAtRisk}) and we can specify if this also applies to patients with the outcome (\texttt{includeAllOutcomes}). Here we also specify the start and end of the risk window relative to target cohort start. For example, if we like the risk window to start 30 days after the at-risk cohort start and end a year later we can set \texttt{riskWindowStart\ =\ 30} and \texttt{riskWindowEnd\ =\ 365}. In some cases the risk window needs to start at the cohort end date. This can be achieved by setting \texttt{addExposureToStart\ =\ TRUE} which adds the cohort (exposure) time to the start date.

In the example below all the settings we defined for our study are imposed:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population <-}\StringTok{ }\KeywordTok{createStudyPopulation}\NormalTok{(}\DataTypeTok{plpData =}\NormalTok{ plpData,}
                                    \DataTypeTok{outcomeId =} \DecValTok{2}\NormalTok{,}
                                    \DataTypeTok{washoutPeriod =} \DecValTok{364}\NormalTok{,}
                                    \DataTypeTok{firstExposureOnly =} \OtherTok{FALSE}\NormalTok{,}
                                    \DataTypeTok{removeSubjectsWithPriorOutcome =} \OtherTok{TRUE}\NormalTok{,}
                                    \DataTypeTok{priorOutcomeLookback =} \DecValTok{9999}\NormalTok{,}
                                    \DataTypeTok{riskWindowStart =} \DecValTok{1}\NormalTok{,}
                                    \DataTypeTok{riskWindowEnd =} \DecValTok{365}\NormalTok{,}
                                    \DataTypeTok{addExposureDaysToStart =} \OtherTok{FALSE}\NormalTok{,}
                                    \DataTypeTok{addExposureDaysToEnd =} \OtherTok{FALSE}\NormalTok{,}
                                    \DataTypeTok{minTimeAtRisk =} \DecValTok{364}\NormalTok{,}
                                    \DataTypeTok{requireTimeAtRisk =} \OtherTok{TRUE}\NormalTok{,}
                                    \DataTypeTok{includeAllOutcomes =} \OtherTok{TRUE}\NormalTok{,}
                                    \DataTypeTok{verbosity =} \StringTok{"DEBUG"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-development}{%
\subsection{Model Development}\label{model-development}}

In the set function of an algorithm the user can specify a list of eligible values for each hyper-parameter. All possible combinations of the hyper-parameters are included in a so-called grid search using cross-validation on the training set. If a user does not specify any value then the default value is used instead.

For example, if we use the following settings for the gradientBoostingMachine: ntrees=c(100,200), maxDepth=4 the grid search will apply the gradient boosting machine algorithm with ntrees=100 and maxDepth=4 plus the default settings for other hyper-parameters and ntrees=200 and maxDepth=4 plus the default settings for other hyper-parameters. The hyper-parameters that lead to the bestcross-validation performance will then be chosen for the final model. For our problem we choose to build a logistic regression model with the default hyper-parameters

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gbmModel <-}\StringTok{ }\KeywordTok{setGradientBoostingMachine}\NormalTok{(}\DataTypeTok{ntrees =} \DecValTok{5000}\NormalTok{, }\DataTypeTok{maxDepth =} \KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{10}\NormalTok{), }
    \DataTypeTok{learnRate =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.001}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.9}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The \texttt{runPlP} function uses the population, \texttt{plpData}, and model settings to train and evaluate the model. We can use the testSplit (person/time) and testFraction parameters to split the data in a 75\%-25\% split and run the patient-level prediction pipeline:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gbmResults <-}\StringTok{ }\KeywordTok{runPlp}\NormalTok{(}\DataTypeTok{population =}\NormalTok{ population, }
                     \DataTypeTok{plpData =}\NormalTok{ plpData, }
                     \DataTypeTok{modelSettings =}\NormalTok{ gbmModel, }
                     \DataTypeTok{testSplit =} \StringTok{'person'}\NormalTok{,}
                     \DataTypeTok{testFraction =} \FloatTok{0.25}\NormalTok{, }
                     \DataTypeTok{nfold =} \DecValTok{2}\NormalTok{, }
                     \DataTypeTok{splitSeed =} \DecValTok{1234}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Under the hood the package will now use the R xgboost package to fit a a gradient boosting machine model using 75\% of the data and will evaluate the model on the remaining 25\%. A results data structure is returned containing information about the model, its performance etc.

In the \texttt{runPlp} function there are several parameters to save the \texttt{plpData}, \texttt{plpResults}, \texttt{plpPlots}, \texttt{evaluation}, etc. objects which are all set to \texttt{TRUE} by default.

You can save the model using:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{savePlpModel}\NormalTok{(gbmResults}\OperatorTok{$}\NormalTok{model, }\DataTypeTok{dirPath =} \StringTok{"model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can load the model using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plpModel <-}\StringTok{ }\KeywordTok{loadPlpModel}\NormalTok{(}\StringTok{"model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can also save the full results structure using:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{savePlpResult}\NormalTok{(gbmResults, }\DataTypeTok{location =} \StringTok{"gbmResults"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To load the full results structure use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gbmResults <-}\StringTok{ }\KeywordTok{loadPlpResult}\NormalTok{(}\StringTok{"gbmResults"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{internal-validation}{%
\subsection{Internal Validation}\label{internal-validation}}

Once we execute the study, the \texttt{runPlp} function returns the trained model and the evaluation of the model on the train/test sets. You can interactively view the results by running: \texttt{viewPlp(runPlp\ =\ gbmResults)}. This will open a Shiny App in your browser in which you can view all performance measures created by the framework, including interactive plots, as shown in Figure \ref{fig:shinysummary}.

\includegraphics[width=1\linewidth]{images/PatientLevelPrediction/shinysummary}

To generate and save all the evaluation plots to a folder run the following code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plotPlp}\NormalTok{(gbmResults, }\StringTok{"plots"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The plots are described in more detail \protect\hyperlink{performance}{here}

\hypertarget{external-validation}{%
\paragraph{External validation}\label{external-validation}}

We recommend to always perform external validation, i.e.~apply the final model on as much new datasets as feasible and evaluate its performance. Here we assume the data extraction has already been peformed on a second database and stored in the \texttt{newData} folder. We load the model we previously fitted from the \texttt{model} folder:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the trained model}
\NormalTok{plpModel <-}\StringTok{ }\KeywordTok{loadPlpModel}\NormalTok{(}\StringTok{"model"}\NormalTok{)}

\CommentTok{#load the new plpData and create the population}
\NormalTok{plpData <-}\StringTok{ }\KeywordTok{loadPlpData}\NormalTok{(}\StringTok{"newData"}\NormalTok{)}

\NormalTok{population <-}\StringTok{ }\KeywordTok{createStudyPopulation}\NormalTok{(}\DataTypeTok{plpData =}\NormalTok{ plpData,}
                                    \DataTypeTok{outcomeId =} \DecValTok{2}\NormalTok{,}
                                    \DataTypeTok{washoutPeriod =} \DecValTok{364}\NormalTok{,}
                                    \DataTypeTok{firstExposureOnly =} \OtherTok{FALSE}\NormalTok{,}
                                    \DataTypeTok{removeSubjectsWithPriorOutcome =} \OtherTok{TRUE}\NormalTok{,}
                                    \DataTypeTok{priorOutcomeLookback =} \DecValTok{9999}\NormalTok{,}
                                    \DataTypeTok{riskWindowStart =} \DecValTok{1}\NormalTok{,}
                                    \DataTypeTok{riskWindowEnd =} \DecValTok{365}\NormalTok{,}
                                    \DataTypeTok{addExposureDaysToStart =} \OtherTok{FALSE}\NormalTok{,}
                                    \DataTypeTok{addExposureDaysToEnd =} \OtherTok{FALSE}\NormalTok{,}
                                    \DataTypeTok{minTimeAtRisk =} \DecValTok{364}\NormalTok{,}
                                    \DataTypeTok{requireTimeAtRisk =} \OtherTok{TRUE}\NormalTok{,}
                                    \DataTypeTok{includeAllOutcomes =} \OtherTok{TRUE}
\NormalTok{)}

\CommentTok{# apply the trained model on the new data}
\NormalTok{validationResults <-}\StringTok{ }\KeywordTok{applyModel}\NormalTok{(population, plpData, plpModel)}
\end{Highlighting}
\end{Shaded}

To make things easier we also provide the \texttt{externalValidatePlp} function for performing external validation that also extracts the required data. Assuming you ran \texttt{result\ \textless{}-\ runPlp(...)} then you can extract the data required for the model and evaluated it on new data. Assuming the validation cohorts are in the table \texttt{mainschema.dob.cohort} with ids 1 and 2 and the cdm data is in the schema \texttt{cdmschema.dob}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{valResult <-}\StringTok{ }\KeywordTok{externalValidatePlp}\NormalTok{(}\DataTypeTok{plpResult =}\NormalTok{ result, }\DataTypeTok{connectionDetails =}\NormalTok{ connectionDetails, }
    \DataTypeTok{validationSchemaTarget =} \StringTok{"mainschema.dob"}\NormalTok{, }\DataTypeTok{validationSchemaOutcome =} \StringTok{"mainschema.dob"}\NormalTok{, }
    \DataTypeTok{validationSchemaCdm =} \StringTok{"cdmschema.dbo"}\NormalTok{, }\DataTypeTok{databaseNames =} \StringTok{"new database"}\NormalTok{, }\DataTypeTok{validationTableTarget =} \StringTok{"cohort"}\NormalTok{, }
    \DataTypeTok{validationTableOutcome =} \StringTok{"cohort"}\NormalTok{, }\DataTypeTok{validationIdTarget =} \DecValTok{1}\NormalTok{, }\DataTypeTok{validationIdOutcome =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If you have multiple databases to validate the model on then you can run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{valResults <-}\StringTok{ }\KeywordTok{externalValidatePlp}\NormalTok{(}\DataTypeTok{plpResult =}\NormalTok{ result, }\DataTypeTok{connectionDetails =}\NormalTok{ connectionDetails, }
    \DataTypeTok{validationSchemaTarget =} \KeywordTok{list}\NormalTok{(}\StringTok{"mainschema.dob"}\NormalTok{, }\StringTok{"difschema.dob"}\NormalTok{, }\StringTok{"anotherschema.dob"}\NormalTok{), }
    \DataTypeTok{validationSchemaOutcome =} \KeywordTok{list}\NormalTok{(}\StringTok{"mainschema.dob"}\NormalTok{, }\StringTok{"difschema.dob"}\NormalTok{, }\StringTok{"anotherschema.dob"}\NormalTok{), }
    \DataTypeTok{validationSchemaCdm =} \KeywordTok{list}\NormalTok{(}\StringTok{"cdms1chema.dbo"}\NormalTok{, }\StringTok{"cdm2schema.dbo"}\NormalTok{, }\StringTok{"cdm3schema.dbo"}\NormalTok{), }
    \DataTypeTok{databaseNames =} \KeywordTok{list}\NormalTok{(}\StringTok{"new database 1"}\NormalTok{, }\StringTok{"new database 2"}\NormalTok{, }\StringTok{"new database 3"}\NormalTok{), }
    \DataTypeTok{validationTableTarget =} \KeywordTok{list}\NormalTok{(}\StringTok{"cohort1"}\NormalTok{, }\StringTok{"cohort2"}\NormalTok{, }\StringTok{"cohort3"}\NormalTok{), }\DataTypeTok{validationTableOutcome =} \KeywordTok{list}\NormalTok{(}\StringTok{"cohort1"}\NormalTok{, }
        \StringTok{"cohort2"}\NormalTok{, }\StringTok{"cohort3"}\NormalTok{), }\DataTypeTok{validationIdTarget =} \KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{), }\DataTypeTok{validationIdOutcome =} \KeywordTok{list}\NormalTok{(}\DecValTok{2}\NormalTok{, }
        \DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\newpage

\hypertarget{exploring-a-single-plp-shiny-app}{%
\section{Exploring a single PLP Shiny App}\label{exploring-a-single-plp-shiny-app}}

Exploring the performance of a single plp model is easiest with the \texttt{viewPlp()} function. This requires a plpResult as the input. If developing models in R you can use the result of \texttt{plpResult\ \textless{}-\ runPLp(...)} as the input. If using the Atlas generated study package, then you need to load one of the models (in this example we will load Analysis\_1):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plpResult <-}\StringTok{ }\KeywordTok{loadPlpResult}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(outputFolder, }\StringTok{"Analysis_1"}\NormalTok{, }\StringTok{"plpResult"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Analysis\_1 corresponds to the lasso logistic regression for the prediction problem: \textbf{Withing new users of ACE inhibitors as first-line monotherapy for hypertension who will develop Acute myocardial infarction (AMI) events within a year.}

You can then launch the shiny app by running:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{viewPlp}\NormalTok{(plpResult)}
\end{Highlighting}
\end{Shaded}

The shiny launches with a summary of the performance metrics on the test and train sets, see Figure \ref{fig:shinySum}. The results show that the AUROC on the train set was 0.78 and this dropped to 0.74 on the test set. The test set AUC is the more accurrate measure. Overal, the model appears to be able to discriminate those who will develop AMI in new users of ACE inhibitors but it slightly over fit as the performance on the train set is higher than the test set. The ROC plot is presented in Figure \ref{fig:shinyROC}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/singleShiny/singleShinySummary} 

}

\caption{The starting summary page of the viewPlp() shiny app}\label{fig:shinySum}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/singleShiny/singleShinyRoc} 

}

\caption{The ROC plot for predicting AMI within a year in new users of ACE inhibitors}\label{fig:shinyROC}
\end{figure}

The calibration plot in Figure \ref{fig:shinyCal} shows that generally the observed risk matches the predicted risk as the dots are around the diagonal line. The demographic calibration plot in Figure \ref{fig:shinyDemo} however shows that the model is not well calibrated for the younger patients, as the blue line (the predicted risk) differs from the red line (the observed risk) for those aged below 40. This may indicate we need to remove the under 40s from the target population (as the observed risk for the younger patients is nearly zero).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/singleShiny/singleShinyCal} 

}

\caption{The calibration of the model}\label{fig:shinyCal}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/singleShiny/singleShinyDemo} 

}

\caption{The demographic calibration of the model}\label{fig:shinyDemo}
\end{figure}

Finally, the attrition plot shows the loss of patients from the labelled data based on inclusion/exclusion criteria, see Figure \ref{fig:shinyAtt}. The plot shows that we lost a large portion of the target population due to them not being observed for the whole time at risk (1 year follow up). Interestingly, not as many patients with the outcome lacked the complete time at risk.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/singleShiny/singleShinyAtt} 

}

\caption{The attrition plot for the prediction problem}\label{fig:shinyAtt}
\end{figure}

\hypertarget{exploring-the-atlas-plp-shiny-app}{%
\section{Exploring the Atlas PLP Shiny App}\label{exploring-the-atlas-plp-shiny-app}}

To view the atlas generated analysis results via an interactive shiny app, run:
\texttt{PatientLevelPrediction::viewMultiplePlp(outputFolder)} where \emph{outputFolder} is the directory path containing the analysis results (e.g., `C:/atlasResults/Example'), it will look like:

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinyResults} 

}

\caption{The directory where the atlas models and results were saved}\label{fig:shinyResults}
\end{figure}

The interactive shiny app will start at the summary page:

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinySummary} 

}

\caption{The shiny summary page containing key hold out set performance metrics for each model trained}\label{fig:shinySummary}
\end{figure}

This summary page table contains:

\begin{itemize}
\tightlist
\item
  basic information about the model (e.g., database information, classifier type, time at risk settings, target population and outcome names)
\item
  hold out target population count and incidence of outcome
\item
  discrimination metrics: AUC, AUPRC
\end{itemize}

To the left of the table is the filter option:

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinyFilter} 

}

\caption{Demonstration of the filter option}\label{fig:shinyFilter}
\end{figure}

Here a user can specify the development/valdiation databases to focus on, the type of model, the time at risk settings of interest and/or the cohorts of interest. For example, to pick the models corresponding to the target population ``New users of ACE inhibitors as first line monotherapy for hypertension'', select this in the \emph{Target Cohort} option.

To explore a model click on the corresponding row, a selected row will be highlighted. To unselect simply click on the selected row again or select a new row.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinySelect} 

}

\caption{The highlighted row shows a selected model.  We can then use other tab to explore the settings and results for the selected model}\label{fig:shinySelect}
\end{figure}

With a row selected, you can now explore the model settings used when developing the model by clicking on the \emph{Model Settings} tab:

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinyModel} 

}

\caption{To view the model settings used when developing the model.}\label{fig:shinyModel}
\end{figure}

To explore the population settings, click on the \emph{Population Settings} tab to display the settings used when developing the model:

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinyPopSet} 

}

\caption{To view the model settings used when developing the model.}\label{fig:shinyPopSet}
\end{figure}

Simialrly, to explore the covariates settings, click on the \emph{Covariate Settings} tab to display which covariates were used as candidate covariates in the model:

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinyCovSet} 

}

\caption{To view the covariate settings used when developing the model.}\label{fig:shinyCovSet}
\end{figure}

The row selection also works for displaying the model performance. To view the performance you need to select `Performance' from the left menu:

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinyBar} 

}

\caption{The shiny option bar for navigating around the interface.}\label{fig:shinyBar}
\end{figure}

By clicking the `Performance' option from the menu you will be taken to a threshold performance summary:

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinyPerformanceSum} 

}

\caption{The summary performance measures at a set threshold.}\label{fig:shinyPerformanceSum}
\end{figure}

This summary view shows the selected prediction question in the standard format, a threshold selector and a dashboard containing key threshold based metrics such as positive predictive value (PPV), negative predictive value (NPV), sensitivity and specificity. See Section \ldots{} for more details about these measurements. In Figure \ref{fig:shinyPerformanceSum} we see the selected prediction model is: ``within new users of ACE inhibitors as first line monotherapy for hypertension predict who will developed acute myocardial infarction during 1 day after cohort start and 365 days after cohort start''. At a threshold of 0.00482 the sensitivity is 83.4\% (83.4\% of patients with the acute MI in the following year have a risk greater than or equal to 0.00482) and the PPV is 1.2\% (1.2\% of patients with a risk greater than or equal to 0.00482 have an acute MI in the following year). As the incidence of the acute MI within the year is 0.741\%, identifying patients with a risk greater than or equal to 0.00482 would find a subgroup of patients that have nearly double the risk of the population average risk.

You can adjust the threshold by moving the dot in the \emph{Input} box:

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinyPerformanceThres} 

}

\caption{Moving this changes the threshold and the values in the Dashboard will update.}\label{fig:shinyPerformanceThres}
\end{figure}

To look at the overal discrimination ability of the model click on the `Discrimination' tab, this then takes you to a view with the ROC plot, PR plot, and distribution plots (the line on the plots corresponds to the selected threshold point):

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinyPerformanceDisc} 

}

\caption{The ROC and PR plots used to access the overal discrimination ability of the model.}\label{fig:shinyPerformanceDisc}
\end{figure}

We see in Figure \ref{fig:shinyPerformanceDisc} that the ROC plot shows the model was able to discriminate between those who will have the acute MI within the year and those who will not. However, the performance looks less impressive when we see the PR plot, as the low incidence of the acute MI means there is a high false positive rate.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinyPerformanceDist} 

}

\caption{The predicted risk distribtion for those with and without the outcome.  The more these overlap the worse the discrimination}\label{fig:shinyPerformanceDist}
\end{figure}

Finally, you can also inspect the calibration of the model by clicking on the `Calibration' tab. This displays the calibration plot and the demographic calibration:

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinyPerformanceCal} 

}

\caption{The risk stratified calibration and demographic calibration}\label{fig:shinyPerformanceCal}
\end{figure}

Figure \ref{fig:shinyPerformanceCal} shows the average predicted risk appears to match the observed fraction who experienced the acute MI within a year, so the model is well calibrated. Interestingly, the demographic calibration shows that the blue line is higher than the red line for young patients, so we are predicting a higher risk for young age groups. Conversely, for the patients above 80 the model is predicting a lower risk than the observed risk. This may prompt us to develop seperate models for the younger or older patients.

To inspect the final model, select the ``Model'' option from the left hand menu. This will open a view containing plots for each variable in the model and a table summarising all the candidate covariates. The variable plots are seperated into binary variables and continuous variables. The x-axis is the prevalance/mean in patients without the outcome and the y-axis is the prevalance/mean in patients with the outcome. Therefore, any variable's dot falling above the diagonal is more common in patients with the outcome and any variable's dot falling below the diagonal is less common in patients with the outcome:

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinyModelPlots} 

}

\caption{Each dot corresponds to a varible included in the model.}\label{fig:shinyModelPlots}
\end{figure}

The table below displays the Name, Value (coefficient if using a glm or variable importance otherwise) all the candidate covariates, Outcome mean (the mean value for those who have the outcome) and non-outcome mean (the mean value for those who do not have the outcome):

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinyModelTable} 

}

\caption{Each dot corresponds to a varible included in the model.}\label{fig:shinyModelTable}
\end{figure}

You can click on the columns headers to order by the chosen column. For example, to order by Value, click on the `Value' heading.

The shiny interface also enables you to view the model development and evaluation log file. Click on `Log' in the left hand option bar:

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinyLog} 

}

\caption{Example log display.}\label{fig:shinyLog}
\end{figure}

Finally, for instructions on accessing a youtube video demonstrating how to use the interactive shiny result viewer click on ``Help'' in the left hand option bar:

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/PatientLevelPrediction/shiny/shinyHelp} 

}

\caption{Instructions for viewing a demo video.}\label{fig:shinyHelp}
\end{figure}

\newpage

\hypertarget{additional-patient-level-prediction-features}{%
\section{Additional Patient-level Prediction Features}\label{additional-patient-level-prediction-features}}

\hypertarget{journal-paper-generation}{%
\subsection{Journal paper generation}\label{journal-paper-generation}}

We have added functionality to automatically generate a word document you can use as start of a journal paper. It contains many of the generated study details and results. If you have performed external validation these results will can be added as well. Optionally, you can add a ``Table 1'' that contains data on many covariates for the target population. You can create the draft journal paper by running this function:

\begin{Shaded}
\begin{Highlighting}[]
 \KeywordTok{createPlpJournalDocument}\NormalTok{(}\DataTypeTok{plpResult =} \OperatorTok{<}\NormalTok{your plp results}\OperatorTok{>}\NormalTok{,}
                          \DataTypeTok{plpValidation =} \OperatorTok{<}\NormalTok{your validation results}\OperatorTok{>}\NormalTok{,}
                          \DataTypeTok{plpData =} \OperatorTok{<}\NormalTok{your plp data}\OperatorTok{>}\NormalTok{,}
                          \DataTypeTok{targetName =} \StringTok{"<target population>"}\NormalTok{,}
                          \DataTypeTok{outcomeName =} \StringTok{"<outcome>"}\NormalTok{,}
                          \DataTypeTok{table1 =}\NormalTok{ F,}
                          \DataTypeTok{connectionDetails =} \OtherTok{NULL}\NormalTok{,}
                          \DataTypeTok{includeTrain =} \OtherTok{FALSE}\NormalTok{,}
                          \DataTypeTok{includeTest =} \OtherTok{TRUE}\NormalTok{,}
                          \DataTypeTok{includePredictionPicture =} \OtherTok{TRUE}\NormalTok{,}
                          \DataTypeTok{includeAttritionPlot =} \OtherTok{TRUE}\NormalTok{,}
                          \DataTypeTok{outputLocation =} \StringTok{"<your location>"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For more details see the help page of the function.

\hypertarget{excercises-1}{%
\section{Excercises}\label{excercises-1}}

\hypertarget{part-evidence-quality}{%
\part{Evidence Quality}\label{part-evidence-quality}}

\hypertarget{EvidenceQuality}{%
\chapter{Evidence Quality}\label{EvidenceQuality}}

\emph{Chapter lead: Jon Duke}

\hypertarget{understanding-evidence-quality}{%
\section{Understanding Evidence Quality}\label{understanding-evidence-quality}}

How do we know if the results of a study are reliable? Can they be trusted for use in clinical settings? What about in regulatory decision-making? Can they serve as a foundation for future research? Each time a new study is published or disseminated, readers must consider these questions, regardless of whether the work was a randomized controlled trial, an observational study, or other type of analysis.

One of the concerns that is often raised around observational studies and the use of ``real world data'' is the topic of data quality \citep{botsis2010secondary, hersh2013caveats, sherman2016real}. Commonly noted is that data used in observational research were not originally gathered for research purposes and thus may suffer from incomplete or inaccurate data capture as well inherent biases. These concerns have given rise to a growing body of research around how to measure, characterize, and ideally improve data quality \citep{kahn2012pragmatic, liaw2013towards, weiskopf2013methods}. The OHDSI community is a strong advocate of such research and community members have led and participated in many studies looking at data quality in the OMOP CDM and the OHDSI network \citep{huser_multisite_2016, kahn_transparent_2015, callahan2017comparison, yoon_2016}.

Given the findings of the past decade in this area, it has become apparent that data quality is not perfect and never will be. This notion is nicely reflected in this quote from Dr Clem McDonald, a pioneer in the field of medical informatics:

\begin{quote}
Loss of fidelity begins with the movement of data from the doctor's brain to the medical record.
\end{quote}

Thus, as a community we must ask the question-- \emph{given imperfect data, how can we achieve the most reliable evidence?} The OHDSI community is seeking to address this question through a holistic focus on ``evidence quality''. Evidence quality considers not only the quality of observational data but also the validity of the methods, software, and clinical definitions used in our observational analyses.

In the following chapters, we will explore four components of evidence quality:

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.19\columnwidth}\raggedright
Component of Evidence Quality\strut
\end{minipage} & \begin{minipage}[b]{0.75\columnwidth}\raggedright
What it Measures\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.19\columnwidth}\raggedright
\href{DataQuality.html}{Data Quality}\strut
\end{minipage} & \begin{minipage}[t]{0.75\columnwidth}\raggedright
Are the data completely captured with plausible values in a manner that is conformant to agreed structure and conventions?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright
\href{ClinicalValidity.html}{Clinical Validity}\strut
\end{minipage} & \begin{minipage}[t]{0.75\columnwidth}\raggedright
To what extent does the analysis conducted match the clinical intention?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright
\href{SoftwareValidity.html}{Software Validity}\strut
\end{minipage} & \begin{minipage}[t]{0.75\columnwidth}\raggedright
Can we trust that the process transforming and analyzing the data does what it is supposed to do?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright
\href{MethodValidity.html}{Method Validity}\strut
\end{minipage} & \begin{minipage}[t]{0.75\columnwidth}\raggedright
Is the methodology appropriate for the question, given the strengths and weaknesses of the data?\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{communicating-evidence-quality}{%
\section{Communicating Evidence Quality}\label{communicating-evidence-quality}}

An important aspect of evidence quality is the ability to express the uncertainty that comes from the data being imperfect. Thus, our efforts around evidence quality include not only concepts but also specific tools and community processes. The overarching goal of OHDSI's work around evidence quality is to produce confidence in health care decision-makers that the evidence generated by OHDSI-- while undoubtedly imperfect in many ways-- has been consistently measured for its weaknesses and strengths and that this information has been communicated in a rigorous and open manner.

\hypertarget{DataQuality}{%
\chapter{Data Quality}\label{DataQuality}}

\hypertarget{introduction-3}{%
\section{Introduction}\label{introduction-3}}

Kahn et al.~define data quality as consisting of three components: (1) conformance (do data values adhere to do specified standard and formats?; subtypes: value, relational and computational conformance); (2) completeness (are data values present?); and (3) plausibility (are data values believable?; subtypes uniqueness, atemporal; temporal) \citep{kahn_harmonized_2016}

Kahn additionaly defines two contexts: verification and validation. Verification focuses on model and data constraints and does not rely on external reference. Validation focuses on data expectations that are derived from comparison to a relative gold standard and uses external knowledge.

Table below shows examples of the above defined data quality (DQ) constructs.

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[b]{0.08\columnwidth}\raggedright
Term\strut
\end{minipage} & \begin{minipage}[b]{0.16\columnwidth}\raggedright
Subtype\strut
\end{minipage} & \begin{minipage}[b]{0.67\columnwidth}\raggedright
Validation example\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.08\columnwidth}\raggedright
Conformance\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedright
Value\strut
\end{minipage} & \begin{minipage}[t]{0.67\columnwidth}\raggedright
Providers are only assigned valid medical specialties.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedright
Relational\strut
\end{minipage} & \begin{minipage}[t]{0.67\columnwidth}\raggedright
Prescribing provider identifier is present in drug dispensation data.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedright
Computational\strut
\end{minipage} & \begin{minipage}[t]{0.67\columnwidth}\raggedright
Computed eGFR value conforms to the expected value for a test case patient scenario.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.08\columnwidth}\raggedright
Completeness\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedright
n/a (no subtypes defined)\strut
\end{minipage} & \begin{minipage}[t]{0.67\columnwidth}\raggedright
A drug product withdrawn from the market at a specific absolute historic date shows expected drop in dispensation.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.08\columnwidth}\raggedright
Plausibility\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedright
Uniqueness\strut
\end{minipage} & \begin{minipage}[t]{0.67\columnwidth}\raggedright
A zip code for a location does not refer to vastly conflicting geographical areas.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedright
Atemporal\strut
\end{minipage} & \begin{minipage}[t]{0.67\columnwidth}\raggedright
Use of a medication (by age group) for a specific disease agrees with the age pattern for that disease.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedright
Temporal\strut
\end{minipage} & \begin{minipage}[t]{0.67\columnwidth}\raggedright
Temporal pattern of an outbreak of a disease (e.g., Zika) agrees with external source pattern.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Kahn introduces the term \emph{data quality check} (sometimes refered to as data quality rule) that tests whether data conform to a given requirement (e.g., implausible age of 141 of a patient (due to incorrect birth year or missing death event)). In support of checks, he also defines \emph{data quality measure} (sometimes refered to as pre-computed analysis) as data analysis that supports evaluation of a check. For example, distribution of days of supply by drug concept.

Two types of DQ checks can be distinguished\citep{weiskopf_methods_2013}

\begin{itemize}
\tightlist
\item
  general checks
\item
  study-specific checks
\end{itemize}

From the point of researcher analyzing the data, the desired situation is that data is free from erros that could have been prevented. \emph{ETL data errors} are errors introduced during extract-tranform-load proces. A special type of ETL data error is \emph{mapping error} that results from incorrect mapping of the data from the source terminology (e.g., Korean national drug terminology) into the target data model's standard terminology (e.g., RxNorm and RxNorm Extension). A \emph{source data error} is an error that is already present in the source data due to various cuases (e.g., human typo during data entry).\citep{huser_multisite_2016}

Data quality can also be seen as a component in a larger effort refered to as \emph{evidence quality} or \emph{evidence validation}. Data quality would fall in this framework under \emph{data validation}.

\hypertarget{achilles-heel-tool}{%
\section{Achilles Heel tool}\label{achilles-heel-tool}}

Since 2014, a component of the OHDSI Achilles tool called Heel was used to check data quality.\citep{huser_methods_2018}

\hypertarget{precomputed-analyses}{%
\subsection{Precomputed Analyses}\label{precomputed-analyses}}

In support of data characterization, Achilles tool pre-computes number of data analyses. Each pre-computed analysis has an analysis ID and a short description of the analysis. For example, ``715: Distribution of days\_supply by drug\_concept\_id'' or ``506: Distribution of age at death by gender''. List of all pre-computed analyses (for Achilles version 1.6.3) as available at \url{https://github.com/OHDSI/Achilles/blob/v1.6.3/inst/csv/achilles/achilles_analysis_details.csv}

Achilles has more than 170 pre-computed analysis that support not only data quality checks but also general data characterization (outside data quality context) such as data density visualizations. The pre-computations are largely guided by the CDM relational database schema and analyze most terminology-based data columns, such as condition\_concept\_id or place\_of\_service\_concept\_id. Pre-computations results are stored in table ACHILLES\_RESULTS and ACHILLES\_RESULTS\_DIST.

\hypertarget{example-dq-check}{%
\subsection{Example DQ check}\label{example-dq-check}}

In complete data about general population, a range of services is provided by a range of providers (with many specialties). A data completness rule with rule\_id of 38 evaluates data completness in the PROVIDER table. Checking optional fields in CDM (such as provider specialty) lead to a notification severity output. Analysis Rule 38 triggers a notification if count of distinct specialties \textless2. It relies on a derived measure \texttt{Provider:SpeciatlyCnt}. The rule SQL-formulated logic can be found here: \url{https://github.com/OHDSI/Achilles/blob/v1.6.3/inst/sql/sql_server/heels/serial/rule_38.sql}

\hypertarget{overview-of-existing-dq-heel-checks}{%
\subsection{Overview of existing DQ Heel checks}\label{overview-of-existing-dq-heel-checks}}

Achilles developers maintain a list of all DQ checks in an overview file. For version 1.6.3, this overview is available here \url{https://github.com/OHDSI/Achilles/blob/v1.6.3/inst/csv/heel/heel_rules_all.csv} Each DQ check has a rule\_id.

Checks are classified into CDM conformance checks and DQ checks.

Depending on the severity of the problem, the Heel output can be error, warning or notification.

\hypertarget{study-specific-checks}{%
\section{Study-specific checks}\label{study-specific-checks}}

The chapter has so far focused on general DQ checks. Such checks are executed regardless of the single research question context. The assumption is that a researcher would formulate additional DQ checks that are required for a specific research question.

We use case studies to demostrate study-specific checks.

\hypertarget{outcomes}{%
\subsection{Outcomes}\label{outcomes}}

For an international analysis, part of OHDSI study diagnostics (for a give dataset) may involve checking whether coding practices (that are country specific) affect a cohort definition. A stringent cohort definition may lead to zero cohort size in one (or multiple dataesets).

\hypertarget{laboratory-data}{%
\subsection{Laboratory data}\label{laboratory-data}}

A diabetes study may utilize HbA1c measurement. A 2018 OHDSI study (\url{https://www.ncbi.nlm.nih.gov/pubmed/30646124}) defined a cohort `HbA1c8Moderate' (see \url{https://github.com/rohit43/DiabetesTxPath/blob/master/inst/settings/CohortsToCreate.csv})

\hypertarget{etl-unit-testing}{%
\section{ETL unit testing}\label{etl-unit-testing}}

Extract Transform Load (ETL) process that transforms data from source (in EHR system or claims sys) to target (OMOP CDM) can contain errors. Unit testing of ETL code allows for preventing coding errors in ETL to cause data errors.

\hypertarget{unit-testing-framwork-in-rabbit-in-a-hat}{%
\subsection{Unit testing framwork in Rabbit-in-a-Hat}\label{unit-testing-framwork-in-rabbit-in-a-hat}}

OHDSI tool Rabbit-in-a-Hat includes an ETL unit testing framwork. This framework defines an a set of function for each table in the source schema and a set of functions for each table in target OMOP CDM schema. Detailed description is available at \url{https://www.ohdsi.org/web/wiki/doku.php?id=documentation:software:whiterabbit:test_framework}.

\hypertarget{ClinicalValidity}{%
\chapter{Clinical Validity}\label{ClinicalValidity}}

\emph{Chapter leads: Joel N. Swerdel, Seng Chan You}

\begin{quote}
The likelihood of transforming matter into energey is something akin to shooting birds in the dark in a country where there are only a few birds. \emph{Einstein, 1935}
\end{quote}

The goal of the Research Network is to lower the barrier to performing large-scale collaborative research using observational data to generate high-quality evidence through peer review across study design, execution, and data analysis \citep{Hripcsak2015}. Patient care and policy decisions demand high-quality evidence. Some have hypothesized that the volume, velocity, and veracity of observational data from electronic health records, administrative claims, and investment in data networks can be positioned to meet this demand. Analyses in large data sets are not necessarily correct simply because they are larger. Deficient studies may lead to misuse of resources and result in poor health care oucomes for patients (Morton et al). This chapter will focus on the question: `To what extent does the analysis conducted match the clinical intention?'

Studies using observational data usually begin by developing cohorts of subjects to compare for determining some effect estimate. During the development of these cohorts certain assumptions are made concerning the validity of the created cohorts, the most important of which is that the subjects in the cohorts have the characteristic that is the basis of the study. For example, if the study involves subjects with myocardial infarction (MI) each of the subjects in the cohort must have had an MI. Ideally, we would definitive evidence of the diagnosis of MI. However, with observational data, we use subject records derived from limited data collected for a specific purpose. In some cases, the data may be derived from data sets specifically collected from subjects with the health condition of interest such as disease registries. While this is high quality data, it is usually a subset of all the subject's data. For example, the data for subjects with MI may include the interpretation of the electrocardiogram (ECG) and not the actual data from the ECG. Subjects are included in these registries based on a clinical review of each subject's health record. In other cases, the data is derived from administrative datasets from insurance claims. These data are usually much more limited than data from health registries but have the advantage of usually including a larger number of subjects from a broader population which may be more generalizable to the overall population. In the case of administrative data, determination of the health claim for a subject is based on administrative codes for the health condition. In the US, for example, these codes are from the International Classification of Diseases (ICD). Regardless of the origin of the data, the validity of the health conditions based on this data need to be validated.
An example of a typical epidemiological study is shown in the diagram below:

\begin{center}\includegraphics[width=0.75\linewidth]{images/ClinicalValidity/figure1} \end{center}

In this example, we are comparing the rates of the occurrence of MI in one year in cohorts of subjects who initiated either Drug A or Drug B. In this example, it is critical for the validity of the study to have valid measures of the rate of MI occurrence. For studies where administrative data is used, the determination of MI is typically from the use of a phenotype algorithm (PA). A PA is a heuristic-based set of rules used to determine the health condition with good precision. These algorithms are often derived from prior research some of which may have been validated. Examples of typical PAs for MI are illustrated below:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  One or more occurrences of MI in the subject's record
\end{enumerate}

\begin{center}\includegraphics[width=0.75\linewidth]{images/ClinicalValidity/figure2} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  One occurrence of MI in the subject's record followed by a second occurrence within 30 days
\end{enumerate}

\begin{center}\includegraphics[width=0.75\linewidth]{images/ClinicalValidity/figure3} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  One or more occurrences of MI in the subject's record from a hospital in-patient setting
\end{enumerate}

\begin{center}\includegraphics[width=0.75\linewidth]{images/ClinicalValidity/figure4} \end{center}

Once the PA for the study has been determined, the validity of the definition needs to be determined. To determine the validity of the different algorithms, we need to examine several performance characteristics of the PA including:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Sensitivity of the PA -- what proportion of the subjects with the health condition in the whole cohort were determined to have the health outcome based on the PA?
\item
  Specificity of the PA - what proportion of the subjects without the health condition in the whole cohort were determined to not have the health outcome based on the PA?
\item
  Positive predictive value of the PA - what proportion of the subjects determined by the PA to have the health condition actually had the health condition?
\item
  Negative predictive value of the PA - what proportion of the subjects determined by the PA to not have the health condition actually did not have the health condition?
\end{enumerate}

The methods used to determine these performance characteristics are described in the remainder of this section.
Validation Methods for Phenotype Algorithms
The traditional method that has been used to validate PAs has been through a thorough examination of subject records by one or more people with sufficient clinical knowledge to accurately determine the health condition of interest. The method generally follows these steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Select a random subset of subjects from the overall cohort.
\item
  Obtain permission from these subjects to receive health records.
\item
  Obtain the health records from those subjects who have granted permission to do so from their physicians.
\item
  Select one or more persons with sufficient clinical expertise to review subject records.
\item
  Determine the guidelines for adjudicating whether a subject is positive or negative for the health condition.
\item
  Use the results from the clinical adjudication to calculate the performance characteristics of the PA used in the study.
\end{enumerate}

Each step in the above process has the potential to bias the results of the study. For example, obtaining permission from subjects may be difficult and may introduce selection bias if those subjects who provide permission differ from those who do not. In addition, obtaining the patient records and conducting a clinical review of those records is a time consuming and costly process. In order to complete this process, many studies only examine the records of those subjects the PA identified as cases for the health conditions. Under those conditions, the only performance characteristic that can be calculated is positive predictive value.
In the OHDSI community, we were in the process of developing a different approach. We are attempting to use diagnostic predictive models as an alternative method for cohort validation. The general idea is to simulate the ascertainment of the health outcome similar to the way clinicians would in a traditional phenotype algorithm validation but at scale. In this process we develop a diagnostic predictive model for a health outcome and then use that model to determine the probability of a health outcome in a large set of subjects, the ``evaluation'' cohort, within the data set. We then use that evaluation cohort to test our phenotype algorithms. Using this method, we are able to determine the full set of performance characteristics (i.e., sensitivity, specificity, and positive and negative predictive value) at scale. The tool is being developed as an open-source R package called \href{https://github.com/OHDSI/PheValuator}{\textbf{PheValuator}}.

The process is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Develop a diagnostic predictive model for a phenotype: Diagnostic predictive models are used to estimate the probability that a specific outcome or disease is present in an individual.\citep{moons_transparent_2015} The output of the model is a set of weighted predictors for diagnosing a phenotype.
\item
  Determine the probability of a phenotype for each individual in a large group of subjects: The set of predictors from the model can be used to estimate the probability of the presence of a phenotype in an individual. We use these predictions as a `probabilistic gold standard'.
\item
  Evaluate the performance characteristics of the PAs: We compare the predicted probability to the binary classification of a PA (the test conditions for the confusion matrix). Using the test conditions and the estimates for the true conditions, we can fully populate the confusion matrix and determine the full set of performance characteristics, i.e., sensitivity, specific, and predictive values.
  There are limitations using this approach. First, we can only use data that is in dataset; we are limited to diagnoses, procedures, observations, measurements, and drug exposures. Moreover, measurements are usually incomplete in administrative datasets as these do not include the actual values for the measures and, as such, we are limited to the existence or absence of the measurements as part of the model. Clinical notes are also not usually present in our data sets. Patient complaints or symptoms may not be recorded in administrative data sets. These may include things like lethargy and acute pain.
  In diagnostic predictive modeling we create a model that discriminates between those with the disease and those without the disease. For the PheValuator process, we use an extremely specific cohort definition, the ``xSpec'' cohort, to determine the cases for the models. The xSpec cohort uses a definition to find those with the disease with a very high probability of having the disease of interest. The xSpec cohort is may be defined as those subjects who have multiple condition code codes in their record for the health outcome of interest. For example, for atrial fibrillation, we may have subjects who have 10 or more codes for atrial fibrillation in their record. For MI, an acute outcome, we may use 5 occurrences of MI and include the requirement of having at least two occurrences from a hospital inpatient setting. For our non-cases, we exclude anybody from the data set who have any of the condition occurrences for the health outcome of interest. There are limitations to this method. It is possible that these xSpec cohort subjects may have may be more severe than other cases of the disease. It may also be that these subjects had longer observation time after initial diagnosis than the average patient. We use logistic regression to create these models, using the LASSO method.\citep{tibshirani_regression_1996} This method produces a parsimonious model. In so doing it removes many of the collinear covariates which may be present in many subjects. In the current version of our software we also do not include the temporality of events in the patient's record.
\end{enumerate}

\textbf{Practice of Phenotype Algorithm Validation}

An example of the process to conduct a PA validation using chart review is provided by Cutrona and colleagues who validated their PA for MI for the US Food and Drug Administration's (FDA) Sentinel program.\citep{cutrona_validation_2013} The steps this group used to conduct the validation were as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Develop a PA for MI: They used the PA ``those with an ICD-9-CM code for AMI (410.x0, 410.x1) in the principal or primary position on facility claims for hospitalizations.''
\item
  Randomly select enough cases to be requested from the primary health care providers in order to achieve a sufficient successful return of records to achieve appropriate statistical precision for their performance measure, in this case PPV.
\item
  Request approximately an equal number of patient records from each of their four data providers.
\item
  Receive records from data providers redacted to ensure patient confidentiality.
\item
  Abstract the data from the records using the clinical expertise of two nurses. Abstracted information included EKG images, cardiac biomarkers, information on ischemic symptoms and results of cardiac diagnostic tests.
\item
  Test inter-rater reliability of the two data abstractors.
\item
  Develop an adjudication protocol based on standardized criteria from the joint European Society of Cardiology and American College of Cardiology Global Task Force
\item
  Review abstracted records using the clinical expertise of two cardiologists. The cardiologists were provided with all abstracted information listed above and an abstracted case summary. Detailed discharge information was not provided to the reviewers.
\item
  Classify each subject as: (1) definite MI; (2) probable MI; (3) no MI; or (4) unable to determine. Distinguishing between definite and probable was not defined for the clinicians but was to based on their own clinical judgement. Subjects were considered cases if the clinicians determined the subject to be definite or probable for MI.
\item
  Disagreement between the two clinicians, i.e., one clinician determining the subject to be definite or probable and the other either not an MI or unable to determine, was settled through a joint discussion between the clinicians until agreement could be reached.
\end{enumerate}

It appears evident that the above process is both very thorough as well as very time consuming and costly. In this example neither sensitivity nor specificity of the PA was determined likely owing to the cost of including records from a random selection of patients not determined to have an MI based on the PA. Overall the process examined the records of 143 of the 153 records requested. One advantage this group had was that patient permission was not required as the FDA Sentinel Initiative activities did not require institutional review board (IRB) approval. At the end of this process the researchers determined the PPV for this algorithm to be 86.0\% (95\% confidence interval; 79.2\%, 91.2\%).
The above study was included in a review of validation efforts for MI PAs by Rubbo et al.\citep{rubbo_use_2015} In this review, the authors examined 33 studies involving validation of phenotype algorithms for acute myocardial infarction. They found that there was significant heterogeneity in the phenotype algorithms used in the studies as well as in the validation methods and the results reported. The authors concluded that for acute myocardial infarction there is no gold standard phenotype algorithm available. They noted that the process was both costly and time consuming. Due to that limitation most studies had small sample sizes in their validation leading to wide variations in the estimates for the performance characteristics. They also noted that in the 33 studies, while all the studies reported positive predictive value, only 11 studies reported sensitivity and only five studies reported specificity. The question then needs to be asked is this really validation of the phenotype algorithm?
As discussed previously an alternative approach currently being developed in the OHDSI community is through the use of diagnostic predictive modeling tool using a tool called PheValuator. The following are the steps for testing PAs for MI using PheValuator:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Develop an extremely specific, xSpec, cohort to determine those with MI with a high probability. For MI, we used an occurrence of MI with one or more occurrences of MI recorded from a hospital in-patient visit within 5 days, and 4 or more occurrences of MI in the patient record within 365 days. The following illustrates this PA for MI:
\end{enumerate}

\begin{center}\includegraphics[width=0.75\linewidth]{images/ClinicalValidity/figure5} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Develop the diagnostic predictive model for MI
\end{enumerate}

\begin{itemize}
\item
  Create a sample of subjects labeled as cases (from the xSpec cohort for MI) or non-cases in approximate proportion to the prevalence of MI in the population.
\item
  Use the OHDSI Patient Level Prediction package to develop a LASSO logistic regression model using all available data in the subject record.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\item
  Randomly select 2M subjects from the database, the evaluation cohort, and apply the model to this cohort to determine the probability of MI in each subject.
\item
  Test possible algorithms for MI, e.g., 1 occurrence of MI in the patient record, using the evaluation cohort.
\item
  Compare performance characteristics for the various algorithms to determine the PA for MI for use within a study.
\end{enumerate}

Using this process, Table 1 displays the performance characteristics for four PAs for MI across five datasets. For a PA similar to the one evaluated by Cutrona and colleagues, ``\textgreater=1 X HOI, In-Patient'', we found a mean PPV of 67\% (range: 59\%-74\%).

Table 1: Performance Characteristics of Four Phenotype Algorithms using Diagnostic Condition Codes to Determine Myocardial Infarction on Multiple Datasets using PheValuator. The continuous 3-color heat map for the data in the table was defined as Red (value = 0), Yellow (value = 0.5), and Green (value = 1).

\begin{center}\includegraphics[width=0.75\linewidth]{images/ClinicalValidity/figure6} \end{center}

Sens -- Sensitivity ; PPV -- Positive Predictive Value ; Spec -- Specificity; NPV -- Negative Predictive Value; Dx Code -- Diagnosis code for the phenotype; CCAE - IBM MarketScan Commercial Claims and Encounters Database, ages 18-62 years; MDCR - IBM MarketScan Medicare Supplemental and Coordination of Benefits Database, ages 66 years and greater; MDCD - IBM MarketScan Multi-State Medicaid, ages 18-62 years; Optum1862 - Optum De-Identified Clinformatics Data Mart Database -- Date of Death, ages 18-62 years; OptumGE66 - ages 66 years and greater

\hypertarget{SoftwareValidity}{%
\chapter{Software Validity}\label{SoftwareValidity}}

\emph{Chapter lead: Martijn Schuemie}

The central question of software validity is

\begin{quote}
Does the software do what it is expected to do?
\end{quote}

Software validity is an essential component of evidence quality: only if our analysis software does what it is expected to do can we produce reliable evidence. As described in Section \ref{automation}, it is essential to view every study as a software development exercise, creating an automated script that executes the entire analysis, from data in the Common Data Model (CDM) to the results such as estimates, figures as tables. It is this script, and any software used in this script, that must be validated. As described in Section \ref{analysisImplementation}, we can write te entire analysis as custom code, or we can use the functionality available in the \href{https://ohdsi.github.io/MethodsLibrary/}{OHDSI Methods Library}. The advantage of using the Methods Library is that great care has already been taken to ensure its validity, so establishing the validity of the entire analysis becomes less burdensome.

In this chapter we first describe best practices for writing valid analysis code. After this we discuss how the Methods library is validated through its software development process and testing.

\hypertarget{study-code-validity}{%
\section{Study code validity}\label{study-code-validity}}

\hypertarget{automation}{%
\subsection{Automation as a requirement for reproducibility}\label{automation}}

Traditionally, observational studies are often viewed as a journey rather than a process: a database expert may extract a data set from the database and hands this over to the data analyst, who may open it in a spreadsheet editor or other interactive tool, and starts working on the analysis. In the end, a result is produced, but little is preserved of how it came about. The destination of the journey was reached, but it is not possible to retrace the exact steps taken to get there. This practice is entirely unacceptable, both because it is not reproducible, but also because it lacks transparency; we do not know exactly what was done to produce the result, so we also cannot verify that no mistakes were made.

Every analysis generating evidence must therefore be fully automated. By automated we mean the analysis should be implemented as a single script, and we should be able to redo the entire analysis from database in CDM format to results, including tables and figures, with a single command. The analysis can be of arbitrary complexity, perhaps producing just a single count, or generating empirically calibrated estimates for millions of research questions, but the same principle applies. The script can invoke other scripts, which in turn can invoke even lower-level analysis processes.

The analysis script can be implemented in any computer language, although in OHDSI the preferred language is R. Thanks to the \href{https://ohdsi.github.io/DatabaseConnector/}{DatabaseConnector} R package, we can connect directly to the data in CDM format, and many advanced analytic are available through the other R packages in the \href{https://ohdsi.github.io/MethodsLibrary/}{OHDSI Methods Library}.

\hypertarget{programming-best-practices}{%
\subsection{Programming best practices}\label{programming-best-practices}}

Observational analyses can become very complex, with many steps needed to produce the final results. This complexity can make it harder to maintain the analysis code, and increase the likelihood of making errors as well as making it harder to notice errors. Luckily, computer programmers have over many years developed best practices for writing code that can deal with complexity, and is easy to read, reuse, adapt, and verify. \citep{Martin_2008} A full discussion of these best practices could fill many books. Here, we highlight these four import principles:

\begin{itemize}
\tightlist
\item
  \textbf{Abstraction}: Rather than write a single large script that does everything, leading to so-call ``spaghetti code'' where dependencies between lines of code can go from anywhere to anywhere (e.g.~a value set on line 10 is used in line 1,000), we can organize our code in units called ``functions''. A function should have a clear goal, for example ``take random sample'', and once created we can then use this function in our larger script without having to think of the minutiae of what the function does; We can abstract the function to a simple-to-understand concept.
\item
  \textbf{Encapsulation}: For abstraction to work, we should make sure that dependencies of a function are minimized and clearly defined. Our example sampling function should have a few arguments (e.g.~a dataset and a sample size), and one output (e.g.~the sample). Nothing else should influence what the function does. So-called ``global variables'', variables that are set outside a function, are not arguments of a function, but are nevertheless used in the function, should be avoided.
\item
  \textbf{Clear naming}: Variables and functions should have clear names, making code read almost like natural language. For example, instead of \texttt{x\ \textless{}-\ spl(y,\ 100)}, we can write code that reads \texttt{sampledPatients\ \textless{}-\ takeSample(patients,\ sampleSize\ =\ 100)}. Try to resist the urge to abbreviate. Modern languages have no limits on the length of variable and function names.
\item
  \textbf{Reuse}: One advantage of writing clear, well encapsulated functions is that they can often be reused. This not only saves time, it also means there will be less code, so less complexity and fewer opportunities for errors.
\end{itemize}

\hypertarget{code-validation}{%
\subsection{Code validation}\label{code-validation}}

Several approaches exist to verify the validity of software code, but two are especially relevant for code implementing an observational study:

\begin{itemize}
\tightlist
\item
  \textbf{Code review}: One person writes the code, and another person reviews the code.
\item
  \textbf{Double coding}: Two persons both independently write the analysis code, and afterwards the results of the two scripts are compared.
\end{itemize}

Code review has the advantage that it is usually less work, but the disadvantage is that the reviewer might miss some errors. Double coding on the other hand is usually very labor intensive, but it is less likely, although not impossible, that errors are missed. Another disadvantages of double coding is that two separate implementations \emph{almost always} produce different results, due to the many minor arbitrary choices that need to made (e.g.~should ``until exposure end'' be interpreted as including the exposure end date, or not?). As a consequence, the two supposedly independent programmers often need to work together to align their analyses, thus breaking their independence.

Other software validation practices such as unit testing are less relevant here because a study is typically a one-time activity with highly complex relationship between input (the data in CDM) and outputs (the study results), making these practices less usable. Note that these practices are applied in the Methods Library.

\hypertarget{using-the-methods-library}{%
\subsection{Using the Methods Library}\label{using-the-methods-library}}

The \href{https://ohdsi.github.io/MethodsLibrary/}{OHDSI Methods Library} provides a large set of functions, allowing most observational studies to be implemented using only a few lines of code. Using the Methods Library therefore shifts most of the burden of establishing the validity of one's study code to the Library. Validity of the Methods Library is ensured by its software development process, and by extensive testing.

\hypertarget{methods-library-software-development-process}{%
\section{Methods Library software development process}\label{methods-library-software-development-process}}

The OHDSI Methods Library is developed by the OHDSI community. Proposed changes to the Library are discussed in two venues: The GitHub issue trackers (for example the CohortMethod issue tracker\footnote{\url{https://github.com/OHDSI/CohortMethod/issues}}) and the OHDSI Forums\footnote{\url{http://forums.ohdsi.org/}}. Both are open to the public. Any member of the community can contribute software code to the Library, however, final approval of any changes incorporated in the released versions of the software is performed by the OHDSI Population-Level Estimation Workgroup leadership (Drs. Marc Suchard and Martijn Schuemie) and OHDSI Patient-Level Prediction Workgroup leadership (Drs. Peter Rijnbeek and Jenna Reps) only.

Users can install the Methods Library in R directly from the master branches in the GitHub repositories, or through a system known as ``drat'' that is always up-to-date with the master branches. A number of the Methods Library packages are available through R's Comprehensive R Archive Network (CRAN), and this number is expected to increase over time.

Reasonable software development and testing methodologies are employed by OHDSI to maximize the accuracy, reliability and consistency of the Methods Library performance. Importantly, as the Methods Library is released under the terms of the Apache License V2, all source code underlying the Methods Library, whether it be in R, C++, SQL, or Java is available for peer review by all members of the OHDSI community, and the public in general. Thus, all the functionality embodied within Methods Library is subject to continuous critique and improvement relative to its accuracy, reliability and consistency.

\hypertarget{source-code-management}{%
\subsection{Source Code Management}\label{source-code-management}}

All of the Methods Library's source code is managed in the source code version control system ``git'' publicly accessible via GitHub. The OHDSI Methods Library repositories are access-controlled. Anyone in the world can view the source code, and any member of the OHDSI community can submit changes through so-called pull requests. Only the OHDSI Population-Level Estimation Workgroup and Patient-Level Prediction Workgroup leadership can approve such request, make changes to the master branches, and release new versions. Continuous logs of code changes are maintained within the GitHub repositories and reflect all aspects of changes in code and documentation. These commit logs are available for public review.

New versions are released by the OHDSI Population-Level Estimation Workgroup and Patient-Level Prediction Workgroup leadership as needed. A new release starts by pushing changes to a master branch with a package version number (as defined in the DESCRIPTION file inside the package) that is greater than the version number of the previous release. This automatically triggers checking and testing of the package. If all tests are passed, the new version is automatically tagged in the version control system and the package is automatically uploaded to the OHDSI drat repository. New versions are numbered using three-component version number:

\begin{itemize}
\tightlist
\item
  New \textbf{micro versions} (e.g.~from 4.3.2 to 4.3.3) indicate bug fixes only. No new functionality, and forward and backward compatibility are guaranteed
\item
  New \textbf{minor versions} (e.g.~from 4.3.3 to 4.4.0) indicate added functionality. Only backward compatibility is guaranteed
\item
  New \textbf{major versions} (e.g.~from 4.4.0 to 5.0.0) indicate major revisions. No guarantees are made in terms of compatibility
\end{itemize}

\hypertarget{documentation-2}{%
\subsection{Documentation}\label{documentation-2}}

All packages in the Methods Library are documented through R's internal documentation framework. Each package has a package manual that describes every function available in the package. To promote alignment between the function documentation and the function implementation, the \href{https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html}{roxygen2} software is used to combine a function's documentation and source code in a single file. The package manual is available on demand through R's command line interface, as a PDF in the package repositories, and as a web page. In addition, many packages also have vignettes that highlight specific use cases of a package. All documentation can be viewed though the Methods Library website\footnote{\url{https://ohdsi.github.io/MethodsLibrary/}}.

All Method Library source code is available to end users. Feedback from the community is facilitated using GitHub's issue tracking system and the OHDSI Forums.

\hypertarget{availability-of-current-and-historical-archive-versions}{%
\subsection{Availability of Current and Historical Archive Versions}\label{availability-of-current-and-historical-archive-versions}}

Current and historical versions of the Methods Library packages are available in two locations: First, the GitHub version control system contains the full development history of each package, and the state of a package at each point in time can be reconstructed and retrieved. Most importantly, each released version is tagged in GitHub. Second, the released R source packages are stored in the OHDSI GitHub drat repository.

\hypertarget{maintenance-support-and-retirement}{%
\subsection{Maintenance, Support and Retirement}\label{maintenance-support-and-retirement}}

Each current version of the Methods Library is actively supported by OHDSI with respect to bug reporting, fixes and patches. Issues can be reported through GitHub's issue tracking system, and through the OHDSI forums. Each package has a package manual, and zero, one or several vignettes. Online video tutorials are available, and in-person tutorials are provided from time to time.

\hypertarget{qualified-personnel}{%
\subsection{Qualified Personnel}\label{qualified-personnel}}

Members of OHDSI community represent multiple statistical disciplines and are based at academic, not-for-profit and industry-affiliated institutions on multiple continents.

All leaders of the OHDSI Population-Level Estimation Workgroup and OHDSI Patient-Level Prediction Workgroup hold PhDs from accredited academic institutions and have published extensively in peer reviewed journals.

\hypertarget{physical-and-logical-security}{%
\subsection{Physical and Logical Security}\label{physical-and-logical-security}}

The OHDSI Methods Library is hosted on the GitHub\footnote{\url{https://github.com/}} system. GitHub's security measures are described at \url{https://github.com/security}. Usernames and passwords are required by all members of the OHDSI community contribute modifications to the Methods Library, and only the Population-Level Estimation Workgroup and Patient-Level Prediction Workgroup leadership can makes changes to the master branches. User accounts are limited in access based upon standard security policies and functional requirements.

\hypertarget{disaster-recovery}{%
\subsection{Disaster Recovery}\label{disaster-recovery}}

The OHDSI Methods Library is hosted on the GitHub system. GitHub's disaster recovery facilities are described at \url{https://github.com/security}.

\hypertarget{methods-library-testing}{%
\section{Methods Library testing}\label{methods-library-testing}}

We distinguish between two types of tests performed on the Methods Library: Tests for individual functions in the packages (so-called ``unit tests''), and tests for more complex functionality using simulations.

\hypertarget{unit-test}{%
\subsection{Unit test}\label{unit-test}}

A large set of automated validation tests is maintained and upgraded by OHDSI to enable the testing of source code against known data and known results. Each test begins with specifying some simple input data, then executes a function in one of the packages on this input, and evaluates whether the output is exactly what would be expected. For simple functions, the expected result is often obvious (for example when performing propensity score matching on example data containing only a few subjects), for more complicated functions the expected result may be generated using combinations of other functions available in R (for example, Cyclops, our large-scale regression engine, is tested among others by comparing results on simple problems with other regression routines in R). We aim for these tests in total to cover 100\% of the lines of executable source code.

These tests are automatically performed when changes are made to a package (specifically, when changes are pushed to the package repository). Any errors noted during testing automatically trigger emails to the leadership of the Workgroups, and must be resolved prior to release of a new version of a package. The source code and expected results for these tests are available for review and use in other applications as may be appropriate. These tests are also available to end users and/or system administrators and can be run as part of their installation process to provide further documentation and objective evidence as to the accuracy, reliability and consistency of their installation of the Methods Library.

\hypertarget{simulation}{%
\subsection{Simulation}\label{simulation}}

For more complex functionality it is not always obvious what the expected output should be given the input. In these cases simulations are sometimes used, generating input given a specific statistical model, and establishing whether the functionality produces results in line with this known model. For example, in the \href{https://ohdsi.github.io/SelfControlledCaseSeries/}{SelfControlledCaseSeries} package simulations are used to verify that the method is able to detect and appropriately model temporal trends in simulated data.

\hypertarget{summary-6}{%
\section{Summary}\label{summary-6}}

\BeginKnitrBlock{rmdsummary}
\begin{itemize}
\item
  An observational study should be implemented as an automated script that executes the entire analysis, from data in the CDM to the results, to ensure reproducability and transparancy.
\item
  Custom study code should adhere to best programming practices, including abstraction, encapsulation, clear naming, and code reuse.
\item
  Custom study code can be validated using code review or double coding.
\item
  The Methods Library provided validated functionality that can be used in observational studies.
\item
  The Methods Library is valdated by using a software development process aimed at creating valid software, and by testing.
\end{itemize}
\EndKnitrBlock{rmdsummary}

\hypertarget{MethodValidity}{%
\chapter{Method Validity}\label{MethodValidity}}

\emph{Chapter lead: Martijn Schuemie}

When considering method validity we aim to answer the question

\begin{quote}
Is this method valid for answering this question?
\end{quote}

Where ``method'' includes not only the study design, but also the data and the implementation of the design. Method validity is therefore somewhat of a catch-all; It is often not possible to observe good method validity without good data quality, clinical validity, and software validity. Those aspects of evidence quality should have already been addressed separately before we consider method validity.

The core activity when establishing method validity is evaluating whether important assumptions in the analysis have been met. For example, we assume that propensity-score matching makes two populations comparable, but we need to evaluate whether this is the case. Where possible, empirical tests should be performed to verify these assumptions. We can for example generate diagnostics to show that our two populations are indeed comparable on a wide range of characteristics after matching. In OHDSI we have developed many standardized diagnostics that should be generated and evaluated whenever an analysis is performed.

In this chapter we will focus on the validity of methods use in population-level estimation. We will first briefly highlight some study design-specific diagnostics, and will then discuss diagnostics that are applicable to most if not all population-level estimation studies. Following this is a step-by-step description of how to execute these diagnostics using the OHDSI tools. We close this chapter with an advanced topic, reviewing the OHDSI Methods Benchmark and its application to the OHDSI Methods Library.

\hypertarget{design-specific-diagnostics}{%
\section{Design-specific diagnostics}\label{design-specific-diagnostics}}

For each study design there are diagnostics specific to such a design. Many of these diagnostics are implemented and readily available in the R packages of the \href{https://ohdsi.github.io/MethodsLibrary/}{OHDSI Methods Library}. For example, Section \ref{studyOutputs} lists a wide range of diagnostics generated by the \href{https://ohdsi.github.io/CohortMethod/}{CohortMethod} package, including:

\begin{itemize}
\tightlist
\item
  \textbf{Propensity score distribution} to asses initial comparability of cohorts.
\item
  \textbf{Propensity model} to identify potential variables that should be excluded from the model.
\item
  \textbf{Covariate balance} to evaluate whether propensity score adjustment has made the cohorts comparable (as measured through baseline covariates).
\item
  \textbf{Attrition} to observe how many subjects were excluded in the various analysis steps, which may inform on the generalizability of the results to the initial cohorts of interest.
\item
  \textbf{Power} to assess whether enough data is available to answer the question.
\item
  \textbf{Kaplan Meier curve} to asses typical time to onset, and whether the proportionality assumption underlying Cox models is met.
\end{itemize}

Other study designs require different diagnostics to test the different assumptions in those designs. For example, for the self-controlled case series (SCCS) design we may check the necessary assumption that the end of observation is independent of the outcome. This assumption is often violated in the case of serious, potentially lethal, events such as myocardial infarction. We can evaluate whether the assumption holds by generating the plot shown in Figure \ref{fig:timeToObsEnd}, which shows histograms of the time to observation period end for those that are censored, and those that uncensored. In our data we consider those whose observation period ends at the end date of data capture (the date when observation stopped for the entire data base, for example the date of extraction, or the study end date) to be uncensored, and all others to be censored. In Figure \ref{fig:timeToObsEnd} we see only minor differences between the two distributions, suggesting our assumptions holds.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/MethodValidity/timeToObsEnd} 

}

\caption{Time to observation end for those that are censored, and those that uncensored.}\label{fig:timeToObsEnd}
\end{figure}

\hypertarget{diagnostics-for-all-estimation}{%
\section{Diagnostics for all estimation}\label{diagnostics-for-all-estimation}}

Next to the design-specific diagnostics, there are also several diagnostics that are applicable across all causal effect estimation methods. Many of these rely on the use of control hypotheses, research questions where the answer is already known. Using control hypotheses we can then evaluate whether our design produces results in line with the truth. Controls can be divided into negative controls and positive controls.

\hypertarget{NegativeControls}{%
\subsection{Negative controls}\label{NegativeControls}}

Negative controls are exposure-outcome pairs where one believes no causal effect exists, and including negative controls or ``falsification endpoints'' \citep{prased_2013} has been recommended as a means to detect confounding, \citep{lipsitch_2010} selection bias and measurement error. \citep{arnold_2016} For example, in one study \citep{zaadstra_2008} investigating the relationship between childhood diseases and later multiple sclerosis (MS), the authors include three negative controls that are not believed to cause MS: a broken arm, concussion, and tonsillectomy. Two of these three controls produce statistically significant associations with MS, suggesting that the study may be biased.

We should select negative controls that are comparable to our hypothesis of interest, which means we typically select exposure-outcome pairs that either have the same exposure as the hypothesis of interest (so-called ``outcome controls'') or the same outcome (``exposure controls''). Our negative controls should further meet these criteria:

\begin{itemize}
\tightlist
\item
  The exposure \textbf{should not cause} the outcome. One way to think of causation is to think of the counterfactual: could the outcome be caused (or prevented) if a patient was not exposed, compared to if the patient had been exposed? Sometimes this is clear, for example ACEi are known to cause angioedema. Other times this is far less obvious. For example, a drug that may cause hypertension can therefore indirectly cause cardiovascular diseases that are a consequence of the hypertension.
\item
  The exposure should also \textbf{not prevent or treat} the outcome. This is just another causal relationship that should be absent if we are to believe the true effect size (e.g.~the hazard ratio) is 1.
\item
  The negative control should \textbf{exist in the data}, ideally with sufficient numbers. We try to achieve this by prioritizing candidate negative controls based on prevalence.
\item
  Negative controls should ideally be \textbf{independent}. For example, we should avoid having negative controls that are either ancestors of each other (e.g.~``ingrown nail'' and ``ingrown nail of foot'') or siblings (e.g.~``fracture of left femur'' and ``fracture of right femur'').
\item
  Negative controls should ideally have \textbf{some potential for bias}. For example, the last digit of someone's social security number is basically a random number, and is unlikely to show confounding. It should therefore not be used as a negative control.
\end{itemize}

Some argue that negative controls should also have the same confounding structure as the exposure-outcome pair of interest. \citep{lipsitch_2010} However, we believe this confounding structure is unknowable; The relationships between variables found in reality is often far more complex than people imagine. Also, even if the confounder structure were known, it is unlikely that a negative control exists having that exact same confounding structure, but lacking the direct causal effect. For this reason in OHDSI we rely on a large number of negative controls, assuming that such a set represents many different types of bias, including the ones present in the hypothesis of interest.

The absence of a causal relationship between an exposure and an outcome is rarely documented. Instead, we often make the assumption that a lack of evidence of a relationship implies the lack of a relationship. This assumption is more likely to hold if the exposure and outcome have both been studies extensively, so a relationship could have been detected. For example, the lack of evidence for a completely novel drug likely implies a lack of knowledge, not the lack of a relationship. With this Principe in mind we have developed a semi-automated procedure for selecting negative controls \citep{voss_2016}. In brief, information from literature, product labels, and spontaneous reporting is automatically extracted and synthesized to produce a candidate list of negative controls. This list must then undergo manual review, not only to verify that the automated extraction was accurate, but also to impose additional criteria such as biological plausibility.

\hypertarget{PositiveControls}{%
\subsection{Positive controls}\label{PositiveControls}}

To understand the behavior of a method when the true relative risk is smaller or greater than one requires the use of positive controls, where the null is believed to not be true. Unfortunately, real positive controls for observational research tend to be problematic for three reasons. First, in most research contexts, for example when comparing the effect of two treatments, there is a paucity of positive controls relevant for that specific context. Second, even if positive controls are available, the magnitude of the effect size may not be known with great accuracy, and often depends on the population in which one measures it. Third, when treatments are widely known to cause a particular outcome, this shapes the behavior of physicians prescribing the treatment, for example by taking actions to mitigate the risk of unwanted outcomes, thereby rendering the positive controls useless as a means for evaluation. \citep{noren_2014}

In OHDSI we therefore use synthetic positive controls, \citep{schuemie_2018} created by modifying a negative control through injection of additional, simulated occurrences of the outcome during the time at risk of the exposure. For example, assume that, during exposure to ACEi, n occurrences of our negative control outcome ``ingrowing nail'' were observed. If we now add an additional n simulated occurrences during exposure, we have doubled the risk. Since this was a negative control, the relative risk compared to the counterfactual was one, but after injection, it becomes two.

One issue that stands important is the preservation of confounding. The negative controls may show strong confounding, but if we inject additional outcomes randomly, these new outcomes will not be confounded, and we may therefore be optimistic in our evaluation of our capacity to deal with confounding for positive controls. To preserve confounding, we want the new outcomes to show similar associations with baseline subject-specific covariates as the original outcomes. To achieve this, for each outcome we train a model to predict the survival rate with respect to the outcome during exposure using covariates captured prior to exposure. These covariates include demographics, as well as all recorded diagnoses, drug exposures, measurements, and medical procedures. An L1-regularized Poisson regression \citep{suchard_2013} using 10-fold cross-validation to select the regularization hyperparameter fits the prediction model. We then use the predicted rates to sample simulated outcomes during exposure to increase the true effect size to the desired magnitude. The resulting positive control thus contains both real and simulated outcomes.

Figure \ref{fig:posControlSynth} depicts this process. Note that although this procedure simulates several important sources of bias, it does not capture all. For example, some effects of measurement error are not present. The synthetic positive controls imply constant positive predictive value and sensitivity, which may not be true in reality.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/MethodValidity/posControlSynth} 

}

\caption{Synthesizing positive controls from negative controls.}\label{fig:posControlSynth}
\end{figure}

Although we refer to a single true ``effect size'' for each control, different methods estimate different statistics of the treatment effect. For negative controls, where we believe no causal effect exists, all such statistics, including the relative risk, hazard ratio, odds ratio, incidence rate ratio, both conditional and marginal, as well as the average treatment effect in the treated (ATT) and the overall average treatment effect (ATE) will be identical to 1. Our process for creating positive controls synthesizes outcomes with a constant incidence rate ratio over time and between patients, using a model conditioned on the patient where this ratio is held constant, up to the point where the marginal effect is achieved. The true effect size is thus guaranteed to hold as the marginal incidence rate ratio in the treated. Under the assumption that our outcome model used during synthesis is correct, this also holds for the conditional effect size and the ATE. Since all outcomes are rare, odds ratios are all but identical to the relative risk.

\hypertarget{metrics}{%
\subsection{Empirical evaluation}\label{metrics}}

Based on the estimates of a particular method for the negative and positive controls, we can then understand the operating characteristic by computing a range of metrics, for example:

\begin{itemize}
\tightlist
\item
  \textbf{Area Under the receiver operator Curve (AUC)}: the ability to discriminate between positive and negative controls.
\item
  \textbf{Coverage}: how often the true effect size is within the 95\% confidence interval.
\item
  \textbf{Mean precision}: precision is computed as \(1/(standard\ error)^2\), higher precision means narrower confidence intervals. We use the geometric mean to account for the skewed distribution of the precision.
\item
  \textbf{Mean squared error (MSE)}: Mean squared error between the log of the effect size point-estimate and the log of the true effect size.
\item
  \textbf{Type 1 error}: For negative controls, how often was the null rejected (at \(\alpha = 0.05\)). This is equivalent to the false positive rate and \(1 - specificity\).
\item
  \textbf{Type 2 error}: For positive controls, how often was the null not rejected (at \(\alpha = 0.05\)). This is equivalent to the false negative rate and \(1 - sensitivity\).
\item
  \textbf{Non-estimable}: For how many of the controls was the method unable to produce an estimate? There can be various reasons why an estimate cannot be produced, for example because there were no subjects left after propensity score matching, or because no subjects remained having the outcome.
\end{itemize}

Depending on our use case, we can evaluate whether these operating characteristics are suitable for our goal. For example, if we wish to perform signal detection, we may care about type 1 and type 2 error, or if we are willing to modify our \(\alpha\) threshold, we may inspect the AUC instead.

\hypertarget{p-value-calibration}{%
\subsection{P-value calibration}\label{p-value-calibration}}

Often the type 1 error (at \(\alpha = 0.05\)) is larger than 5\%. In other words, we are often more likely than 5\% to reject the null hypothesis when in fact the null hypothesis is true. The reason is that the p-value only reflects random error, the error due to having a limited sample size. It does not reflect systematic error, for example the error due to confounding. OHDSI has developed a process for calibrating p-values to restore the type 1 error to nominal. \citep{schuemie_2014} We derive an empirical null distribution from the actual effect estimates for the negative controls. These negative control estimates give us an indication of what can be expected when the null hypothesis is true, and we use them to estimate an empirical null distribution.

Formally, we fit a Gaussian probability distribution to the estimates, taking into account the sampling error of each estimate. Let \(\hat{\theta}_i\) denote the estimated log effect estimate (relative risk, odds or incidence rate ratio) from the \(i\)th negative control drug--outcome pair, and let \(\hat{\tau}_i\) denote the corresponding estimated standard error, \(i=1,\ldots,n\). Let \(\theta_i\) denote the true log effect size (assumed 0 for negative controls), and let \(\beta_i\) denote the true (but unknown) bias associated with pair \(i\) , that is, the difference between the log of the true effect size and the log of the estimate that the study would have returned for control \(i\) had it been infinitely large. As in the standard p-value computation, we assume that \(\hat{\theta}_i\) is normally distributed with mean \(\theta_i + \beta_i\) and standard deviation \(\hat{\tau}_i^2\). Note that in traditional p-value calculation, \(\beta_i\) is always assumed to be equal to zero, but that we assume the \(\beta_i\)'s, arise from a normal distribution with mean \(\mu\) and variance \(\sigma^2\). This represents the null (bias) distribution. We estimate \(\mu\) and \(\sigma^2\) via maximum likelihood. In summary, we assume the following:

\[\beta_i \sim N(\mu,\sigma^2) \text{  and} \\ \hat{\theta}_i \sim N(\theta_i + \beta_i, \tau_i^2)\]

where \(N(a,b)\) denotes a Gaussian distribution with mean \(a\) and variance \(b\), and estimate \(\mu\) and \(\sigma^2\) by maximizing the following likelihood:

\[L(\mu, \sigma | \theta, \tau) \propto \prod_{i=1}^{n}\int p(\hat{\theta}_i|\beta_i, \theta_i, \hat{\tau}_i)p(\beta_i|\mu, \sigma) \text{d}\beta_i\]

yielding maximum likelihood estimates \(\hat{\mu}\) and \(\hat{\sigma}\). We compute a calibrated p-value that uses the empirical null distribution. Let \(\hat{\theta}_{n+1}\) denote the log of the effect estimate from a new drug--outcome pair, and let \(\hat{\tau}_{n+1}\) denote the corresponding estimated standard error. From the aforementioned assumptions and assuming \(\beta_{n+1}\) arises from the same null distribution, we have the following:

\[\hat{\theta}_{n+1} \sim N(\hat{\mu}, \hat{\sigma} + \hat{\tau}_{n+1})\]

When \(\hat{\theta}_{n+1}\) is smaller than \(\hat{\mu}\), the one-sided calibrated p-value for the new pair is then

\[\phi\left(\frac{\theta_{n+1} - \hat{\mu}}{\sqrt{\hat{\sigma}^2 + \hat{\tau}_{n+1}^2}}\right)\]

where \(\phi(\cdot)\) denotes the cumulative distribution function of the standard normal distribution. When \(\hat{\theta}_{n+1}\) is bigger than \(\hat{\mu}\), the one-sided calibrated p-value is then

\[1-\phi\left(\frac{\theta_{n+1} - \hat{\mu}}{\sqrt{\hat{\sigma}^2 + \hat{\tau}_{n+1}^2}}\right)\]

\hypertarget{confidence-interval-calibration}{%
\subsection{Confidence interval calibration}\label{confidence-interval-calibration}}

Similarly, we typically observe that the coverage of the 95\% confidence interval is less than 95\%: the true effect size is inside the 95\% confidence interval less than 95\% of the time. For confidence interval calibration \citep{schuemie_2018} we extend the framework for p-value calibration by also making use of our positive controls. Typically, but not necessarily, the calibrated confidence interval is wider than the nominal confidence interval, reflecting the problems unaccounted for in the standard procedure (such as unmeasured confounding, selection bias and measurement error) but accounted for in the calibration.

Formally, we assume that \(beta_i\), the bias associated with pair \(i\), again comes from a Gaussian distribution, but this time using a mean and standard deviation that are linearly related to \(theta_i\), the true effect size:

\[\beta_i \sim N(\mu(\theta_i) , \sigma^2(\theta_i))\]

where

\[\mu(\theta_i) = a + b \times \theta_i \text{ and} \\
  \sigma(\theta_i) ^2= c + d \times \mid \theta_i \mid\]

We estimate \(a\), \(b\), \(c\) and \(d\) by maximizing the marginalized likelihood in which we integrate out the unobserved \(\beta_i\):

\[l(a,b,c,d | \theta, \hat{\theta}, \hat{\tau} ) \propto \prod_{i=1}^{n}\int p(\hat{\theta}_i|\beta_i, \theta_i, \hat{\tau}_i)p(\beta_i|a,b,c,d,\theta_i) \text{d}\beta_i ,\]

yielding maximum likelihood estimates \((\hat{a}, \hat{b}, \hat{c}, \hat{d})\).

We compute a calibrated CI that uses the systematic error model. Let \(\hat{\theta}_{n+1}\) again denote the log of the effect estimate for a new outcome of interest, and let \(\hat{\tau}_{n+1}\) denote the corresponding estimated standard error. From the assumptions above, and assuming \(\beta_{n+1}\) arises from the same systematic error model, we have:

\[\hat{\theta}_{n+1} \sim N(
\theta_{n+1} + \hat{a} + \hat{b} \times \theta_{n+1},
\hat{c} + \hat{d} \times \mid \theta_{n+1} \mid) + \hat{\tau}_{n+1}^2) .\]

We find the lower bound of the calibrated 95\% CI by solving this equation for \(\theta_{n+1}\):

\[\Phi\left(
\frac{\theta_{n+1} + \hat{a} + \hat{b} \times \theta_{n+1}-\hat{\theta}_{n+1}}
{\sqrt{(\hat{c} + \hat{d} \times \mid \theta_{n+1} \mid) + \hat{\tau}_{n+1}^2}}
\right) = 0.025 ,\]

where \(\Phi(\cdot)\) denotes the cumulative distribution function of the standard normal distribution. We find the upper bound similarly for probability 0.975. We define the calibrated point estimate by using probability 0.5.

Both p-value calibration and confidence interval calibration are implemented in the \href{https://ohdsi.github.io/EmpiricalCalibration/}{EmpiricalCalibration} package.

\hypertarget{replication-across-sites}{%
\subsection{Replication across sites}\label{replication-across-sites}}

Another form of method validation comes from executing the study across several different databases that represent different populations, different health care systems, and/or different data capture processes. Prior research has shown that executing the same study design across different databases can produce vastly different effect size estimates, \citep{madigan_2013} suggesting that either the effect differs greatly for different populations, or that the design does not adequately address the different biases found in the different databases. In fact, we observe that accounting for residual bias in a database through empirical calibration of confidence intervals can greatly reduce between-study heterogeneity. \citep{schuemie_2018}

One way to express between-database heterogeneity is the \(I^2\) score, describing the percentage of total variation across studies that is due to heterogeneity rather than chance. \citep{higgins_2003} A naive categorization of values for \(I^2\) would not be appropriate for all circumstances, although one could tentatively assign adjectives of low, moderate, and high to \(I^2\) values of 25\%, 50\%, and 75\%. In a study estimating the effects for many depression treatments using a new-user cohort design with large-scale propensity score adjustment, \citet{schuemie_2018b} observed only 58\% of the estimates to have an \(I^2\) below 25\%. After empirical calibration this increased to 83\%.

\BeginKnitrBlock{rmdimportant}
Observing between-database heterogeneity casts doubt on the validity of the estimates. Unfortunately, the inverse is not true. Not observing heterogeneity does not guarantee an unbiased estimate. It is not unlikely that all databases share a similar bias, and that all estimates are therefore consistently wrong.
\EndKnitrBlock{rmdimportant}

\hypertarget{sensitivity-analyses}{%
\subsection{Sensitivity analyses}\label{sensitivity-analyses}}

When designing a study there are often design choices that are uncertain. For example, should propensity score matching of stratification be used? If stratification is used, how many strata? What is the appropriate time-at-risk? When faced with such uncertainty, one solution is to evaluate various options, and observe the sensitivity of the results to the design choice. If the estimate remains the same under various options, we can say the study is robust to the uncertainty.

This definition of sensitivity analysis should not be confused with the definitions used by others such as \citet{rosenbaum_2005}, who define sensitivity analysis to ``appraise how the conclusions of a study might be altered by hidden biases of various magnitudes''.

\hypertarget{method-validation-in-practice}{%
\section{Method validation in practice}\label{method-validation-in-practice}}

Here we build on the example in Chapter \ref{PopulationLevelEstimation}, where we investigate the effect of ACE inhibitors (ACEi) on the risk of angioedema and acute myocardial infarction (AMI), compared to thiazides and thiazide-like diuretics (THZ). In that chapter we already explore many of the diagnostics specific to the design we used: the cohort method. Here, we apply additional diagnostics that could also have been applied had other designs been used. If the study is implemented using ATLAS as described in Section \ref{PleAtlas} these diagnostics are available in the Shiny app that is included in the study R package generated by ATLAS. If the study is implemented using R instead, as described in Section \ref{pleR}, then R functions available in the various packages should be used, as described in the next sections.

\hypertarget{selecting-negative-controls}{%
\subsection{Selecting negative controls}\label{selecting-negative-controls}}

We must select negative controls, exposure-outcome pairs where no causal effect is believed to exist. For comparative effect estimation such as our example study, we select negative control outcomes that are believed to be neither caused by the target nor the comparator exposure. We want enough negative controls to make sure we have a diverse mix of biases represented in the controls, and also to allow empirical calibration. As a rule-of-thumb we typically aim to have 50-100 such negative controls. We could come up with these controls completely manually, but fortunately ATLAS provides features to aid the selection of negative controls using data from literature, product labels, and spontaneous reports.

To generate a candidate list of negative controls, we first must create a concept set containing all exposures of interest. In this case we select all ingredients in the ACEi and THZ classes, as shown in Figure \ref{fig:exposuresConceptSet}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/MethodValidity/exposuresConceptSet} 

}

\caption{A concept set containing the concepts defining the target and comparator exposures.}\label{fig:exposuresConceptSet}
\end{figure}

Next, we go to the ``Explore Evidence'' tab, and click on the \includegraphics{images/MethodValidity/generate.png} button. Generating the evidence overview will take a few minutes, after which you can click on the \includegraphics{images/MethodValidity/viewEvidence.png} button. This will open the list of outcomes as shown in Figure \ref{fig:candidateNcs}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/MethodValidity/candidateNcs} 

}

\caption{Candidate control outcomes with an overview of the evidence found in literature, product labels, and spontaneous reports.}\label{fig:candidateNcs}
\end{figure}

This list shows condition concepts, along with an overview of the evidence linking the condition to any of the exposures we defined. For example, we see the number of publications that link the exposures to the outcomes found in PubMed using various strategies, the number of product labels of our exposures of interest that list the condition as a possible adverse effect, and the number of spontaneous reports. By default the list is sorted to show candidate negative controls first. It is then sorted by the ``Sort Order'', which represents the prevalence of the condition in a collection of observational databases. The higher the Sort Order, the higher the prevalence. Although the prevalence in these databases might not correspond with the prevalence in the database we wish to run the study, it is likely a good approximation.

The next step is to manually review the candidate list, typically starting at the top, so with the most prevalent condition, and working our way down until we are satisfied we have enough. One typical way to do this is to export the list to a CSV (comma separated values) file, and have clinicians review these, considering the criteria mentioned in Section \ref{NegativeControls}.

For our example study we select the 76 negative controls listed in Appendix \ref{AceiThzNsc}.

\hypertarget{including-controls}{%
\subsection{Including controls}\label{including-controls}}

Once we have defined our set of negative controls we must include them in our study. First we must define some logic for turning our negative control condition concepts into outcome cohorts. Section \ref{evaluationSettings} discusses how ATLAS allows creating such cohorts based on a few choices the user must make. Often we simply choose to create a cohort based on any occurrence of a negative control concept or any of its descendants. If the study is implemented in R then SQL (Structured Query Language) can be used to construct the negative control cohorts. Chapter \ref{SqlAndR} describes how cohorts can be created using SQL and R. We leave it as an exercise for the reader to write the appropriate SQL and R.

The OHDSI tools also provide functionality for automatically generating and including positive controls derived from the negative controls. This functionality can be found in the Evaluation Settings section in ATLAS described in Section \ref{evaluationSettings}, and is implemented in the \texttt{injectSignals} function in the \href{https://ohdsi.github.io/MethodEvaluation/}{MethodEvaluation} package. Here we generate three positive controls for each negative control, with true effect sizes of 1.5, 2, and 4, using a survival model:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MethodEvaluation)}
\CommentTok{# Create a data frame with all negative control exposure-}
\CommentTok{# outcome pairs, using only the target exposure (ACEi = 1).}
\NormalTok{eoPairs <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{exposureId =} \DecValTok{1}\NormalTok{,}
                      \DataTypeTok{outcomeId =}\NormalTok{ ncs)}

\NormalTok{pcs <-}\StringTok{ }\KeywordTok{injectSignals}\NormalTok{(}\DataTypeTok{connectionDetails =}\NormalTok{ connectionDetails,}
                     \DataTypeTok{cdmDatabaseSchema =}\NormalTok{ cdmDbSchema,}
                     \DataTypeTok{exposureDatabaseSchema =}\NormalTok{ cohortDbSchema,}
                     \DataTypeTok{exposureTable =}\NormalTok{ cohortTable,}
                     \DataTypeTok{outcomeDatabaseSchema =}\NormalTok{ cohortDbSchema,}
                     \DataTypeTok{outcomeTable =}\NormalTok{ cohortTable,}
                     \DataTypeTok{outputDatabaseSchema =}\NormalTok{ cohortDbSchema,}
                     \DataTypeTok{outputTable =}\NormalTok{ cohortTable,}
                     \DataTypeTok{createOutputTable =} \OtherTok{FALSE}\NormalTok{,}
                     \DataTypeTok{modelType =} \StringTok{"survival"}\NormalTok{,}
                     \DataTypeTok{firstExposureOnly =} \OtherTok{TRUE}\NormalTok{,}
                     \DataTypeTok{firstOutcomeOnly =} \OtherTok{TRUE}\NormalTok{,}
                     \DataTypeTok{removePeopleWithPriorOutcomes =} \OtherTok{TRUE}\NormalTok{,}
                     \DataTypeTok{washoutPeriod =} \DecValTok{365}\NormalTok{,}
                     \DataTypeTok{riskWindowStart =} \DecValTok{1}\NormalTok{,}
                     \DataTypeTok{riskWindowEnd =} \DecValTok{0}\NormalTok{,}
                     \DataTypeTok{addExposureDaysToEnd =} \OtherTok{TRUE}\NormalTok{,}
                     \DataTypeTok{exposureOutcomePairs =}\NormalTok{ eoPairs,}
                     \DataTypeTok{effectSizes =} \KeywordTok{c}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{),}
                     \DataTypeTok{cdmVersion =}\NormalTok{ cdmVersion,}
                     \DataTypeTok{workFolder =} \KeywordTok{file.path}\NormalTok{(outputFolder,}
                                            \StringTok{"pcSynthesis"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Note that we must mimic the time-at-risk settings used in our estimation study design. The \texttt{injectSignals} function will extract information about the exposures and negative controls outcomes, fit outcome models per exposure-outcome pair, and synthesize outcomes. The positive control outcome cohorts will be added to the cohort table specified by \texttt{cohortDbSchema} and \texttt{cohortTable}. The resulting \texttt{pcs} data frame contains the information on the synthesized positive controls.

Next we must execute the same study used to estimate the effect of interest to also estimate effects for the negative and positive controls. Setting the set of negative controls in the comparisons dialog in ATLAS instructs ATLAS to compute estimates for these controls. Similarly, specifying that positive controls be generated in the Evaluation Settings includes these in our analysis. In R, the negative and positive controls should be treated as any other outcome. All estimation packages in the \href{https://ohdsi.github.io/MethodsLibrary/}{OHDSI Methods Library} readily allow estimation of many effects in an efficient manner.

\hypertarget{empirical-performance}{%
\subsection{Empirical performance}\label{empirical-performance}}

Figure \ref{fig:controls} shows the estimated effect sizes for the negative and positive controls included in our example study, stratified by true effect size. This plot is included in the Shiny app that comes with the study R package generated by ATLAS, and can be generated using the \texttt{plotControls} function in the \href{https://ohdsi.github.io/MethodEvaluation/}{MethodEvaluation} package. Note that the number of controls is often lower than what was defined because there was not enough data to either produce an estimate, or to synthesize a positive control.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/MethodValidity/controls} 

}

\caption{Estimates for the negative (true hazard ratio = 1) and positive controls (true hazard ratio > 1). Each dot represents a control. Estimates below the dashed line have a confidence interval that doesn't include the true effect size.}\label{fig:controls}
\end{figure}

Based on these estimates we can compute the metrics shown in Table \ref{tab:exampleMetrics} using the \texttt{computeMetrics} function in the \href{https://ohdsi.github.io/MethodEvaluation/}{MethodEvaluation} package.

\begin{longtable}[]{@{}lr@{}}
\caption{\label{tab:exampleMetrics} Method performance metrics derived from the negative and positive control estimates.}\tabularnewline
\toprule
Metric & Value\tabularnewline
\midrule
\endfirsthead
\toprule
Metric & Value\tabularnewline
\midrule
\endhead
AUC & 0.96\tabularnewline
Coverage & 0.97\tabularnewline
Mean Precision & 19.33\tabularnewline
MSE & 2.08\tabularnewline
Type 1 error & 0.00\tabularnewline
Type 2 error & 0.18\tabularnewline
Non-estimable & 0.08\tabularnewline
\bottomrule
\end{longtable}

We see that coverage and type 1 error are very close to their nominal values of 95\% and 5\%, respectively, and that the AUC is very high. This is certainly not always the case.

Note that although in Figure \ref{fig:controls} not all confidence intervals include one when the true hazard ratio is one, the type 1 error in Table \ref{tab:exampleMetrics} is 0\%. This is an exceptional situation, caused by the fact that confidence intervals in the \href{https://ohdsi.github.io/Cyclops/}{Cyclops} package are estimated using likelihood profiling, which is more accurate than traditional methods but can result in asymmetric confidence intervals. The p-value instead is computed assuming symmetrical confidence intervals, and this is what was used to compute the type 1 error.

\hypertarget{p-value-calibration-1}{%
\subsection{P-value calibration}\label{p-value-calibration-1}}

We can use the estimates for our negative controls to calibrate our p-values. This is done automatically in the Shiny app, and can be done manually in R. Assuming we have created the summary object \texttt{summ} as described in Section \ref{MultipleAnalyses}, we can plot the empirical calibration effect plot:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Estimates for negative controls (ncs) and outcomes of interest (ois):}
\NormalTok{ncEstimates <-}\StringTok{ }\NormalTok{summ[summ}\OperatorTok{$}\NormalTok{outcomeId }\OperatorTok{%in%}\StringTok{ }\NormalTok{ncs, ]}
\NormalTok{oiEstimates <-}\StringTok{ }\NormalTok{summ[summ}\OperatorTok{$}\NormalTok{outcomeId }\OperatorTok{%in%}\StringTok{ }\NormalTok{ois, ]}

\KeywordTok{library}\NormalTok{(EmpiricalCalibration)}
\KeywordTok{plotCalibrationEffect}\NormalTok{(}\DataTypeTok{logRrNegatives =}\NormalTok{ ncEstimates}\OperatorTok{$}\NormalTok{logRr,}
                      \DataTypeTok{seLogRrNegatives =}\NormalTok{ ncEstimates}\OperatorTok{$}\NormalTok{seLogRr,}
                      \DataTypeTok{logRrPositives =}\NormalTok{ oiEstimates}\OperatorTok{$}\NormalTok{logRr,}
                      \DataTypeTok{seLogRrPositives =}\NormalTok{ oiEstimates}\OperatorTok{$}\NormalTok{seLogRr,}
                      \DataTypeTok{showCis =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/MethodValidity/pValueCal} 

}

\caption{P-value calibration: estimates below the dashed line have a conventional p < 0.05. Estimates in the orange area have calibrated p < 0.05. The pink area denotes the 95\% credible interval around the edge of the orange area. Blue dots indicate negative controls. Yellow diamonds indicate outcomes of interest.}\label{fig:pValueCal}
\end{figure}

In Figure \ref{fig:pValueCal} we see that the orange area almost exactly overlaps with the area denoted by the dashed lines, indicated hardly any bias was observed for the negative controls. One of the outcomes of interest (AMI) is above the dashed line and the orange area, indicating we cannot reject the null according to both the uncalibrated and calibrated p-value. The other outcome (angioedema) clearly stands out from the negative control, and falls well within the area where both uncalibrated and calibrated p-values are smaller than 0.05.

We can compute the calibrated p-values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null <-}\StringTok{ }\KeywordTok{fitNull}\NormalTok{(}\DataTypeTok{logRr =}\NormalTok{ ncEstimates}\OperatorTok{$}\NormalTok{logRr,}
                \DataTypeTok{seLogRr =}\NormalTok{ ncEstimates}\OperatorTok{$}\NormalTok{seLogRr)}
\KeywordTok{calibrateP}\NormalTok{(null,}
           \DataTypeTok{logRr=}\NormalTok{ oiEstimates}\OperatorTok{$}\NormalTok{logRr,}
           \DataTypeTok{seLogRr =}\NormalTok{ oiEstimates}\OperatorTok{$}\NormalTok{seLogRr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.604351e-06 7.159506e-01
\end{verbatim}

And contrast these with the uncalibrated p-values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oiEstimates}\OperatorTok{$}\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] [1] 1.483652e-06 7.052822e-01
\end{verbatim}

As expected, because little to no bias was observed, the uncalibrated and calibrated p-values are very similar.

\hypertarget{confidence-interval-calibration-1}{%
\subsection{Confidence interval calibration}\label{confidence-interval-calibration-1}}

Similarly, we can use the estimates for our negative and positive controls to calibrate the confidence intervals. The Shiny app automatically reports the calibrate confidence intervals. In R we can calibrate intervals using the \texttt{fitSystematicErrorModel} and \texttt{calibrateConfidenceInterval} functions in the \href{https://ohdsi.github.io/EmpiricalCalibration/}{EmpiricalCalibration} package, as described in detail in the \href{https://ohdsi.github.io/EmpiricalCalibration/articles/EmpiricalCiCalibrationVignette.html}{``Empirical calibration of confidence intervals'' vignette}.

Before calibration, the estimated hazard ratios (95\% confidence interval) are 4.32 (2.45 - 8.08) and 1.13 (0.59 - 2.18), for angioedema and AMI respectively. The calibrated hazard ratios are 4.75 (2.52 - 9.04) and 1.15 (0.58 - 2.30).

\hypertarget{between-database-heterogeneity}{%
\subsection{Between-database heterogeneity}\label{between-database-heterogeneity}}

Just as we executed our analysis on one database, in this case the IBM MarketScan Medicaid (MDCD) database, we can also run the same analysis code on other databases that adhere to the Common Data Model (CDM). Figure \ref{fig:forest} shows the forest plot and meta-analytic estimates (assuming random effects) \citep{dersimonian_1986} across a total of five databases for the outcome of angioedema. This figure was generated using the \texttt{plotMetaAnalysisForest} function in the \href{https://ohdsi.github.io/EvidenceSynthesis/}{EvidenceSynthesis} package.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/MethodValidity/forest} 

}

\caption{Effect size estimates and 95\% confidence intervals (CI) from five different databases and a meta-analytic estimatewhen comparing ACE inhibitors to thiazides and thiazide-like diuretics for the risk of angioedema.}\label{fig:forest}
\end{figure}

Although all confidence intervals are above one, suggesting agreement on the fact that there is an effect, the \(I^2\) suggests between-database heterogeneity. However, if we compute the \(I^2\) using the calibrated confidence intervals as shown in Figure \ref{fig:forestCal}, we see that this heterogeneity can be explained by the bias measured in each database through the negative and positive controls. The empirical calibration appears to properly taken this bias into account.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/MethodValidity/forestCal} 

}

\caption{Calibrated Effect size estimates and 95\% confidence intervals (CI) from five different databases and a meta-analytic estimate for the hazard ratio of angioedema when comparing ACE inhibitors to thiazides and thiazide-like diuretics.}\label{fig:forestCal}
\end{figure}

\hypertarget{sensitivity-analyses-1}{%
\subsection{Sensitivity analyses}\label{sensitivity-analyses-1}}

One of the design choices in our analysis was to use variable-ratio matching on the propensity score. However, we could have also used stratification on the propensity score. Because we we are uncertain about this choice, we may decide to use both. Table \ref{tab:sensAnalysis} shows the effect size estimates for AMI and angioedema, both calibrated and uncalibrated, when using variable-ratio matching and stratification (with 10 equally-sized strata).

\begin{longtable}[]{@{}llll@{}}
\caption{\label{tab:sensAnalysis} Uncalibrated and calibrated hazard ratios (95\% confidence interval) for the two analysis variants.}\tabularnewline
\toprule
\begin{minipage}[b]{0.32\columnwidth}\raggedright
Outcome\strut
\end{minipage} & \begin{minipage}[b]{0.17\columnwidth}\raggedright
Adjustment\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright
Uncalibrated\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright
Calibrated\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.32\columnwidth}\raggedright
Outcome\strut
\end{minipage} & \begin{minipage}[b]{0.17\columnwidth}\raggedright
Adjustment\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright
Uncalibrated\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright
Calibrated\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.32\columnwidth}\raggedright
Angioedema\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright
Matching\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
4.32 (2.45 - 8.08)\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
4.75 (2.52 - 9.04)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright
Angioedema\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright
Stratification\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
4.57 (3.00 - 7.19)\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
4.52 (2.85 - 7.19)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright
Acute myocardial infarction\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright
Matching\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
1.13 (0.59 - 2.18)\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
1.15 (0.58 - 2.30)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright
Acute myocardial infarction\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright
Stratification\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
1.43 (1.02 - 2.06)\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
1.45 (1.03 - 2.06)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

We see that the estimates from the matched and stratified analysis are in strong agreement, with the confidence intervals for stratification falling completely inside of the confidence intervals for matching. This suggests that our uncertainty around this design choice does not impact the validity of our estimates. Stratification does appear to give us more power (narrower confidence intervals), which is not surprising since matching results in loss of data, whereas stratification does not. The price for this could be an increase in bias, due to within-strata residual confounding, although we see no evidence of increased bias reflected in the calibrated confidence intervals.

\BeginKnitrBlock{rmdimportant}
Study diagnostics allow us to evaluate design choices even before fully executing a study. It is recommended not to finalize the protocol before generating and reviewing all study diagnostics. To avoid p-hacking (adjusting the design to achieve a desired result), this should be done while blinded to the effect size estimate of interest.
\EndKnitrBlock{rmdimportant}

\hypertarget{ohdsi-methods-benchmark}{%
\section{OHDSI Methods Benchmark}\label{ohdsi-methods-benchmark}}

Although the recommended practice is to empirically evaluate a method's performance within the context that it is applied, using negative and positive controls that are in ways similar to the exposures-outcomes pairs of interest (for example using the same exposure or the same outcome) and on the database used in the study, there is also value in evaluating a method's performance in general. This is why the OHDSI Methods Evaluation Benchmark was developed. The benchmark evaluates performance using a wide range of control questions, including those with chronic or acute outcomes, and long-term or short-term exposures. The results on this benchmark can help demonstrate the overall usefulness of a method, and can be used to form a prior belief about the performance of a method when a context-specific empirical evaluation is not (yet) available. The benchmark consists of 200 carefully selected negative controls that can be stratified into eight categories, with the controls in each category either sharing the same exposure or the same outcome. From these 200 negative controls, 600 synthetic positive controls are derived as described in Section \ref{PositiveControls}. To evaluate a method, it must be used to produce effect size estimates for all controls, after which the metrics described in Section \ref{metrics} can be computed. The benchmark is publicly available, and can be deployed as described in the \href{https://ohdsi.github.io/MethodEvaluation/articles/OhdsiMethodsBenchmark.html}{Running the OHDSI Methods Benchmark vignette} in the \href{https://ohdsi.github.io/MethodEvaluation/}{MethodEvaluation} package.

We have run all the methods in the OHDSI Methods Library through this benchmark, with various analysis choices per method. For example, the cohort method was evaluated using propensity score matching, stratification, and weighting. This experiment was executed on four large observational healthcare databases. The results, viewable in an online Shiny app\footnote{\url{http://data.ohdsi.org/MethodEvalViewer/}}, show that although several methods show high AUC (the ability to distinguish positive controls from negative controls), most methods in most settings demonstrate high type 1 error and low coverage of the 95\% confidence interval, as shown in Figure \ref{fig:methodEval}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/MethodValidity/methodEval} 

}

\caption{Coverage of the 95\% confidence interval for the methods in the Methods Library. Each dot represents the performance of a specific set of analysis choices. The dashed line indicates nominal performance (95\% coverage). SCCS = Self-Controlled Case Series, GI = Gastrointestinal, IBD = inflammatory bowel disease.}\label{fig:methodEval}
\end{figure}

This emphasizes the need for empirical evaluation and calibration: if no empirical evaluation is performed, which is true for almost all published observational studies, we must assume a prior informed by the results in Figure \ref{fig:methodEval}, and conclude that it is likely that the true effect size is not contained in the 95\% confidence interval!

Our evaluation of the designs in the Methods Library also shows that empirical calibration restores type 1 error and coverage to their nominal values, although often at the cost of increasing type 2 error and decreasing precision.

\hypertarget{summary-7}{%
\section{Summary}\label{summary-7}}

\BeginKnitrBlock{rmdsummary}
\begin{itemize}
\item
  A method's validity depends on whether the assumptions underlying the method are met.
\item
  Where possible, these assumptions should be empirically tested using study diagnostics.
\item
  Control hypotheses, questions where the answer is known, should be used to evaluate whether a specific study design produces answers in line with the truth.
\item
  Often, p-values and confidence intervals do not demonstrate nominal characteristics as measured using control hypotheses.
\item
  These characteristics can often be restored to nominal using empirical calibration.
\item
  Study diagnostics can be used to guide analytic design choices and adapt the protocol, as long as the researcher remains blinded to the effect of interest to avoid p-hacking.
\end{itemize}
\EndKnitrBlock{rmdsummary}

\hypertarget{exercises-4}{%
\section{Exercises}\label{exercises-4}}

Todo

\hypertarget{part-ohdsi-studies}{%
\part{OHDSI Studies}\label{part-ohdsi-studies}}

\hypertarget{StudySteps}{%
\chapter{Study steps}\label{StudySteps}}

Writing the protocol, OHDSI style: \url{http://www.ohdsi.org/web/wiki/lib/exe/fetch.php?media=projects:workgroups:wg_study_protocols_eastern_hemisphere.pptx}

Study reproducibility (Martijn has some slides that might help: \url{http://www.ohdsi.org/web/wiki/lib/exe/fetch.php?media=projects:workgroups:wg_study_reproducability.pptx} )

\hypertarget{NetworkResearch}{%
\chapter{OHDSI Network Research}\label{NetworkResearch}}

Contributors: Greg Klebanov, Kristin Kostka \& Vojtech Huser

The mission of OHDSI is to generate high-quality evidence through observational research. A primary way this is accomplished is through collaborative research studies. In prior chapters we discussed how the OHDSI community has authored standards and tools to facilitate high-quality, reproducible research, including Standardized Vocabularies , the Common Data Model (CDM) , analytical methods packages, ATLAS and the study steps (Chapter\ref{StudySteps}) to run a retrospective database study. OHDSI Network Studies represent the culmination of a transparent, consistent and reproducible way to conduct research across a large number of geographically dispersed data. In this chapter we will discuss what constitutes an OHDSI network study, how to run a network study and discuss enabling technologies such as the ARACHNE Research Network.

\hypertarget{what-is-the-ohdsi-research-network}{%
\section{What is the OHDSI Research Network?}\label{what-is-the-ohdsi-research-network}}

The OHDSI Research Network is an international collaboration of researchers seeking to advance observational data research in healthcare. Today, the network consists of over 1.2 billion patient records (\textasciitilde650 million de-duplicated patient records) in the OMOP CDM. This includes more than 200 researchers and 82 observational health databases across 17 countries with regional central coordinating centers housed at Columbia University (USA), Erasmus Medical Center (Europe) and Ajou University (South Korea). The OHDSI community continues to grow rapidly across Europe (in collaboration with the IMI EHDEN project), Central America (e.g.~Argentina, Brazil, Colombia), and Asia (e.g.~China, Japan, Singapore).

OHDSI is open network, inviting healthcare institutions across the globe with active patient data to join the network and convert data to the OMOP CDM. As OMOP data conversions are complete, collaborators are invited to report site information in the Data Network census maintained by the OHDSI Program Manager (\href{mailto:beaton@ohdsi.org}{\nolinkurl{beaton@ohdsi.org}}). Each OHDSI network site participates voluntarily. There are no hard obligations. Each site opts-in to each respective network study. In each study, data remains at the site behind a firewall. No data pooling across network sites. Only aggregate results are shared.

\BeginKnitrBlock{rmdimportant}
\textbf{Benefits of Joining the OHDSI Network}
Unlock the power of institutional data: Transforming institutional EHR data in the OMOP Common Data Model enables clinical research on populations of your patients, something EHR systems don't support.
Access to free tools: OHDSI publishes free, open source tools for data characterization and analytics (e.g.~browsing the clinical concepts, defining and characterizing cohorts, running Population-Level Estimation and Patient-Level Prediction studies).
Participate in a premier research community: Author and publish network research, gain access to eminent leaders in global real-world evidence community.
Buildout Quality Benchmarks: Network can validate quality improvement benchmarks against other institutions (e.g.~On average how long does it take to get an appendectomy discharged?)
\EndKnitrBlock{rmdimportant}

\hypertarget{what-is-an-ohdsi-network-study}{%
\section{What is an OHDSI Network Study?}\label{what-is-an-ohdsi-network-study}}

In the study steps chapter (Chapter\ref{StudySteps}), we discussed the steps to execute a retrospective database study using the OMOP CDM. A study may be conducted on a single OMOP CDM or on multiple OMOP CDMs. It can be conducted within a single institution's OMOP CDM data or across many institutions. There is no requirement that an OHDSI research study package be shared across the entire OMOP network. In fact, there may be legitimate instances when a study protocol is written for specific clinical practice that cannot be generalized to the entirety of the network. The principal investigator of each OHDSI research study will determine which, if any, sites they would like to include in an analysis.

\BeginKnitrBlock{rmdimportant}
\textbf{When is a study considered a network study?} An OHDSI research project becomes a network study when it is published and shared for execution across the OHDSI community.

Elements of a Network Study:
- Must have a protocol (a description of the analysis to be performed)
- Must have a study code package designed for the OMOP CDM
- Must be executed across two or more network sites (not just 2 or more databases at a single site)
- Encouraged to publish all documentation on GitHub
- At the end of the analysis, the results are made available in Github or other public repository (e.g.~a Shiny Application)
\EndKnitrBlock{rmdimportant}

\hypertarget{executing-an-ohdsi-network-study}{%
\section{Executing an OHDSI Network Study}\label{executing-an-ohdsi-network-study}}

Conducting an OHDSI Network Study requires a substantial amount of preparation to ensure success.

\begin{quote}
''You'll never walk alone in your OHDSI journey.'' - Peter Rijnbeek
\end{quote}

\BeginKnitrBlock{rmdimportant}
\textbf{New to Network Studies?} The OHDSI Study Nurture Committee is a resource for you as you navigate your journey. This committee helps train and guide researchers to complete OHDSI Studies including how to effectively use OHDSI tools, providing guidance to the OHDSI study design for increase reproducibility and reliability and assisting with helping study investigators recruit data partners to run study packages.
\EndKnitrBlock{rmdimportant}

Running an OHDSI Network Study has three distinct stages:

\begin{itemize}
\tightlist
\item
  Study Feasibility and Design
\item
  Study Execution
\item
  Results Dissemination and Publication
\end{itemize}

\hypertarget{study-feasibility-and-design}{%
\subsection{Study Feasibility and Design}\label{study-feasibility-and-design}}

The study feasibility stage \emph{(pre-study stage)} is focused on supporting a definition of a study and a creation of the study protocol, \emph{e.g.~undertaking activities to make sure the study is feasible to be executed as described in the formal protocol.}

The feasibility stage does not have a well-defined process but rather is driven by various supporting activities, including identification and enrollment of relevant databases that contain the targeted patient population with required drug exposure, procedure information, condition or demographics information through data characterization, validating and agreeing on target analytical methods and algorithms. These activities may involve sharing JSON files of cohort definitions from ATLAS and provisional test of study R packages. A study lead may have enough data to do this inside their own organization or may opt for support from other OHDSI network sites.

The outcome of the feasibility stage is generation of a final protocol as well as a list of target collaborators. The formal protocol will detail the study team, including the designated study lead (often the corresponding author for publication purposes), and information on the timeline for the study. The protocol is a critical component for additional network sites to review, approve and execute the study package on their OMOP CDM data. A protocol must include information on study population, the methods being used, how the results will be stored and analyzed as well as how the study results will be disseminated after completion (e.g.~a publication, a poster, etc).

\hypertarget{study-execution}{%
\subsection{Study Execution}\label{study-execution}}

After completing feasibility, a study advances to the execution phase. The key activities in executing a network study include the following:

\begin{itemize}
\tightlist
\item
  The study lead formally initiates a new OHDSI network study with the OHDSI Coordinating Center. \emph{In tandem, this may include undertaking other organization-specific processes to approve an OHDSI study.}
\item
  The study lead publishes the study protocol to the OHDSI GitHub.
\item
  The study lead announces the study on the OHDSI Community Call and OHDSI Forum, inviting participating centers and collaborators.
\item
  Study participating organizations assemble teams within each site, assign study roles (e.g.~data analyst(s) executing the study package, site leadership reviewing the study design and manuscript).
\item
  The data scientist/statisticians for the study lead will use a study protocol to design study analyses and generate study code.
\item
  The data scientist/statisticians will conduct a feasibility test of study code within their own environment. The package will be shared to 1-2 network sites for additional validation.
\item
  The data scientist/statistician will publish the validated study code in the OHDSI GitHub for execution at participating sites.
\item
  Site data scientists/statisticians access the OHDSI study package and generate results in the standardized format following OHDSI guidelines. Each participating site will follow internal institutional processes regarding data sharing rules. \textbf{Sites should not share results unless approval is obtained from IRB or other institutional approval processes.}
\item
  Data scientist/statisticians and Study Lead collect and review the analysis execution results.
\item
  Iterate steps 5-7, if reasonable adjustments required.
\item
  Collaboratively finalize study results. Study lead disseminates study results (e.g.~a Shiny Application).
\item
  Study lead formally closes the study out with OHDSI Coordinating Center.
\end{itemize}

While OHDSI processes can be executed rapidly, it is advised to allow for a few weeks to months for all participating sites to execute the study and receive appropriate approvals to publish results. A study leads should set study milestones and anticipated closure date in advance to assist with managing the overall study timeline.

\hypertarget{results-dissemination-and-publication}{%
\subsection{Results Dissemination and Publication}\label{results-dissemination-and-publication}}

During this stage, the study lead will collaborate with other sites on various administrative tasks, such as manuscript development and optimizing data visualizations.

\BeginKnitrBlock{rmdimportant}
Not sure where to publish your OHDSI network study? Consult JANE (Journal/Author Name Estimator), a tool which takes your abstract and scans publications for relevance and fit (\url{http://jane.biosemantics.org/}).
\EndKnitrBlock{rmdimportant}

Researchers are also invited to present OHDSI Network Studies on weekly OHDSI community calls and at OHDSI Symposiums across the globe.

\hypertarget{types-of-network-studies}{%
\section{Types of Network Studies}\label{types-of-network-studies}}

The network studies can be of different types - ranging from simple characterization questions to more advanced predictive studies. A large number of studies conducted today are focused on epidemiology and drug efficacy and safety and thus carry different type of characterization analyses such as patient population characterization, incidence rates of certain outcomes/conditions, comparative drug effectiveness comparison, prevalence of disease and similar. However, more and more studies cary predictive nature including the probability of an outcome for a certain type of a patient (personalized medicine).

\hypertarget{forward-looking-using-network-study-automation}{%
\section{Forward Looking: Using Network Study Automation}\label{forward-looking-using-network-study-automation}}

The current network study process is manual and rudimentary - with study team members using various mechanisms (including Wiki, GitHub and email) to collaborate on study design, share code and results. This process is not consistent and scalable and to solve that issue, the OHDSI community is actively working to systemize study processes. The ARACHNE Research Network platform is a community-driven solution to streamline and automate the process of network studies.

\begin{figure}
\includegraphics[width=1\linewidth]{images/NetworkStudies/ARACHNE} \caption{The ARACHNE Network Study Process.}\label{fig:arachne}
\end{figure}

The ARACHNE Platform includes multiple core components:

\begin{itemize}
\tightlist
\item
  The \textbf{ARACHNE Data Catalog} where different network participants register and maintain information about data sets available for network research
\item
  The \textbf{Study Workflow Manager} that allows study teams to orchestrate and end to end network study process. The ARACHNE Research Network platform It is taking full advantage of OHDSI standards and establishes a consistent, transparent, secure and compliant observational research process, across multiple organizations. ARACHNE standardizes the communication protocol to access the data and exchange analysis results, while enabling authentication and authorization for restricted content. It brings participating organizations - data providers, investigators, sponsors and data scientists - into a single collaborative study team and facilitates an end-to-end observational study. The tool enables the creation of a complete, standards-based R, Python and SQL execution environment including approval workflows controlled by the data custodian.
\end{itemize}

ARACHNE is built to provide a seamless integration with other OHDSI tools, including ACHILLES reports and an ability to import ATLAS design artefacts, create self-contained packages and automatically execute those across multiple sites. The future vision is to eventually enable multiple networks to be linked together for the purpose of conducting research not only between organizations within a single network, but also between organizations across multiple networks.

\begin{figure}
\includegraphics[width=1\linewidth]{images/NetworkStudies/ARACHNENON} \caption{The ARACHNE Network of Networks.}\label{fig:arachneNon}
\end{figure}

\hypertarget{best-practices-for-network-research}{%
\section{Best Practices for Network Research}\label{best-practices-for-network-research}}

There are multiple best practices that study teams should be following while executing the network study:

\begin{itemize}
\tightlist
\item
  Make sure that study questions can be supported by data available. Perform study feasibility to identify the best databases.
\item
  Write code in generic way and parametrize all functions and variables e.g.~do not hard database connection, local hard drive path, assume a certain operating system.
\item
  Ensure the target databases have required OMOP CDM version and OMOP Standardized Vocabularies.
\item
  Ensure the target database ETL has followed THEMIS business rules and conventions and correct data was placed into correct CDM tables and fields.
\item
  Do not tweak the study code to get desired results
\end{itemize}

\hypertarget{example-legend---hypertension}{%
\section{Example: LEGEND - Hypertension}\label{example-legend---hypertension}}

To be added.

\hypertarget{appendix-appendix}{%
\appendix \addcontentsline{toc}{chapter}{\appendixname}}


\hypertarget{Glossary}{%
\chapter{Glossary}\label{Glossary}}

Cohort
A cohort is a list of person\_ids with start and end date. It is stored in a study specific cohort table or a CDM specified cohort table can also be used. Cohort can be represented as .json file. It is used for import and export but not during an analysis. OHDSI tools use SQL so Atlas also generates a .sql file that creates the cohort during analysis.

Parametized SQL code
An SQL code that allows for use of parameters. Parameters are prefixed with @. Such code has to be ``rendered''. Synonym: OHDSI SQL code.

\hypertarget{CohortDefinitions}{%
\chapter{Cohort definitions}\label{CohortDefinitions}}

This Appendix contains cohort definitions used throughout the book.

\hypertarget{AceInhibitors}{%
\section{ACE inhibitors}\label{AceInhibitors}}

\textbf{Initial Event Cohort}

People having any of the following:

\begin{itemize}
\tightlist
\item
  a drug exposure of \emph{ACE inhibitors} (Table \ref{tab:aceInhibitors}) for the first time in the person's history
\end{itemize}

with continuous observation of at least 365 days prior and 0 days after event index date, and limit initial events to: all events per person.

Limit qualifying cohort to: all events per person.

\textbf{End Date Strategy}

Custom Drug Era Exit Criteria
This strategy creates a drug era from the codes found in the specified concept set. If the index event is found within an era, the cohort end date will use the era's end date. Otherwise, it will use the observation period end date that contains the index event.

Use the era end date of \emph{ACE inhibitors} (Table \ref{tab:aceInhibitors})

\begin{itemize}
\tightlist
\item
  allowing 30 days between exposures
\item
  adding 0 days after exposure end
\end{itemize}

\textbf{Cohort Collapse Strategy}

Collapse cohort by era with a gap size of 30 days.

\textbf{Concept Set Definitions}

\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:aceInhibitors} ACE inhibitors}\tabularnewline
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endfirsthead
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endhead
1308216 & Lisinopril & NO & YES & NO\tabularnewline
1310756 & moexipril & NO & YES & NO\tabularnewline
1331235 & quinapril & NO & YES & NO\tabularnewline
1334456 & Ramipril & NO & YES & NO\tabularnewline
1335471 & benazepril & NO & YES & NO\tabularnewline
1340128 & Captopril & NO & YES & NO\tabularnewline
1341927 & Enalapril & NO & YES & NO\tabularnewline
1342439 & trandolapril & NO & YES & NO\tabularnewline
1363749 & Fosinopril & NO & YES & NO\tabularnewline
1373225 & Perindopril & NO & YES & NO\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{AceInhibitorsMono}{%
\section{New users of ACE inhibitors as first-line monotherapy for hypertension}\label{AceInhibitorsMono}}

\textbf{Initial Event Cohort}

People having any of the following:

\begin{itemize}
\tightlist
\item
  a drug exposure of \emph{ACE inhibitors} (Table \ref{tab:aceInhibitorsMono}) for the first time in the person's history
\end{itemize}

with continuous observation of at least 365 days prior and 0 days after event index date, and limit initial events to: earliest event per person.

\textbf{Inclusion Rules}

Inclusion Criteria \#1: has hypertension diagnosis in 1 yr prior to treatment

Having all of the following criteria:

\begin{itemize}
\tightlist
\item
  at least 1 occurrences of a condition occurrence of \emph{Hypertensive disorder} (Table \ref{tab:hypertensionAceMono}) where event starts between 365 days Before and 0 days After index start date
\end{itemize}

Inclusion Criteria \#2: Has no prior antihypertensive drug exposures in medical history

Having all of the following criteria:

\begin{itemize}
\tightlist
\item
  exactly 0 occurrences of a drug exposure of \emph{Hypertension drugs} (Table \ref{tab:htnDrugsAceMono}) where event starts between all days Before and 1 days Before index start date
\end{itemize}

Inclusion Criteria \#3: Is only taking ACE as monotherapy, with no concomitant combination treatments

Having all of the following criteria:

\begin{itemize}
\tightlist
\item
  exactly 1 distinct occurrences of a drug era of \emph{Hypertension drugs} (Table \ref{tab:htnDrugsAceMono}) where event starts between 0 days Before and 7 days After index start date
\end{itemize}

Limit qualifying cohort to: earliest event per person.

\textbf{End Date Strategy}

Custom Drug Era Exit Criteria.
This strategy creates a drug era from the codes found in the specified concept set. If the index event is found within an era, the cohort end date will use the era's end date. Otherwise, it will use the observation period end date that contains the index event.

Use the era end date of \emph{ACE inhibitors} (Table \ref{tab:aceInhibitorsMono})

\begin{itemize}
\tightlist
\item
  allowing 30 days between exposures
\item
  adding 0 days after exposure end
\end{itemize}

\textbf{Cohort Collapse Strategy}

Collapse cohort by era with a gap size of 0 days.

\textbf{Concept Set Definitions}

\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:aceInhibitorsMono} ACE inhibitors}\tabularnewline
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endfirsthead
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endhead
1308216 & Lisinopril & NO & YES & NO\tabularnewline
1310756 & moexipril & NO & YES & NO\tabularnewline
1331235 & quinapril & NO & YES & NO\tabularnewline
1334456 & Ramipril & NO & YES & NO\tabularnewline
1335471 & benazepril & NO & YES & NO\tabularnewline
1340128 & Captopril & NO & YES & NO\tabularnewline
1341927 & Enalapril & NO & YES & NO\tabularnewline
1342439 & trandolapril & NO & YES & NO\tabularnewline
1363749 & Fosinopril & NO & YES & NO\tabularnewline
1373225 & Perindopril & NO & YES & NO\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:hypertensionAceMono} Hypertensive disorder}\tabularnewline
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endfirsthead
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endhead
316866 & Hypertensive disorder & NO & YES & NO\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:htnDrugsAceMono} Hypertension drugs}\tabularnewline
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endfirsthead
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endhead
904542 & Triamterene & NO & YES & NO\tabularnewline
907013 & Metolazone & NO & YES & NO\tabularnewline
932745 & Bumetanide & NO & YES & NO\tabularnewline
942350 & torsemide & NO & YES & NO\tabularnewline
956874 & Furosemide & NO & YES & NO\tabularnewline
970250 & Spironolactone & NO & YES & NO\tabularnewline
974166 & Hydrochlorothiazide & NO & YES & NO\tabularnewline
978555 & Indapamide & NO & YES & NO\tabularnewline
991382 & Amiloride & NO & YES & NO\tabularnewline
1305447 & Methyldopa & NO & YES & NO\tabularnewline
1307046 & Metoprolol & NO & YES & NO\tabularnewline
1307863 & Verapamil & NO & YES & NO\tabularnewline
1308216 & Lisinopril & NO & YES & NO\tabularnewline
1308842 & valsartan & NO & YES & NO\tabularnewline
1309068 & Minoxidil & NO & YES & NO\tabularnewline
1309799 & eplerenone & NO & YES & NO\tabularnewline
1310756 & moexipril & NO & YES & NO\tabularnewline
1313200 & Nadolol & NO & YES & NO\tabularnewline
1314002 & Atenolol & NO & YES & NO\tabularnewline
1314577 & nebivolol & NO & YES & NO\tabularnewline
1317640 & telmisartan & NO & YES & NO\tabularnewline
1317967 & aliskiren & NO & YES & NO\tabularnewline
1318137 & Nicardipine & NO & YES & NO\tabularnewline
1318853 & Nifedipine & NO & YES & NO\tabularnewline
1319880 & Nisoldipine & NO & YES & NO\tabularnewline
1319998 & Acebutolol & NO & YES & NO\tabularnewline
1322081 & Betaxolol & NO & YES & NO\tabularnewline
1326012 & Isradipine & NO & YES & NO\tabularnewline
1327978 & Penbutolol & NO & YES & NO\tabularnewline
1328165 & Diltiazem & NO & YES & NO\tabularnewline
1331235 & quinapril & NO & YES & NO\tabularnewline
1332418 & Amlodipine & NO & YES & NO\tabularnewline
1334456 & Ramipril & NO & YES & NO\tabularnewline
1335471 & benazepril & NO & YES & NO\tabularnewline
1338005 & Bisoprolol & NO & YES & NO\tabularnewline
1340128 & Captopril & NO & YES & NO\tabularnewline
1341238 & Terazosin & NO & YES & NO\tabularnewline
1341927 & Enalapril & NO & YES & NO\tabularnewline
1342439 & trandolapril & NO & YES & NO\tabularnewline
1344965 & Guanfacine & NO & YES & NO\tabularnewline
1345858 & Pindolol & NO & YES & NO\tabularnewline
1346686 & eprosartan & NO & YES & NO\tabularnewline
1346823 & carvedilol & NO & YES & NO\tabularnewline
1347384 & irbesartan & NO & YES & NO\tabularnewline
1350489 & Prazosin & NO & YES & NO\tabularnewline
1351557 & candesartan & NO & YES & NO\tabularnewline
1353766 & Propranolol & NO & YES & NO\tabularnewline
1353776 & Felodipine & NO & YES & NO\tabularnewline
1363053 & Doxazosin & NO & YES & NO\tabularnewline
1363749 & Fosinopril & NO & YES & NO\tabularnewline
1367500 & Losartan & NO & YES & NO\tabularnewline
1373225 & Perindopril & NO & YES & NO\tabularnewline
1373928 & Hydralazine & NO & YES & NO\tabularnewline
1386957 & Labetalol & NO & YES & NO\tabularnewline
1395058 & Chlorthalidone & NO & YES & NO\tabularnewline
1398937 & Clonidine & NO & YES & NO\tabularnewline
40226742 & olmesartan & NO & YES & NO\tabularnewline
40235485 & azilsartan & NO & YES & NO\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{Ami}{%
\section{Acute myocardial infarction (AMI)}\label{Ami}}

\textbf{Initial Event Cohort}

People having any of the following:

\begin{itemize}
\tightlist
\item
  a condition occurrence of \emph{Acute myocardial Infarction} (Table \ref{tab:ami})
\end{itemize}

with continuous observation of at least 0 days prior and 0 days after event index date, and limit initial events to: all events per person.

For people matching the Primary Events, include: Having any of the following criteria:

\begin{itemize}
\tightlist
\item
  at least 1 occurrences of a visit occurrence of \emph{Inpatient or ER visit} (Table \ref{tab:inpatientOrErAmi}) where event starts between all days Before and 0 days After index start date and event ends between 0 days Before and all days After index start date
\end{itemize}

Limit cohort of initial events to: all events per person.

Limit qualifying cohort to: all events per person.

\textbf{End Date Strategy}

Date Offset Exit Criteria.
This cohort defintion end date will be the index event's start date plus 7 days

\textbf{Cohort Collapse Strategy}

Collapse cohort by era with a gap size of 180 days.

\textbf{Concept Set Definitions}

\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:ami} Inpatient or ER visit}\tabularnewline
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endfirsthead
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endhead
314666 & Old myocardial infarction & YES & YES & NO\tabularnewline
4329847 & Myocardial infarction & NO & YES & NO\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:inpatientOrErAmi} Inpatient or ER visit}\tabularnewline
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endfirsthead
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endhead
262 & Emergency Room and Inpatient Visit & NO & YES & NO\tabularnewline
9201 & Inpatient Visit & NO & YES & NO\tabularnewline
9203 & Emergency Room Visit & NO & YES & NO\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{Angioedema}{%
\section{Angioedema}\label{Angioedema}}

\textbf{Initial Event Cohort}

People having any of the following:

\begin{itemize}
\tightlist
\item
  a condition occurrence of \emph{Angioedema} (Table \ref{tab:angioedema})
\end{itemize}

with continuous observation of at least 0 days prior and 0 days after event index date, and limit initial events to: all events per person.

For people matching the Primary Events, include:
Having any of the following criteria:

\begin{itemize}
\tightlist
\item
  at least 1 occurrences of a visit occurrence of \emph{Inpatient or ER visit} (Table \ref{tab:inpatientOrEr}) where event starts between all days Before and 0 days After index start date and event ends between 0 days Before and all days After index start date
\end{itemize}

Limit cohort of initial events to: all events per person.

Limit qualifying cohort to: all events per person.

\textbf{End Date Strategy}

This cohort defintion end date will be the index event's start date plus 7 days

\textbf{Cohort Collapse Strategy}

Collapse cohort by era with a gap size of 30 days.

\textbf{Concept Set Definitions}

\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:angioedema} Angioedema}\tabularnewline
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endfirsthead
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endhead
432791 & Angioedema & NO & YES & NO\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:inpatientOrEr} Inpatient or ER visit}\tabularnewline
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endfirsthead
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endhead
262 & Emergency Room and Inpatient Visit & NO & YES & NO\tabularnewline
9201 & Inpatient Visit & NO & YES & NO\tabularnewline
9203 & Emergency Room Visit & NO & YES & NO\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{ThiazidesMono}{%
\section{New users of Thiazide-like diuretics as first-line monotherapy for hypertension}\label{ThiazidesMono}}

\textbf{Initial Event Cohort}

People having any of the following:

\begin{itemize}
\tightlist
\item
  a drug exposure of \emph{Thiazide or thiazide-like diuretic} (Table \ref{tab:thiazidesMono}) for the first time in the person's history
\end{itemize}

with continuous observation of at least 365 days prior and 0 days after event index date, and limit initial events to: earliest event per person.

\textbf{Inclusion Rules}

Inclusion Criteria \#1: has hypertension diagnosis in 1 yr prior to treatment

Having all of the following criteria:

\begin{itemize}
\tightlist
\item
  at least 1 occurrences of a condition occurrence of \emph{Hypertensive disorder} (Table \ref{tab:hypertensionThzMono}) where event starts between 365 days Before and 0 days After index start date
\end{itemize}

Inclusion Criteria \#2: Has no prior antihypertensive drug exposures in medical history

Having all of the following criteria:

\begin{itemize}
\tightlist
\item
  exactly 0 occurrences of a drug exposure of \emph{Hypertension drugs} (Table \ref{tab:htnDrugsThzMono}) where event starts between all days Before and 1 days Before index start date
\end{itemize}

Inclusion Criteria \#3: Is only taking ACE as monotherapy, with no concomitant combination treatments

Having all of the following criteria:

\begin{itemize}
\tightlist
\item
  exactly 1 distinct occurrences of a drug era of \emph{Hypertension drugs} (Table \ref{tab:htnDrugsThzMono}) where event starts between 0 days Before and 7 days After index start date
\end{itemize}

Limit qualifying cohort to: earliest event per person.

\textbf{End Date Strategy}

Custom Drug Era Exit Criteria.
This strategy creates a drug era from the codes found in the specified concept set. If the index event is found within an era, the cohort end date will use the era's end date. Otherwise, it will use the observation period end date that contains the index event.

Use the era end date of \emph{Thiazide or thiazide-like diuretic} (Table \ref{tab:thiazidesMono})

\begin{itemize}
\tightlist
\item
  allowing 30 days between exposures
\item
  adding 0 days after exposure end
\end{itemize}

\textbf{Cohort Collapse Strategy}

Collapse cohort by era with a gap size of 0 days.

\textbf{Concept Set Definitions}

\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:thiazidesMono} Thiazide or thiazide-like diuretic}\tabularnewline
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endfirsthead
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endhead
907013 & Metolazone & NO & YES & NO\tabularnewline
974166 & Hydrochlorothiazide & NO & YES & NO\tabularnewline
978555 & Indapamide & NO & YES & NO\tabularnewline
1395058 & Chlorthalidone & NO & YES & NO\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:hypertensionThzMono} Hypertensive disorder}\tabularnewline
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endfirsthead
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endhead
316866 & Hypertensive disorder & NO & YES & NO\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:htnDrugsThzMono} Hypertension drugs}\tabularnewline
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endfirsthead
\toprule
Concept Id & Concept Name & Excluded & Descendants & Mapped\tabularnewline
\midrule
\endhead
904542 & Triamterene & NO & YES & NO\tabularnewline
907013 & Metolazone & NO & YES & NO\tabularnewline
932745 & Bumetanide & NO & YES & NO\tabularnewline
942350 & torsemide & NO & YES & NO\tabularnewline
956874 & Furosemide & NO & YES & NO\tabularnewline
970250 & Spironolactone & NO & YES & NO\tabularnewline
974166 & Hydrochlorothiazide & NO & YES & NO\tabularnewline
978555 & Indapamide & NO & YES & NO\tabularnewline
991382 & Amiloride & NO & YES & NO\tabularnewline
1305447 & Methyldopa & NO & YES & NO\tabularnewline
1307046 & Metoprolol & NO & YES & NO\tabularnewline
1307863 & Verapamil & NO & YES & NO\tabularnewline
1308216 & Lisinopril & NO & YES & NO\tabularnewline
1308842 & valsartan & NO & YES & NO\tabularnewline
1309068 & Minoxidil & NO & YES & NO\tabularnewline
1309799 & eplerenone & NO & YES & NO\tabularnewline
1310756 & moexipril & NO & YES & NO\tabularnewline
1313200 & Nadolol & NO & YES & NO\tabularnewline
1314002 & Atenolol & NO & YES & NO\tabularnewline
1314577 & nebivolol & NO & YES & NO\tabularnewline
1317640 & telmisartan & NO & YES & NO\tabularnewline
1317967 & aliskiren & NO & YES & NO\tabularnewline
1318137 & Nicardipine & NO & YES & NO\tabularnewline
1318853 & Nifedipine & NO & YES & NO\tabularnewline
1319880 & Nisoldipine & NO & YES & NO\tabularnewline
1319998 & Acebutolol & NO & YES & NO\tabularnewline
1322081 & Betaxolol & NO & YES & NO\tabularnewline
1326012 & Isradipine & NO & YES & NO\tabularnewline
1327978 & Penbutolol & NO & YES & NO\tabularnewline
1328165 & Diltiazem & NO & YES & NO\tabularnewline
1331235 & quinapril & NO & YES & NO\tabularnewline
1332418 & Amlodipine & NO & YES & NO\tabularnewline
1334456 & Ramipril & NO & YES & NO\tabularnewline
1335471 & benazepril & NO & YES & NO\tabularnewline
1338005 & Bisoprolol & NO & YES & NO\tabularnewline
1340128 & Captopril & NO & YES & NO\tabularnewline
1341238 & Terazosin & NO & YES & NO\tabularnewline
1341927 & Enalapril & NO & YES & NO\tabularnewline
1342439 & trandolapril & NO & YES & NO\tabularnewline
1344965 & Guanfacine & NO & YES & NO\tabularnewline
1345858 & Pindolol & NO & YES & NO\tabularnewline
1346686 & eprosartan & NO & YES & NO\tabularnewline
1346823 & carvedilol & NO & YES & NO\tabularnewline
1347384 & irbesartan & NO & YES & NO\tabularnewline
1350489 & Prazosin & NO & YES & NO\tabularnewline
1351557 & candesartan & NO & YES & NO\tabularnewline
1353766 & Propranolol & NO & YES & NO\tabularnewline
1353776 & Felodipine & NO & YES & NO\tabularnewline
1363053 & Doxazosin & NO & YES & NO\tabularnewline
1363749 & Fosinopril & NO & YES & NO\tabularnewline
1367500 & Losartan & NO & YES & NO\tabularnewline
1373225 & Perindopril & NO & YES & NO\tabularnewline
1373928 & Hydralazine & NO & YES & NO\tabularnewline
1386957 & Labetalol & NO & YES & NO\tabularnewline
1395058 & Chlorthalidone & NO & YES & NO\tabularnewline
1398937 & Clonidine & NO & YES & NO\tabularnewline
40226742 & olmesartan & NO & YES & NO\tabularnewline
40235485 & azilsartan & NO & YES & NO\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{NegativeControlsAppendix}{%
\chapter{Negative controls}\label{NegativeControlsAppendix}}

This Appendix contains negative controls used in various chapters of the book.

\hypertarget{AceiThzNsc}{%
\section{ACEi and THZ}\label{AceiThzNsc}}

\begin{longtable}[]{@{}rl@{}}
\caption{\label{tab:AceiThzNsc} Negative control outcomes when comparing ACE inhibitors (ACEi) to thiazides and thiazide-like diuretics (THZ).}\tabularnewline
\toprule
Concept ID & Concept Name\tabularnewline
\midrule
\endfirsthead
\toprule
Concept ID & Concept Name\tabularnewline
\midrule
\endhead
434165 & Abnormal cervical smear\tabularnewline
436409 & Abnormal pupil\tabularnewline
199192 & Abrasion and/or friction burn of trunk without infection\tabularnewline
4088290 & Absence of breast\tabularnewline
4092879 & Absent kidney\tabularnewline
44783954 & Acid reflux\tabularnewline
75911 & Acquired hallux valgus\tabularnewline
137951 & Acquired keratoderma\tabularnewline
77965 & Acquired trigger finger\tabularnewline
376707 & Acute conjunctivitis\tabularnewline
4103640 & Amputated foot\tabularnewline
73241 & Anal and rectal polyp\tabularnewline
133655 & Burn of forearm\tabularnewline
73560 & Calcaneal spur\tabularnewline
434327 & Cannabis abuse\tabularnewline
4213540 & Cervical somatic dysfunction\tabularnewline
140842 & Changes in skin texture\tabularnewline
81378 & Chondromalacia of patella\tabularnewline
432303 & Cocaine abuse\tabularnewline
4201390 & Colostomy present\tabularnewline
46269889 & Complication due to Crohn's disease\tabularnewline
134438 & Contact dermatitis\tabularnewline
78619 & Contusion of knee\tabularnewline
201606 & Crohn's disease\tabularnewline
76786 & Derangement of knee\tabularnewline
4115402 & Difficulty sleeping\tabularnewline
45757370 & Disproportion of reconstructed breast\tabularnewline
433111 & Effects of hunger\tabularnewline
433527 & Endometriosis\tabularnewline
4170770 & Epidermoid cyst\tabularnewline
4092896 & Feces contents abnormal\tabularnewline
259995 & Foreign body in orifice\tabularnewline
40481632 & Ganglion cyst\tabularnewline
4166231 & Genetic predisposition\tabularnewline
433577 & Hammer toe\tabularnewline
4231770 & Hereditary thrombophilia\tabularnewline
440329 & Herpes zoster without complication\tabularnewline
4012570 & High risk sexual behavior\tabularnewline
4012934 & Homocystinuria\tabularnewline
441788 & Human papilloma virus infection\tabularnewline
4201717 & Ileostomy present\tabularnewline
374375 & Impacted cerumen\tabularnewline
4344500 & Impingement syndrome of shoulder region\tabularnewline
139099 & Ingrowing nail\tabularnewline
444132 & Injury of knee\tabularnewline
196168 & Irregular periods\tabularnewline
432593 & Kwashiorkor\tabularnewline
434203 & Late effect of contusion\tabularnewline
438329 & Late effect of motor vehicle accident\tabularnewline
195873 & Leukorrhea\tabularnewline
4083487 & Macular drusen\tabularnewline
4103703 & Melena\tabularnewline
4209423 & Nicotine dependence\tabularnewline
377572 & Noise effects on inner ear\tabularnewline
40480893 & Nonspecific tuberculin test reaction\tabularnewline
136368 & Non-toxic multinodular goiter\tabularnewline
140648 & Onychomycosis due to dermatophyte\tabularnewline
438130 & Opioid abuse\tabularnewline
4091513 & Passing flatus\tabularnewline
4202045 & Postviral fatigue syndrome\tabularnewline
373478 & Presbyopia\tabularnewline
46286594 & Problem related to lifestyle\tabularnewline
439790 & Psychalgia\tabularnewline
81634 & Ptotic breast\tabularnewline
380706 & Regular astigmatism\tabularnewline
141932 & Senile hyperkeratosis\tabularnewline
36713918 & Somatic dysfunction of lumbar region\tabularnewline
443172 & Splinter of face, without major open wound\tabularnewline
81151 & Sprain of ankle\tabularnewline
72748 & Strain of rotator cuff capsule\tabularnewline
378427 & Tear film insufficiency\tabularnewline
437264 & Tobacco dependence syndrome\tabularnewline
194083 & Vaginitis and vulvovaginitis\tabularnewline
140641 & Verruca vulgaris\tabularnewline
440193 & Wristdrop\tabularnewline
4115367 & Wrist joint pain\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{SuggestedAnswers}{%
\chapter{Suggested Answers}\label{SuggestedAnswers}}

This Appendix contains suggested answers for the exercises in the book.

\hypertarget{SqlAndRanswers}{%
\section{SQL and R}\label{SqlAndRanswers}}

\textbf{Exercise \ref{exr:exercisePeopleCount}}

To compute the number of people we can simply query the PERSON table:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DatabaseConnector)}
\NormalTok{connection <-}\StringTok{ }\KeywordTok{connect}\NormalTok{(connectionDetails)}
\NormalTok{sql <-}\StringTok{ "SELECT COUNT(*) AS person_count}
\StringTok{FROM @cdm.person;"}

\KeywordTok{renderTranslateQuerySql}\NormalTok{(connection, sql, }\DataTypeTok{cdm =} \StringTok{"main"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   PERSON_COUNT
## 1         2694
\end{verbatim}

\textbf{Exercise \ref{exr:exerciseCelecoxibUsers}}

To compute the number of people with at least one prescription of celecoxib, we can query the DRUG\_EXPOSURE table. To find all drugs containing the ingredient celecoxib, we join to the CONCEPT\_ANCESTOR and CONCEPT tables:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DatabaseConnector)}
\NormalTok{connection <-}\StringTok{ }\KeywordTok{connect}\NormalTok{(connectionDetails)}
\NormalTok{sql <-}\StringTok{ "SELECT COUNT(DISTINCT(person_id)) AS person_count}
\StringTok{FROM @cdm.drug_exposure}
\StringTok{INNER JOIN @cdm.concept_ancestor}
\StringTok{  ON drug_concept_id = descendant_concept_id}
\StringTok{INNER JOIN @cdm.concept ingredient}
\StringTok{  ON ancestor_concept_id = ingredient.concept_id}
\StringTok{WHERE LOWER(ingredient.concept_name) = 'celecoxib'}
\StringTok{  AND ingredient.concept_class_id = 'Ingredient'}
\StringTok{  AND ingredient.standard_concept = 'S';"}

\KeywordTok{renderTranslateQuerySql}\NormalTok{(connection, sql, }\DataTypeTok{cdm =} \StringTok{"main"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   PERSON_COUNT
## 1         1844
\end{verbatim}

Note that we use \texttt{COUNT(DISTINCT(person\_id))} to find the number of distinct persons, considering that a person might have more than one prescription. Also note that we use the \texttt{LOWER} function to make our search for ``celecoxib'' case-insensitive.

Alternatively, we can use the DRUG\_ERA table, which is already rolled up to the ingredient level:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DatabaseConnector)}
\NormalTok{connection <-}\StringTok{ }\KeywordTok{connect}\NormalTok{(connectionDetails)}

\NormalTok{sql <-}\StringTok{ "SELECT COUNT(DISTINCT(person_id)) AS person_count}
\StringTok{FROM @cdm.drug_era}
\StringTok{INNER JOIN @cdm.concept ingredient}
\StringTok{  ON drug_concept_id = ingredient.concept_id}
\StringTok{WHERE LOWER(ingredient.concept_name) = 'celecoxib'}
\StringTok{  AND ingredient.concept_class_id = 'Ingredient'}
\StringTok{  AND ingredient.standard_concept = 'S';"}

\KeywordTok{renderTranslateQuerySql}\NormalTok{(connection, sql, }\DataTypeTok{cdm =} \StringTok{"main"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   PERSON_COUNT
## 1         1844
\end{verbatim}

\textbf{Exercise \ref{exr:exerciseGiBleedsDuringCelecoxib}}

To compute the number of diagnoses during exposure we extend our previous query by joining to the CONDITION\_OCCURRENCE table. We join to the CONCEPT\_ANCESTOR table to find all condition concepts that imply a gastrointestinal haemorrhage:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DatabaseConnector)}
\NormalTok{connection <-}\StringTok{ }\KeywordTok{connect}\NormalTok{(connectionDetails)}
\NormalTok{sql <-}\StringTok{ "SELECT COUNT(*) AS diagnose_count}
\StringTok{FROM @cdm.drug_era}
\StringTok{INNER JOIN @cdm.concept ingredient}
\StringTok{  ON drug_concept_id = ingredient.concept_id}
\StringTok{INNER JOIN @cdm.condition_occurrence}
\StringTok{  ON condition_start_date >= drug_era_start_date}
\StringTok{    AND condition_start_date <= drug_era_end_date}
\StringTok{INNER JOIN @cdm.concept_ancestor }
\StringTok{  ON condition_concept_id =descendant_concept_id}
\StringTok{WHERE LOWER(ingredient.concept_name) = 'celecoxib'}
\StringTok{  AND ingredient.concept_class_id = 'Ingredient'}
\StringTok{  AND ingredient.standard_concept = 'S'}
\StringTok{  AND ancestor_concept_id = 192671;"}

\KeywordTok{renderTranslateQuerySql}\NormalTok{(connection, sql, }\DataTypeTok{cdm =} \StringTok{"main"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   DIAGNOSE_COUNT
## 1         41
\end{verbatim}

Note that in this case it is essential to use the DRUG\_ERA table instead of the DRUG\_EXPOSURE table, because drug exposures with the same ingredient can overlap, but drug eras can. This could lead to double counting. For example, imagine a person received two drug drugs containing celecoxib at the same time. This would be recorded as two drug exposures, so any diagnoses occurring during the exposure would be counted twice. The two exposures will be merged into a single non-overlapping drug era.

\bibliography{book.bib,packages.bib}

\backmatter
\printindex


\end{document}
